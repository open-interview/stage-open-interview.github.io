{"questions":[{"id":"gh-12","question":"What are the three main service models of cloud computing and how do they differ?","answer":"Cloud computing has three service models: IaaS provides infrastructure, PaaS offers platforms for development, and SaaS delivers ready-to-use software applications.","explanation":"## Why Asked\nAssesses fundamental cloud knowledge and understanding of service hierarchy\n## Key Concepts\nIaaS (Infrastructure), PaaS (Platform), SaaS (Software), resource abstraction levels\n## Code Example\n```\n# AWS Service Models Examples\nIaaS: EC2 instances, S3 storage\nPaaS: Elastic Beanstalk, Lambda\nSaaS: AWS WorkMail, Chime\n```\n## Follow-up Questions\nWhen would you choose IaaS vs PaaS?\nWhat are the cost implications of each model?","diagram":"flowchart TD\n    A[Cloud Computing] --> B[IaaS]\n    A --> C[PaaS]\n    A --> D[SaaS]\n    B --> E[Virtual Machines]\n    B --> F[Storage]\n    C --> G[Runtime Environment]\n    C --> H[Development Tools]\n    D --> I[Applications]\n    D --> J[User Interface]","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=M--5UlkNAl0","longVideo":"https://www.youtube.com/watch?v=YpXpmc6lTEg"},"companies":["Amazon","Google","Meta"],"eli5":"Imagine you're at a toy store! IaaS is like buying just the empty shelves - you get the space but must bring your own toys and arrange them. PaaS is like getting a toy box with shelves already set up - you just need to put your toys in. SaaS is like buying a ready-to-play toy set that's already assembled and fun to use right away! Each way gives you less work to do but also less control over how your toys are set up.","relevanceScore":null,"lastUpdated":"2025-12-22T08:34:42.401Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-13","question":"What is AWS (Amazon Web Services)?","answer":"AWS is a comprehensive and widely adopted cloud platform, offering over 200 fully featured services from data centers globally. Key services include:","explanation":"AWS is a comprehensive and widely adopted cloud platform, offering over 200 fully featured services from data centers globally. Key services include:\n\n1. **Compute:**\n- EC2 (Elastic Compute Cloud)\n- Lambda (Serverless Computing)\n- ECS (Elastic Container Service)\n\n2. **Storage:**\n- S3 (Simple Storage Service)\n- EBS (Elastic Block Store)\n- EFS (Elastic File System)\n\n3. **Database:**\n- RDS (Relational Database Service)\n- DynamoDB (NoSQL Database)\n- Redshift (Data Warehouse)","diagram":"\ngraph TD\n    AWS --> EC2[EC2 Compute]\n    AWS --> S3[(S3 Storage)]\n    AWS --> RDS[(RDS Database)]\n    AWS --> Lambda[Lambda]\n","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"],"eli5":"Imagine AWS is like a giant toy store in the sky! Instead of buying all your toys and keeping them at home, you can rent toys whenever you want. Need a toy car? Just grab one! Need building blocks? They're ready! The toy store has everything - dolls, puzzles, games, and even special tools to help you build amazing things. You don't have to worry about where to keep your toys or fixing them when they break. The toy store takes care of everything! You just play and have fun. AWS is like that toy store, but for computer stuff instead of toys.","relevanceScore":null,"lastUpdated":"2025-12-24T12:47:05.716Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-15","question":"Compare AWS IaaS, PaaS, and SaaS service models with specific examples and use cases?","answer":"IaaS provides infrastructure (EC2), PaaS offers platforms (Elastic Beanstalk), SaaS delivers applications (WorkDocs). Choose based on control vs management needs.","explanation":"## Interview Context\nTests understanding of cloud service models and decision-making for architecture design.\n\n## Key Concepts\n- **IaaS**: Virtual machines, storage, networking - full control\n- **PaaS**: Platform services - managed runtime, deployment\n- **SaaS**: Complete applications - zero infrastructure management\n\n## AWS Examples\n```bash\n# IaaS - EC2 instance management\naws ec2 run-instances --image-id ami-12345 --instance-type t3.medium\n\n# PaaS - Elastic Beanstalk deployment\neb create my-app --application-version v1.0\n\n# SaaS - WorkDocs usage (no infrastructure needed)\n```\n\n## Decision Framework\n- **IaaS**: Custom OS, specific requirements, full control needed\n- **PaaS**: Rapid development, managed scaling, cost optimization\n- **SaaS**: Business applications, quick deployment, minimal IT overhead\n\n## Trade-offs\n- **Control vs Management**: IaaS max control, SaaS min management\n- **Cost**: IaaS pay-per-resource, PaaS pay-per-use, SaaS subscription\n- **Vendor Lock-in**: SaaS highest, IaaS lowest\n\n## Follow-up Questions\n1. How would you migrate from IaaS to PaaS for cost optimization?\n2. What security considerations differ between service models?\n3. How do you handle compliance requirements across different models?","diagram":"\ngraph TD\n    IaaS[IaaS - Infra] --> PaaS[PaaS - Platform]\n    PaaS --> SaaS[SaaS - Software]\n    FaaS[FaaS - Serverless]\n","difficulty":"intermediate","tags":["cloud","aws","azure","gcp"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=NhDYbskXRgc"},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you want to build a sandcastle at the beach! IaaS is like getting a big empty sandbox - you have all the sand and tools, but you build everything yourself. PaaS is like getting a pre-made sandcastle kit - the castle shape is ready, you just add decorations. SaaS is like renting a finished sandcastle - you just show up and play! Choose based on how much work you want to do versus how much fun you want to have.","relevanceScore":null,"lastUpdated":"2025-12-24T16:41:03.127Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-34","question":"What is Auto Scaling?","answer":"Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.","explanation":"Auto Scaling is a feature that automatically adjusts the number of compute resources based on the current demand.\n\nKey concepts:\n1. **Scaling Policies:**\n- Target tracking\n- Step scaling\n- Simple scaling\n\n2. **Metrics:**\n- CPU utilization\n- Memory usage\n- Request count\n- Custom metrics\n\nExample of AWS Auto Scaling configuration:\n```yaml\nAutoScalingGroup:\nMinSize: 1\nMaxSize: 10\nDesiredCapacity: 2\nHealthCheckType: ELB\nHealthCheckGracePeriod: 300\nLaunchTemplate:\nLaunchTemplateId: !Ref LaunchTemplate\nVersion: !GetAtt LaunchTemplate.LatestVersionNumber\n```","diagram":"\ngraph LR\n    Metrics[Metrics] --> ASG[Auto Scaling]\n    ASG -->|Scale Out| Add[Add Instances]\n    ASG -->|Scale In| Remove[Remove Instances]\n","difficulty":"advanced","tags":["scale","ha"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":"Imagine you're having a birthday party! Sometimes only 5 friends come, so you need just 1 pizza. But then 20 friends show up, so you need 4 pizzas! Auto Scaling is like having a magic pizza maker that watches how many friends arrive and automatically makes more or fewer pizzas. When lots of kids want to play on the swings, more swings magically appear. When only a few kids are playing, some swings go away so you don't waste space. It's like having a helper that counts how many people need something and adjusts it perfectly - never too much, never too little!","relevanceScore":null,"lastUpdated":"2025-12-24T12:49:33.166Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-57","question":"What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending in production environments?","answer":"Cloud Cost Optimization reduces cloud spend by identifying waste, right-sizing resources, using reserved instances, implementing auto-scaling, and monitoring usage patterns.","explanation":"## Why Asked\nInterviewers test understanding of cloud financial management and practical cost-saving techniques\n## Key Concepts\nResource right-sizing, reserved instances, spot instances, auto-scaling, cost monitoring, tagging strategies\n## Code Example\n```\nresource \"aws_instance\" \"optimized\" {\n  instance_type = \"t3.medium\" # Right-sized\n  spot_price    = \"0.02\" # Use spot when possible\n  tags = {\n    CostCenter = \"engineering\"\n  }\n}\n```\n## Follow-up Questions\nHow do you measure cost optimization success? What tools do you use for cost monitoring?","diagram":"flowchart TD\n  A[Cost Analysis] --> B[Right-sizing]\n  A --> C[Reserved Instances]\n  A --> D[Auto-scaling]\n  B --> E[Reduced Waste]\n  C --> E\n  D --> E\n  E --> F[Optimized Costs]","difficulty":"beginner","tags":["finops","cost"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a big box of LEGOs that you rent from a toy store. Cloud cost optimization is like being super smart with your LEGO money! You don't want to pay for LEGOs you're not playing with, right? So you only take out the exact pieces you need for your castle, not the whole box. If you're building a small house, you use small LEGOs, not giant ones. Sometimes you tell the toy store 'I'll play with these LEGOs every Tuesday' and they give you a special discount. And you keep checking your LEGO box to make sure you're not wasting pieces on things you don't play with anymore. It's all about using your toy money wisely!","relevanceScore":null,"lastUpdated":"2025-12-22T08:39:34.913Z","createdAt":"2025-12-24 12:51:26"},{"id":"gh-58","question":"What are AWS Reserved Instances and how do they compare to On-Demand pricing?","answer":"Reserved Instances provide up to 75% discount vs On-Demand pricing in exchange for 1-3 year commitment to specific instance configuration.","explanation":"Reserved Instances (RIs) provide significant cost savings compared to On-Demand pricing in exchange for a commitment to use a specific instance configuration for a one or three-year term.\n\n## Types of Reserved Instances:\n\n**Standard RIs:**\n- Highest discount (up to 75%)\n- Least flexibility - cannot change instance attributes\n- Best for steady-state workloads with predictable usage\n- Can be sold in RI Marketplace\n\n**Convertible RIs:**\n- Lower discount (up to 54%)\n- More flexibility - can exchange for different instance families, OS, tenancy\n- Good for workloads that may change over time\n- Cannot be sold in RI Marketplace\n\n**Scheduled RIs:**\n- For predictable recurring schedules (daily, weekly, monthly)\n- Match capacity reservation to specific usage patterns\n- Available in limited regions and instance types\n\n## Payment Options:\n- **All Upfront:** Highest discount, pay entire term upfront\n- **Partial Upfront:** Medium discount, pay portion upfront + monthly\n- **No Upfront:** Lowest discount, pay monthly only\n\n## Key Benefits:\n- Significant cost reduction for predictable workloads\n- Capacity reservation in specific AZ\n- Can be shared across accounts in organization","diagram":"graph TD\n    A[AWS EC2 Pricing] --> B[On-Demand]\n    A --> C[Reserved Instances]\n    A --> D[Spot Instances]\n    \n    C --> E[Standard RI<br/>Up to 75% discount]\n    C --> F[Convertible RI<br/>Up to 54% discount]\n    C --> G[Scheduled RI<br/>Recurring patterns]\n    \n    E --> H[All Upfront]\n    E --> I[Partial Upfront]\n    E --> J[No Upfront]\n    \n    F --> K[Can Exchange<br/>Instance Types]\n    G --> L[Time-based<br/>Reservations]","difficulty":"intermediate","tags":["finops","cost"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":"Imagine you want to rent a toy car at the playground. You can either pay for one ride at a time (that's On-Demand), or you can promise to use the same toy car every day for a whole year and get a special discount (that's Reserved Instances). When you make a promise to use the same toy for a long time, the playground owner gives you a much cheaper price because they know you'll keep coming back. The reserved toy car costs way less per ride than paying each time you show up. But you have to stick with the same toy car you picked at the beginning!","relevanceScore":null,"lastUpdated":"2025-12-24T12:52:16.276Z","createdAt":"2025-12-24 12:51:26"},{"id":"gh-83","question":"How do you evaluate cloud services for business needs using TCO analysis, SLA metrics, and migration strategies?","answer":"Cloud evaluation combines TCO analysis (3-year total cost including data transfer, storage, compute), SLA assessment (uptime, RTO/RPO), service comparison (EC2 vs Lambda vs Fargate), and migration strategy (rehost, refactor, rearchitect). Key factors include performance requirements, security compliance, vendor lock-in risks, and multi-cloud considerations.","explanation":"## TCO Analysis Methods\nCalculate 3-year total cost including:\n- Compute instances (on-demand vs reserved vs spot)\n- Storage costs (EBS, S3 tiers, Glacier)\n- Data transfer fees (egress costs often dominate)\n- Management overhead and operational costs\n\n## SLA Evaluation Framework\n- **Uptime**: 99.9% (8.76h downtime/year) vs 99.99% (52min)\n- **RTO/RPO**: Recovery time and point objectives\n- **Performance**: Latency SLAs, throughput guarantees\n- **Support**: Response times, escalation paths\n\n## Service Comparison Matrix\n| Use Case | EC2 | Lambda | Fargate |\n|----------|-----|--------|---------|\n| Web servers | ✓ | Limited | ✓ |\n| Event processing | ✓ | ✓ | ✓ |\n| Batch jobs | ✓ | Limited | ✓ |\n\n## Migration Strategies\n- **Rehost** (Lift & Shift): Quick, minimal changes\n- **Replatform** (Lift & Reshape): Some optimization\n- **Refactor**: Full cloud-native redesign\n- **Replace**: SaaS substitution\n\n## Multi-cloud Considerations\n- **Portability**: Container-based deployments\n- **Vendor lock-in**: Managed services vs open source\n- **Cost optimization**: Spot instances across providers\n- **Resilience**: Geographic distribution\n\n## Code Example: TCO Calculator\n```python\ndef calculate_tco(instance_type, storage_gb, months=36):\n    # AWS pricing example (simplified)\n    hourly_cost = get_pricing(instance_type)\n    monthly_compute = hourly_cost * 24 * 30\n    monthly_storage = storage_gb * 0.1  # $0.10/GB-month\n    data_transfer = estimate_egress(storage_gb * 0.3)  # 30% monthly\n    \n    monthly_total = monthly_compute + monthly_storage + data_transfer\n    return monthly_total * months\n```\n\n## Key Evaluation Criteria\n- **Performance**: Latency requirements, throughput needs\n- **Scalability**: Auto-scaling capabilities, burst capacity\n- **Security**: Compliance certifications, data residency\n- **Cost**: Pay-as-you-go vs committed spend discounts\n- **Vendor lock-in**: Proprietary services vs open standards","diagram":"flowchart TD\n    A[Business Requirements] --> B[Define Assessment Criteria]\n    B --> C[Identify Cloud Services]\n    C --> D[Technical Evaluation]\n    D --> E[Cost Analysis]\n    E --> F[Security Review]\n    F --> G[Compliance Check]\n    G --> H[Scoring Matrix]\n    H --> I[Recommendation Report]\n    I --> J[Decision & Implementation]\n    \n    D --> D1[Performance Metrics]\n    D --> D2[Scalability Tests]\n    E --> E1[TCO Calculation]\n    E --> E2[ROI Analysis]\n    F --> F1[Security Controls]\n    F --> F2[Risk Assessment]","difficulty":"advanced","tags":["migration","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Google","IBM","Microsoft","Oracle","Salesforce"],"eli5":"Imagine you're picking toys for a playground! You look at each toy and ask: Is it fun? Does it cost too much? Will it break easily? Can all kids play with it safely? Cloud services are like digital toys for businesses. You check if they're fast enough, don't cost too much money, keep your secrets safe, and follow the rules. Just like you'd pick the best slide that's not too scary, not too expensive, and everyone can use - you pick cloud services that work perfectly for what your business needs!","relevanceScore":null,"lastUpdated":"2025-12-24T16:52:08.161Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-85","question":"How do cloud migration tools automate application and data transfer between on-premise and cloud environments, and what are the key technical challenges in ensuring data consistency and minimal downtime?","answer":"Tools like AWS Migration Hub and Azure Migrate automate discovery, planning, replication, and cutover while maintaining data consistency through continuous synchronization and validation.","explanation":"## Why Asked\nTests understanding of enterprise cloud migration complexity, tool selection, and technical implementation challenges that architects face in real-world migrations.\n\n## Key Concepts\n- Migration strategies (6 R's: Rehost, Replatform, Refactor, Rearchitect, Repurchase, Retire)\n- Discovery and assessment automation (inventory mapping, dependency analysis)\n- Data replication mechanisms (block-level, file-level, database-level)\n- Cutover strategies (big bang, phased, blue-green)\n- Validation and rollback procedures\n\n## Code Example\n```\n# AWS Migration Hub example workflow\n1. Discovery: Application Discovery Service collects metrics\n2. Assessment: Migration Evaluator analyzes TCO\n3. Replication: AWS DMS continuous data sync\n4. Validation: Compare source/target checksums\n5. Cutover: DNS switch with rollback plan\n```\n\n## Follow-up Questions\n- How would you handle a 10TB database migration with <5min downtime?\n- What tools would you choose for a hybrid multi-cloud migration strategy?\n- How do you ensure data consistency during the cutover phase?","diagram":"graph TD\n    A[On-Premise Infrastructure] --> B[Discovery Engine]\n    B --> C[Assessment Tools]\n    C --> D[Migration Planning]\n    D --> E[Replication Engine]\n    E --> F[Cloud Staging Environment]\n    F --> G[Validation Testing]\n    G --> H{Validation Passed?}\n    H -->|Yes| I[Cutover Automation]\n    H -->|No| J[Remediation]\n    J --> G\n    I --> K[Cloud Production Environment]\n    \n    subgraph \"Migration Tools\"\n        B\n        C\n        E\n        I\n    end\n    \n    subgraph \"Cloud Provider\"\n        F\n        K\n    end","difficulty":"intermediate","tags":["migration","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"],"eli5":"Imagine you're moving all your toys from your bedroom to a new playroom. Special robot helpers come and count every toy you have, then make a plan to move them safely. While you're still playing in your old room, the robots make exact copies of all your toys in the new playroom. They keep checking that every toy is in the right place and nothing got lost during the move. When everything's ready, you just walk into the new playroom and can start playing immediately - no waiting! The hardest part is making sure no toys get lost or broken while moving, and that you can keep playing the whole time without stopping.","relevanceScore":null,"lastUpdated":"2025-12-24T12:58:57.546Z","createdAt":"2025-12-24 12:51:26"},{"id":"gh-87","question":"How would you implement a multi-cloud cost allocation system using tagging strategies and automation APIs?","answer":"Implement centralized tagging policies with automated cost allocation using cloud provider APIs and custom chargeback logic.","explanation":"## Interview Context\nThis question assesses your ability to design and implement technical solutions for cloud cost management, focusing on automation and system integration rather than financial governance.\n\n## Technical Implementation\n### Tagging Strategy\n```yaml\n# Centralized tagging policy\ntags:\n  - cost-center: \"engineering\"\n  - project: \"microservices-platform\"\n  - environment: \"${env}\"\n  - owner: \"${team}\"\n  - auto-tag: \"true\"\n```\n\n### Cost Allocation API Integration\n```python\n# AWS Cost Explorer API integration\nimport boto3\n\nclass CostAllocator:\n    def __init__(self):\n        self.ce = boto3.client('ce')\n        \n    def get_costs_by_tag(self, tag_key, time_period):\n        response = self.ce.get_cost_and_usage(\n            TimePeriod=time_period,\n            Granularity='MONTHLY',\n            GroupBy=[\n                {'Type': 'TAG', 'Key': tag_key},\n                {'Type': 'DIMENSION', 'Key': 'SERVICE'}\n            ],\n            Metrics=['BlendedCost']\n        )\n        return response['ResultsByTime']\n```\n\n### Multi-Cloud Aggregation\n```javascript\n// Multi-cloud cost aggregation\nclass MultiCloudCostAggregator {\n  constructor() {\n    this.providers = {\n      aws: new AWSCostExplorer(),\n      azure: new AzureCostManagement(),\n      gcp: new GCPCostAnalyzer()\n    };\n  }\n  \n  async getUnifiedCosts(timeRange) {\n    const costs = await Promise.all(\n      Object.entries(this.providers).map(\n        ([provider, client]) => client.getCosts(timeRange)\n      )\n    );\n    return this.normalizeAndAggregate(costs);\n  }\n}\n```\n\n## Follow-up Questions\n1. How would you handle tag propagation across auto-scaling resources?\n2. What strategies would you use for cost allocation in serverless architectures?\n3. How do you ensure data consistency when aggregating costs across different cloud providers?","diagram":"graph TD\n    A[Cloud Resources] --> B[Cost Tagging]\n    B --> C[Cost Collection Engine]\n    C --> D[Cost Allocation Rules]\n    D --> E[Showback Reports]\n    D --> F[Chargeback Invoices]\n    E --> G[Team Visibility]\n    F --> H[Finance Integration]\n    G --> I[Cost Optimization]\n    H --> J[Budget Planning]\n    I --> K[Resource Efficiency]\n    J --> L[Financial Governance]\n    K --> M[Reduced Waste]\n    L --> N[Business Alignment]","difficulty":"advanced","tags":["advanced","cloud"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=n7h1XFkQouU"},"companies":["Amazon","Google","Microsoft","Stripe","Uber"],"eli5":"Imagine you have a big box of LEGOs and you want to know who used which pieces. You put special stickers on each LEGO - red stickers for your toys, blue stickers for your sister's toys, and green stickers for shared toys. Then you have a magic robot that counts all the stickers and tells you exactly how many pieces each person used. The cloud is like that big LEGO box, the stickers are tags that show who owns what, and the robot is the computer that automatically counts everything and sends a bill to the right person!","relevanceScore":null,"lastUpdated":"2025-12-24T12:59:09.350Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-174","question":"You have an EC2 instance that suddenly becomes unresponsive. What step-by-step troubleshooting methodology would you follow, which specific AWS tools and commands would you use at each stage, and how would you handle different instance states and recovery scenarios?","answer":"Start with CloudWatch metrics (CPUUtilization, NetworkIn/Out, StatusCheckFailed). Use AWS Systems Manager Session Manager or EC2 Serial Console for direct access. Check instance state transitions via AWS CLI: `aws ec2 describe-instance-status`. Examine system logs via CloudWatch Logs or `/var/log`. Verify security groups and Network ACLs. If needed, reboot via `aws ec2 reboot-instances` or stop/start for full recovery. Use AWS Backup or EBS snapshots for disaster recovery.","explanation":"## Troubleshooting Methodology\n\n**Step 1: Initial Assessment**\n- Check CloudWatch metrics for CPU, memory, network anomalies\n- Verify instance status checks (system/instance)\n- Use AWS CLI: `aws ec2 describe-instance-status --instance-ids i-1234567890abcdef0`\n\n**Step 2: Direct Access**\n- AWS Systems Manager Session Manager for SSH-less access\n- EC2 Serial Console for kernel-level debugging\n- Commands: `ssm start-session --target i-1234567890abcdef0`\n\n**Step 3: Log Analysis**\n- CloudWatch Logs integration\n- System logs: `/var/log/syslog`, `/var/log/messages`\n- Application logs in `/var/log/app`\n\n**Step 4: Network Verification**\n- Security group rules: `aws ec2 describe-security-groups`\n- Network ACLs: `aws ec2 describe-network-acls`\n- VPC Flow Logs for traffic analysis\n\n**Step 5: Recovery Procedures**\n- Soft reboot: `aws ec2 reboot-instances`\n- Hard stop/start: `aws ec2 stop-instances` + `aws ec2 start-instances`\n- EBS volume recovery: detach/attach to new instance\n\n**Step 6: Prevention**\n- CloudWatch alarms for proactive monitoring\n- AWS Backup for automated snapshots\n- Enhanced monitoring with detailed metrics\n\n## Edge Cases & Gotchas\n\n- **Instance Store**: Data loss on stop/restart\n- **EBS Optimization**: Verify I/O performance\n- **Burst Performance**: Check T2/T3 credit balance\n- **IAM Permissions**: Ensure SSM access policies\n\n## Real-World Applications\n\n- Production web servers with 99.9% uptime SLA\n- Database instances requiring consistent performance\n- Batch processing jobs with strict completion deadlines\n\n## Performance Monitoring Tools\n\n- CloudWatch Custom Metrics\n- AWS X-Ray for distributed tracing\n- Third-party tools: Datadog, New Relic integration\n- Enhanced Monitoring for RDS instances","diagram":"graph TD\n    A[EC2 Instance Unresponsive] --> B[Check CloudWatch Metrics]\n    B --> C{Resource Issues?}\n    C -->|Yes| D[Scale Up Resources]\n    C -->|No| E[Use Serial Console]\n    E --> F[Examine System Logs]\n    F --> G{Network Issues?}\n    G -->|Yes| H[Check Security Groups/NACLs]\n    G -->|No| I[Verify Instance Status]\n    I --> J[Reboot Instance]\n    J --> K{Fixed?}\n    K -->|No| L[Stop/Restart Instance]\n    K -->|Yes| M[Issue Resolved]","difficulty":"intermediate","tags":["ec2","compute"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine your toy robot suddenly stops moving! First, you'd check if its batteries are working (that's like checking CloudWatch metrics). Then you'd try talking to it through a special phone (that's the EC2 Serial Console). Next, you'd look at its diary to see what it was doing before it got stuck (that's reading system logs). Finally, you'd make sure no one put a 'keep out' sign on your playground or blocked the door (that's checking security groups and network ACLs). Just like fixing a stuck toy, you check its power, try to talk to it, see what it was doing, and make sure nothing is blocking its way!","relevanceScore":null,"lastUpdated":"2025-12-23T06:38:40.586Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-321","question":"You have a containerized web application that needs to handle variable traffic loads. When would you choose ECS Fargate over EKS and what are the key trade-offs?","answer":"Choose ECS Fargate for simpler workloads with less operational overhead. EKS for complex microservices needing Kubernetes features. Trade-offs: control vs simplicity, cost vs flexibility.","explanation":"## Why Asked\nInterview context at Figma and Cloudflare tests understanding of container orchestration decisions and cost optimization in production environments.\n## Key Concepts\n- ECS Fargate: Serverless containers, managed service\n- EKS: Managed Kubernetes, more control\n- Cost implications, operational overhead\n- Scaling patterns and traffic handling\n## Code Example\n```\n# ECS Fargate Task Definition\n{\n  \"family\": \"web-app\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"256\",\n  \"memory\": \"512\"\n}\n```\n## Follow-up Questions\n- How would you handle auto-scaling for each service?\n- What monitoring strategies would you implement?\n- How do you handle secrets management in both?","diagram":"flowchart TD\n  A[Containerized App] --> B{Traffic Pattern?}\n  B -->|Variable/Simple| C[ECS Fargate]\n  B -->|Complex/Microservices| D[EKS]\n  C --> E[Lower Ops Overhead]\n  D --> F[More Control]","difficulty":"beginner","tags":["ec2","ecs","eks","fargate"],"channel":"aws","subChannel":"compute","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=oO-mGql5JvQ","longVideo":"https://www.youtube.com/watch?v=esISkPlnxL0"},"companies":["Cloudflare","Figma","MongoDB"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T13:34:04.391Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-216","question":"How would you design an eventual consistency strategy for a multi-region DynamoDB application using Global Tables to handle write conflicts, ensure data convergence, and minimize latency?","answer":"Implement DynamoDB Global Tables with version vectors for conflict resolution, using conditional writes with乐观并发控制. Combine Last-Writer-Wins for simple conflicts and custom application-specific resolvers for business logic conflicts. Monitor replication lag and implement conflict detection metrics.","explanation":"## Conflict Resolution Patterns\n\n**Version Vectors**: Track causal dependencies across regions to determine conflicting writes:\n\n```javascript\n// Item with version tracking\n{\n  \"pk\": \"user#123\",\n  \"sk\": \"profile\",\n  \"data\": {\"name\": \"John\"},\n  \"version\": {\"us-east-1\": 3, \"eu-west-1\": 2},\n  \"timestamp\": 1703123456789\n}\n\n// Conditional write with version check\nawait dynamodb.put({\n  TableName: \"users\",\n  Item: updatedItem,\n  ConditionExpression: \"attribute_not_exists(pk) OR version < :newVersion\"\n}).promise();\n```\n\n## Conflict Resolution Strategies\n\n**Last Writer Wins (LWW)**: Simple but acceptable for non-critical data like user preferences\n\n**Custom Resolvers**: For business-critical conflicts requiring domain logic:\n- Merge operations (shopping cart consolidation)\n- Business rule resolution (inventory vs orders)\n- Manual escalation for high-value conflicts\n\n## Consistency Trade-offs\n\n- **Latency**: ~100-200ms replication between regions\n- **Conflict Rate**: Typically <0.1% with proper data partitioning\n- **Storage Overhead**: 20-30 bytes per item for version metadata\n\n## Monitoring & Observability\n\nTrack key metrics:\n- Replication lag per region pair\n- Conflict frequency and resolution success\n- Conditional write failure rates\n- Data convergence time\n\nImplement dead letter queues for unresolvable conflicts requiring manual intervention.","diagram":"flowchart LR\n    A[Client Write US-East] --> B[DynamoDB US-East]\n    C[Client Write EU-West] --> D[DynamoDB EU-West]\n    B --> E[Async Replication]\n    D --> E\n    E --> F[Conflict Resolution]\n    F --> G[Final State]","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T06:07:12.959Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-357","question":"You're designing a security monitoring system that needs to store 10M+ events per day with millisecond read latency. How would you choose between DynamoDB, Aurora, and ElastiCache, and what's your data partitioning strategy?","answer":"Use DynamoDB with time-based partitioning (YYYY-MM-DD) + hot partitioning for recent data, Aurora for analytics, ElastiCache for frequently accessed security rules.","explanation":"## Why This Is Asked\nTests real-world database selection skills, understanding of AWS services trade-offs, and ability to design for scale - critical for security products at Palo Alto Networks.\n\n## Expected Answer\nStrong candidates discuss: DynamoDB for high-throughput writes with TTL, Aurora for complex queries/joins, ElastiCache Redis for hot data, partitioning strategy to avoid hot keys, cost considerations, and backup/recovery.\n\n## Code Example\n```typescript\n// DynamoDB partition key strategy\nconst partitionKey = `security-events#${date.slice(0, 7)}`; // YYYY-MM\nconst sortKey = `${event.timestamp}#${event.eventType}`;\n\n// Hot partition for recent data (last 24h)\nif (isRecentEvent(timestamp)) {\n  const hotPartition = `security-events#hot#${date.slice(0, 10)}`;\n  // Write to both hot and regular partition\n}\n```\n\n## Follow-up Questions\n- How would you handle a sudden 10x traffic spike?\n- What's your backup and disaster recovery strategy?\n- How would you optimize costs while maintaining performance?","diagram":"flowchart TD\n  A[Security Event] --> B{Event Age?}\n  B -->|< 24h| C[Write to Hot Partition]\n  B -->|>= 24h| D[Write to Regular Partition]\n  C --> E[DynamoDB + ElastiCache]\n  D --> F[DynamoDB]\n  E --> G[Millisecond Reads]\n  F --> G\n  G --> H[Aurora Analytics]","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=JIbIYCM48to","longVideo":null},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix","Palo Alto Networks"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:02:41.869Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-401","question":"You're designing a real-time analytics dashboard for Scale AI that needs to handle 10,000 events/second. Your team is debating between using DynamoDB with DAX vs. Aurora with ElastiCache. What are the key trade-offs you'd consider, and which would you choose for this use case?","answer":"Choose DynamoDB with DAX for predictable performance at scale, lower operational overhead, and cost-effectiveness for high-throughput analytics.","explanation":"## Why This Is Asked\nTests practical decision-making skills, understanding of AWS database services, and ability to analyze trade-offs for real-world scenarios at scale.\n\n## Expected Answer\nCandidate should discuss:\n- DynamoDB: NoSQL, unlimited scaling, DAX for caching, pay-per-request, eventual consistency\n- Aurora: Relational, ACID compliance, complex queries, connection pooling, higher cost\n- For 10k events/sec: DynamoDB's horizontal scaling and DAX caching provide better performance\n- Aurora's transaction overhead and connection limits could bottleneck at this scale\n\n## Code Example\n```typescript\n// DynamoDB with DAX configuration\nconst dax = new DAXClient({\n  region: 'us-west-2',\n  endpoints: ['dax-cluster.abc123.cache.amazonaws.com:8111']\n});\n\nconst putEvent = async (eventId: string, eventData: any) => {\n  await dax.putItem({\n    TableName: 'analytics-events',\n    Item: {\n      eventId,\n      timestamp: Date.now(),\n      ...eventData\n    }\n  });\n};\n```\n\n## Follow-up Questions\n- How would you handle data consistency requirements?\n- What monitoring metrics would you track?\n- How would you optimize costs at this scale?","diagram":"flowchart TD\n  A[Real-time Events 10k/sec] --> B{Database Choice}\n  B -->|DynamoDB + DAX| C[Horizontal Scaling]\n  B -->|Aurora + ElastiCache| D[Vertical Scaling]\n  C --> E[Millisecond Latency]\n  C --> F[Pay-per-request]\n  D --> G[Complex Queries]\n  D --> H[Higher Cost]\n  E --> I[Analytics Dashboard]\n  G --> I","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=crHwekf0gTA","longVideo":"https://www.youtube.com/watch?v=5iZ1o4w7354"},"companies":["Cohere","Oscar Health","Scale Ai"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:22:29.211Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-413","question":"You're designing a real-time analytics dashboard for an IoT application that receives 10,000 events per second. The dashboard needs to show current metrics and historical trends. How would you design the database architecture using AWS services, and what caching strategy would you implement?","answer":"Use DynamoDB for event ingestion with time-based partitioning, Aurora for analytical queries, and ElastiCache Redis for real-time dashboard caching.","explanation":"## Why This Is Asked\nApple tests your ability to make informed database decisions, understand trade-offs between consistency and performance, and design scalable architectures that handle high-throughput workloads.\n\n## Expected Answer\nStrong candidates discuss DynamoDB for write-heavy workloads with on-demand capacity, Aurora for complex analytical queries with its MySQL compatibility, and ElastiCache Redis for sub-millisecond dashboard response times. They should mention data partitioning strategies, read replicas, and cache invalidation patterns.\n\n## Code Example\n```typescript\n// DynamoDB event ingestion\nconst putEvent = async (deviceId: string, eventData: any) => {\n  const params = {\n    TableName: 'iot-events',\n    Item: {\n      deviceId,\n      timestamp: Date.now(),\n      ...eventData,\n      ttl: Math.floor(Date.now() / 1000) + (30 * 24 * 60 * 60) // 30 days\n    }\n  };\n  await dynamodb.put(params).promise();\n  \n  // Update cache\n  await redis.zadd(`device:${deviceId}:metrics`, Date.now(), JSON.stringify(eventData));\n};\n```\n\n## Follow-up Questions\n- How would you handle backpressure if event ingestion exceeds 10,000/sec?\n- What's your strategy for data retention and cost optimization?\n- How would you ensure data consistency between DynamoDB and Aurora?","diagram":"flowchart TD\n    A[IoT Events] --> B[DynamoDB Stream]\n    B --> C[AWS Lambda Processor]\n    C --> D[Aurora Analytics DB]\n    C --> E[ElastiCache Redis]\n    E --> F[Real-time Dashboard]\n    D --> G[Historical Reports]\n    A --> H[DynamoDB Events Table]\n    H --> I[TTL Cleanup]\n    C --> J[CloudWatch Metrics]","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"channel":"aws","subChannel":"database","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Databricks","Google","Micron","Microsoft","Netflix","Snowflake"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T12:54:19.556Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-454","question":"You need to host a static website with high availability and low latency globally. How would you configure AWS S3 and CloudFront to achieve this?","answer":"Configure S3 bucket with static website hosting, enable versioning, and block public access. Create CloudFront distribution with S3 origin, set up OAI for secure access, enable caching, and configure ","explanation":"## S3 Configuration\n- Enable static website hosting\n- Set bucket policy for CloudFront OAI\n- Enable versioning for backup\n\n## CloudFront Setup\n- Create distribution with S3 as origin\n- Configure Origin Access Identity\n- Set cache TTLs for static assets\n- Enable compression\n\n## Best Practices\n- Use HTTPS with AWS Certificate Manager\n- Configure custom domain\n- Set up logging and monitoring","diagram":"flowchart TD\n  A[User Request] --> B[CloudFront Edge]\n  B --> C[Cache Hit?]\n  C -->|Yes| D[Return Cached]\n  C -->|No| E[S3 Origin]\n  E --> F[Return Content]\n  F --> B","difficulty":"beginner","tags":["aws"],"channel":"aws","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Netflix"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:45:13.220Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-220","question":"How would you design a multi-AZ VPC architecture with Route53 latency-based routing to CloudFront, ALB, and private EC2 instances while ensuring failover within 30 seconds?","answer":"Use Route53 latency records with health checks, CloudFront with origin failover, ALB across AZs, and cross-AZ private subnets with NAT gateways.","explanation":"## Concept Overview\nDesigning a resilient AWS networking architecture requires understanding how different services interact for high availability and low latency.\n\n## Implementation Details\n\n### VPC Architecture\n- Create VPC with /16 CIDR block\n- 3 public subnets (one per AZ) for ALB and NAT\n- 3 private subnets for EC2 instances\n- Configure Internet Gateway and NAT gateways\n\n### Route53 Configuration\n```json\n{\n  \"RecordType\": \"A\",\n  \"SetIdentifier\": \"primary\",\n  \"HealthCheckId\": \"health-check-id\",\n  \"TTL\": 30\n}\n```\n\n### Load Balancer Setup\n- Application Load Balancer in public subnets\n- Cross-AZ deployment enabled\n- Health checks on /health endpoint\n- Target groups for EC2 instances\n\n### CloudFront Origin\n- Primary origin: ALB DNS name\n- Failover origin: S3 static backup\n- Origin Access Identity for security\n\n## Common Pitfalls\n- Health check intervals too long (>30s)\n- Missing cross-AZ ALB configuration\n- NAT gateway single point of failure\n- Inconsistent security group rules\n- Route53 TTL too high for quick failover","diagram":"graph TD\n    A[User] --> B[Route53]\n    B --> C[CloudFront]\n    C --> D[ALB Primary]\n    C --> E[ALB Secondary]\n    D --> F[EC2 AZ1]\n    D --> G[EC2 AZ2]\n    D --> H[EC2 AZ3]\n    E --> I[EC2 AZ1 Backup]\n    E --> J[EC2 AZ2 Backup]\n    E --> K[EC2 AZ3 Backup]\n    F --> L[Private Subnet]\n    G --> L\n    H --> L\n    I --> L\n    J --> L\n    K --> L\n    L --> M[NAT Gateway]\n    M --> N[Internet Gateway]","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"channel":"aws","subChannel":"networking","sourceUrl":null,"videos":null,"companies":["Amazon","Databricks","Goldman Sachs","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:06:50.427Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-384","question":"You're designing a multi-region SaaS application with users in North America and Europe. How would you configure Route53, CloudFront, ALB, and VPC to ensure low latency and high availability? What are the key trade-offs?","answer":"Use Route53 latency-based routing pointing to CloudFront edge locations, which cache content at regional ALBs in separate VPCs with cross-region replication.","explanation":"## Why This Is Asked\nTests understanding of AWS networking architecture, global content delivery, and trade-offs between latency, cost, and complexity.\n\n## Expected Answer\nCandidate should explain: Route53 latency-based routing to nearest CloudFront edge, CloudFront caching static content and routing dynamic requests to regional ALBs, ALBs distributing traffic across EC2 instances in separate VPCs per region, VPC peering or Transit Gateway for cross-region communication, and trade-offs like cost vs performance, data consistency vs availability.\n\n## Code Example\n```typescript\n// Route53 latency-based routing configuration\nconst hostedZone = new route53.HostedZone(this, 'HostedZone', {\n  zoneName: 'example.com'\n});\n\nnew route53.RecordSet(this, 'LatencyRecord', {\n  hostedZoneId: hostedZone.zoneId,\n  recordName: 'api.example.com',\n  type: 'A',\n  setIdentifier: 'us-east-1',\n  region: 'us-east-1',\n  latency: 50,\n  target: albUsEast.loadBalancer.dnsName\n});\n```\n\n## Follow-up Questions\n- How would you handle database replication between regions?\n- What happens if CloudFront cache misses and how do you optimize?\n- How do you implement failover between regions?","diagram":"flowchart TD\n    A[User Request] --> B[Route53 Latency Routing]\n    B --> C{User Region}\n    C -->|North America| D[CloudFront US-East Edge]\n    C -->|Europe| E[CloudFront EU-West Edge]\n    D --> F[ALB US-East]\n    E --> G[ALB EU-West]\n    F --> H[VPC US-East Subnets]\n    G --> I[VPC EU-West Subnets]\n    H --> J[EC2 Instances US]\n    I --> K[EC2 Instances EU]\n    J --> L[RDS Cross-Region Replication]\n    K --> L","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"channel":"aws","subChannel":"networking","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=27r4Bzuj5NQ","longVideo":null},"companies":["Airtable","Cisco","Epic Games"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:16:21.927Z","createdAt":"2025-12-24 12:51:27"},{"id":"gh-66","question":"How does serverless computing abstract infrastructure management and what are its key execution characteristics?","answer":"Serverless computing abstracts infrastructure through event-driven functions that auto-scale, with pay-per-use billing and zero server maintenance.","explanation":"## Concept Overview\nServerless computing is a cloud execution model where providers manage infrastructure, scaling, and resource allocation automatically. Developers focus solely on business logic through functions triggered by events.\n\n## Implementation\nServerless platforms use container-based execution environments that spin up on demand:\n\n```javascript\n// AWS Lambda example\nexports.handler = async (event) => {\n  const { action, data } = JSON.parse(event.body);\n  \n  switch(action) {\n    case 'process':\n      return await processData(data);\n    case 'validate':\n      return await validateInput(data);\n    default:\n      throw new Error('Unsupported action');\n  }\n};\n```\n\nKey execution characteristics:\n- **Cold Starts**: Initial invocation latency (100-3000ms)\n- **Stateless**: Each execution is independent\n- **Resource Limits**: Memory (128-3008MB), duration (max 15 minutes)\n- **Auto-scaling**: Concurrent instances based on request volume\n\n## Trade-offs\n\n**Pros:**\n- Zero operational overhead\n- Cost-effective for sporadic workloads\n- Built-in high availability and fault tolerance\n- Automatic scaling from 0 to thousands\n\n**Cons:**\n- Cold start latency\n- Vendor lock-in\n- Limited execution time and resources\n- Debugging complexity in distributed environments\n\n**When to use:**\n- API endpoints with unpredictable traffic\n- Data processing pipelines\n- Scheduled tasks and cron jobs\n- Real-time file processing\n\n## Common Pitfalls\n\n1. **Ignoring Cold Starts**: Not implementing provisioned concurrency for latency-sensitive applications\n\n2. **Stateful Anti-patterns**: Storing local data between invocations\n```javascript\n// BAD - stateful approach\nlet counter = 0;\nexports.handler = async (event) => {\n  counter++; // Lost between invocations\n  return { count: counter };\n};\n\n// GOOD - stateless with external storage\nexports.handler = async (event) => {\n  const currentCount = await getCounterFromDB();\n  await updateCounterInDB(currentCount + 1);\n  return { count: currentCount + 1 };\n};\n```\n\n3. **Timeout Misconfiguration**: Not setting appropriate timeouts for external service calls\n\n4. **Resource Over-provisioning**: Allocating excessive memory, increasing costs unnecessarily\n\n5. **Missing Error Handling**: Not implementing retry logic for transient failures","diagram":"flowchart TD\n    A[Client Request] --> B[API Gateway]\n    B --> C{Event Trigger}\n    C -->|HTTP| D[HTTP Function]\n    C -->|File Upload| E[Storage Function]\n    C -->|Database Change| F[DB Trigger Function]\n    \n    D --> G[Function Container]\n    E --> G\n    F --> G\n    \n    G --> H[Business Logic]\n    H --> I[External Services]\n    H --> J[Database]\n    \n    I --> K[Response]\n    J --> K\n    K --> L[Client]\n    \n    M[Auto Scaling] --> G\n    N[Monitoring] --> O[Logs & Metrics]\n    G --> N\n    \n    style G fill:#e1f5fe\n    style M fill:#f3e5f5\n    style N fill:#fff3e0","difficulty":"beginner","tags":["serverless","lambda"],"channel":"aws","subChannel":"serverless","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=W_VV2Fx32_Y","longVideo":"https://www.youtube.com/watch?v=Fx3ZGy-mbV4"},"companies":["Airbnb","Amazon","Google","Microsoft","Uber"],"eli5":"Imagine you have a magic toy box that makes any toy you want, exactly when you want it! When you ask for a car, poof - a car appears. When you're done playing, it disappears. You don't have to clean up, store toys, or even know how the magic works. Serverless computing is like that magic toy box for computer programs. When someone needs your program to do something, it magically appears, does the job, then vanishes. You only pay for the few seconds it was working, like paying for just one ride at the playground instead of buying the whole playground. The best part? You never have to worry about fixing broken toys or organizing the toy box - the magic takes care of everything!","relevanceScore":null,"lastUpdated":"2025-12-24T12:54:30.274Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-246","question":"How would you design a serverless order processing workflow using AWS Step Functions with Lambda functions, implementing specific retry patterns, error handling, and state management?","answer":"Define state machine JSON with Task states, Lambda ARNs, Retry/Catch policies, and Choice states for order validation logic.","explanation":"## Interview Context\nThis question assesses AWS Step Functions expertise, serverless architecture design, and error handling patterns in production workflows.\n\n## Key Components\n- **State Machine JSON**: Define Task states with Lambda ARNs, Retry/Catch policies, and Choice states\n- **Error Handling**: Implement exponential backoff, dead letter queues, and custom error handling\n- **Lambda Integration**: Use event source mapping, proper IAM roles, and payload validation\n\n## Code Example\n```json\n{\n  \"Comment\": \"Order Processing Workflow\",\n  \"StartAt\": \"ValidateOrder\",\n  \"States\": {\n    \"ValidateOrder\": {\n      \"Type\": \"Task\",\n      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:validate-order\",\n      \"Retry\": [{\n        \"ErrorEquals\": [\"Lambda.Timeout\", \"Lambda.Unknown\"],\n        \"IntervalSeconds\": 2,\n        \"MaxAttempts\": 3,\n        \"BackoffRate\": 2.0\n      }],\n      \"Catch\": [{\n        \"ErrorEquals\": [\"States.All\"],\n        \"Next\": \"OrderFailed\",\n        \"ResultPath\": \"$.error\"\n      }],\n      \"Next\": \"ProcessPayment\"\n    }\n  }\n}\n```\n\n## NFRs & Calculations\n- **Availability**: 99.9% with Step Functions built-in redundancy\n- **Latency**: <500ms for state transitions, <2s total workflow\n- **Throughput**: 1000 orders/minute with parallel Lambda execution\n- **Cost**: $0.025 per 1000 state transitions + Lambda costs\n\n## Follow-up Questions\n1. How would you implement saga pattern for distributed transactions?\n2. What monitoring strategies would you use for workflow observability?\n3. How do you handle state machine versioning and blue-green deployments?","diagram":"graph TD\n    A[Order Received] --> B[Validate Order Lambda]\n    B -->|Success| C[Process Payment Lambda]\n    B -->|Invalid Order| D[Send Failure Notification]\n    C -->|Payment Success| E[Update Inventory Lambda]\n    C -->|Payment Failed| F[Retry Payment]\n    F -->|3 Attempts Failed| G[Cancel Order]\n    E --> H[Send Confirmation Lambda]\n    H --> I[Order Complete]\n    D --> J[End]\n    G --> J\n    I --> K[End]","difficulty":"intermediate","tags":["lambda","api-gateway","step-functions"],"channel":"aws","subChannel":"serverless","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you're building a LEGO castle with friends! Each friend has a special job - one checks if you have enough bricks, another builds the walls, and one adds the roof. If a friend drops their bricks, they can try again 3 times before asking for help. If they still can't do it, you call the teacher over! You keep a checklist to remember which part is finished and which still needs work. Sometimes you need to make choices - if you're building a tower, you go up, but if it's a bridge, you go across. Everything happens step by step, like following a recipe for making cookies!","relevanceScore":null,"lastUpdated":"2025-12-22T08:32:42.433Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-292","question":"How would you design a data lifecycle strategy for a media company storing petabytes of video content requiring immediate access, archiving, and cost optimization across AWS storage services?","answer":"Use S3 Standard for active videos, S3 IA for infrequent access, and Glacier for deep archive with lifecycle policies for automated transitions.","explanation":"## Why Asked\nTests ability to design cost-effective storage solutions using AWS storage hierarchy and lifecycle management.\n\n## Key Concepts\nS3 storage classes (Standard, IA, Glacier), lifecycle policies, cost optimization, data access patterns, transition rules.\n\n## Code Example\n```\n{\n  \"Rules\": [{\n    \"ID\": \"VideoLifecycle\",\n    \"Status\": \"Enabled\",\n    \"Transitions\": [\n      {\"Days\": 30, \"StorageClass\": \"STANDARD_IA\"},\n      {\"Days\": 90, \"StorageClass\": \"GLACIER\"},\n      {\"Days\": 365, \"StorageClass\": \"DEEP_ARCHIVE\"}\n    ]\n  }]\n}\n```\n\n## Follow-up Questions\nHow do you handle retrieval costs? What about data replication? How do you monitor storage costs?","diagram":"flowchart TD\n  A[New Video Upload] --> B[S3 Standard]\n  B --> C{30 days?}\n  C -->|Yes| D[S3 Standard IA]\n  C -->|No| B\n  D --> E{90 days?}\n  E -->|Yes| F[S3 Glacier]\n  E -->|No| D\n  F --> G{365 days?}\n  G -->|Yes| H[S3 Deep Archive]\n  G -->|No| F","difficulty":"advanced","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Netflix","Youtube"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T08:36:48.037Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-307","question":"What are the key differences between S3, EBS, and EFS in terms of performance, scalability, and use cases?","answer":"S3: object storage, unlimited scale, 99.999999999% durability. EBS: block storage, attached to EC2, high performance. EFS: file storage, multi-AZ, shared access.","explanation":"## Why Asked\nTests understanding of AWS storage services and when to use each one based on requirements.\n## Key Concepts\nObject vs Block vs File storage, durability, availability, performance characteristics, cost models.\n## Code Example\n```\n# S3 - Object storage for static assets\naws s3 cp ./index.html s3://my-bucket/\n\n# EBS - Block storage for databases\nebs-volume = ec2.create_volume(Size=100, VolumeType='gp3')\n\n# EFS - File storage for shared access\naws efs create-file-system --performance-mode generalPurpose\n```\n## Follow-up Questions\nHow do you choose between storage classes? What about backup strategies? How do you optimize costs?","diagram":"flowchart TD\n  A[Storage Need] --> B{Data Type?}\n  B -->|Objects| C[S3]\n  B -->|Block| D[EBS]\n  B -->|Files| E[EFS]\n  C --> F[Static Assets, Archives]\n  D --> G[Databases, Boot Volumes]\n  E --> H[Shared File Systems]","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=_CN7KqC3y3s"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T13:26:04.000Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-370","question":"You're designing a file storage system for Canva's design assets. Users upload large PSD files (up to 10GB) that need versioning and quick access. How would you architect this using AWS storage services, considering cost, performance, and durability?","answer":"Use S3 Standard for active files, S3 Standard-IA for older versions, and Glacier Deep Archive for archival. Implement lifecycle policies and CloudFront CDN.","explanation":"## Why This Is Asked\nTests practical AWS storage knowledge, cost optimization, and understanding of trade-offs between performance and expense - crucial for Canva's asset-heavy platform.\n\n## Expected Answer\nStrong candidates discuss S3 storage classes, lifecycle policies, CloudFront for CDN, versioning, cross-region replication, and cost calculations. They should mention EBS vs EFS vs S3 trade-offs.\n\n## Code Example\n```typescript\n// S3 lifecycle policy for cost optimization\nconst lifecyclePolicy = {\n  Rules: [{\n    ID: 'DesignAssetLifecycle',\n    Status: 'Enabled',\n    Transitions: [\n      { Days: 30, StorageClass: 'STANDARD_IA' },\n      { Days: 90, StorageClass: 'GLACIER' },\n      { Days: 365, StorageClass: 'DEEP_ARCHIVE' }\n    ]\n  }]\n};\n```\n\n## Follow-up Questions\n- How would you handle concurrent uploads of the same file?\n- What's your strategy for disaster recovery?\n- How would you optimize for global user access?","diagram":"flowchart TD\n    A[User Uploads 10GB PSD] --> B[S3 Standard - Active Files]\n    B --> C[CloudFront CDN Edge]\n    C --> D[Global User Access]\n    B --> E{30 Days Old?}\n    E -->|Yes| F[S3 Standard-IA]\n    E -->|No| B\n    F --> G{90 Days Old?}\n    G -->|Yes| H[S3 Glacier]\n    G -->|No| F\n    H --> I{365 Days Old?}\n    I -->|Yes| J[Glacier Deep Archive]\n    I -->|No| H","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"channel":"aws","subChannel":"storage","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Affirm","Booking.com","Canva"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T12:46:08.223Z","createdAt":"2025-12-24 12:51:26"}],"subChannels":["compute","database","general","networking","serverless","storage"],"companies":["Affirm","Airbnb","Airtable","Amazon","Apple","Booking.com","Canva","Cisco","Citadel","Cloudflare","Cohere","Databricks","Epic Games","Figma","Goldman Sachs","Google","IBM","Meta","Micron","Microsoft","MongoDB","Netflix","Oracle","Oscar Health","Palo Alto Networks","Salesforce","Scale Ai","Snowflake","Stripe","Uber","Youtube"],"stats":{"total":23,"beginner":6,"intermediate":13,"advanced":4,"newThisWeek":23}}