{"questions":[{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","answer":"Use pandas with chunking for memory efficiency: `pd.read_csv('file.csv', chunksize=10000)`. Process each chunk, validate data types, convert timestamps with `pd.to_datetime()`, and use `psycopg2.extra","explanation":"## Key Components\n\n- **Memory Management**: Chunk large files to avoid memory issues\n- **Data Validation**: Check for missing values, correct data types\n- **Bulk Operations**: Use batch inserts instead of row-by-row\n- **Error Handling**: Log failed records for retry\n\n## Implementation Strategy\n\n```python\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_batch\n\ndef process_csv_chunk(chunk):\n    # Clean and validate data\n    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n    chunk.dropna(inplace=True)\n    return chunk\n\ndef load_to_db(chunks):\n    conn = psycopg2.connect(db_url)\n    cursor = conn.cursor()\n    \n    for chunk in chunks:\n        processed = process_csv_chunk(chunk)\n        execute_batch(cursor, insert_query, processed.values)\n    \n    conn.commit()\n```\n\n## Performance Considerations\n\n- Use database indexes on frequently queried columns\n- Consider parallel processing for multiple files\n- Monitor memory usage and adjust chunk size accordingly","diagram":"flowchart TD\n  A[CSV Files] --> B[Chunk Processing]\n  B --> C[Data Validation]\n  C --> D[Type Conversion]\n  D --> E[Batch Insert]\n  E --> F[PostgreSQL]\n  B --> G[Error Logging]\n  G --> H[Retry Queue]","difficulty":"beginner","tags":["data-engineering"],"channel":"data-engineering","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Discord","MongoDB"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:45:38.396Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","answer":"Lambda architecture using batch layer for historical accuracy and speed layer for real-time insights, combined through serving layer.","explanation":"## Why Asked\nTests understanding of modern data architecture patterns and handling multiple processing paradigms\n## Key Concepts\nLambda architecture, batch processing, stream processing, data consistency, real-time analytics\n## Code Example\n```\n// Stream processing example (Apache Flink)\nDataStream<Event> stream = env.addSource(kafkaSource);\nstream.window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n      .aggregate(new CountAggregate())\n      .addSink(sink);\n```","diagram":"flowchart TD\n    A[Data Source] --> B[Batch Layer]\n    A --> C[Speed Layer]\n    B --> D[Batch View]\n    C --> E[Real-time View]\n    D --> F[Serving Layer]\n    E --> F\n    F --> G[Analytics/Queries]","difficulty":"beginner","tags":["streaming","kafka"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Netflix","Uber"],"eli5":"Imagine you have two ways to make lemonade at your lemonade stand! One way is making big batches in the morning (that's your batch processing) - you mix everything perfectly and it tastes great. The other way is making fresh cups one by one as friends arrive (that's your streaming) - super fast but maybe not as perfect. The smart trick is having both! You serve the fresh cups right away for instant refreshment, but you also have your perfect batch ready for when someone wants the best-tasting lemonade. Your brain keeps track of both - you know who got fresh cups now and who will get the perfect batch later. That way, everyone gets lemonade exactly when they need it, and you can tell your parents exactly how much you sold today!","relevanceScore":null,"lastUpdated":"2025-12-21T12:43:15.700Z","createdAt":"2025-12-24 12:51:25"},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","answer":"Configure EOS_ALPHA with processing.guarantee=exactly_once_v2, use RocksDB state stores with changelog compaction, enable standby replicas, tune num.stream.threads=cores*2, set cache.max.bytes.buffering=10MB, and monitor consumer lag with Prometheus metrics.","explanation":"## Architecture\n**Exactly-once semantics**: EOS_ALPHA with transactional producers ensures atomic state updates and output commits\n**State management**: RocksDB local state + compacted changelog topics for fast recovery\n**Performance tuning**: Optimize thread pool, buffer sizes, and batch processing for sub-second latency\n\n## NFRs & Calculations\n**Throughput**: 100K events/sec รท 4 cores = 25K events/thread/sec\n**Latency**: Target <500ms with 100ms batch intervals\n**Storage**: 1GB state store รท 10MB cache = 100 cache entries\n**Recovery**: Standby replicas enable <30s failover\n\n## Key Configurations\n```properties\nprocessing.guarantee=exactly_once_v2\nnum.standby.replicas=1\ncache.max.bytes.buffering=10485760\ncommit.interval.ms=100\n```\n\n## Monitoring & Error Handling\n**Metrics**: consumer-lag, stream-latency, state-size\n**Alerts**: lag > 1000 events, latency > 1s\n**Recovery**: Automatic state restoration from changelog with incremental backups","diagram":"flowchart LR\n    A[Producer] --> B[Kafka Topic]\n    B --> C[Kafka Streams App]\n    C --> D[State Store]\n    C --> E[Standby Replica]\n    D --> F[Compact Topic]\n    E --> F\n    C --> G[Output Topic]\n    G --> H[Consumer]\n    I[Traffic Spike] --> C\n    C --> J[Adaptive Processing]\n    J --> K[Scale Out]","difficulty":"advanced","tags":["kafka","flink","kinesis"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T06:39:49.854Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss?","answer":"Use Kafka transactions with idempotent producers + database transaction IDs + offset commits in atomic transaction.","explanation":"## Concept Overview\nExactly-once processing guarantees that each record is processed precisely once, despite failures. This requires coordinating between the source system (Kafka), processing logic, and sink system (database) in a transactional manner.\n\n## Implementation Details\n\n**1. Kafka Producer Configuration**\n- Enable idempotence: `enable.idempotence=true`\n- Set transactional ID: `transactional.id=unique-app-id`\n- Initialize transactions before processing\n\n**2. Database Transaction Management**\n- Use application-level transaction IDs\n- Implement idempotent writes (UPSERT operations)\n- Store processing metadata alongside business data\n\n**3. Atomic Commit Pattern**\n- Process records in database transaction\n- Commit Kafka offsets within same transaction\n- Use `sendOffsetsToTransaction()` API\n\n## Code Example\n```java\n// Kafka producer with transactions\nProperties props = new Properties();\nprops.put(\"enable.idempotence\", \"true\");\nprops.put(\"transactional.id\", \"pipeline-001\");\nKafkaProducer<String, String> producer = new KafkaProducer<>(props);\nproducer.initTransactions();\n\n// Processing loop\nwhile (true) {\n    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));\n    if (!records.isEmpty()) {\n        producer.beginTransaction();\n        \n        try {\n            // Process and store to database with transaction ID\n            for (ConsumerRecord<String, String> record : records) {\n                String txId = UUID.randomUUID().toString();\n                database.upsertWithTxId(record.key(), record.value(), txId);\n            }\n            \n            // Commit offsets atomically\n            producer.sendOffsetsToTransaction(getConsumerOffsets());\n            producer.commitTransaction();\n        } catch (Exception e) {\n            producer.abortTransaction();\n        }\n    }\n}\n```\n\n## Common Pitfalls\n- **Consumer lag**: Transactions increase processing time, monitor consumer lag\n- **Deadlocks**: Ensure consistent ordering of operations\n- **Transaction timeout**: Configure appropriate timeout values\n- **Partial failures**: Handle database rollback when Kafka commit fails","diagram":"graph TD\n    A[Kafka Topic] --> B[Consumer Poll]\n    B --> C[Begin Transaction]\n    C --> D[Process Records]\n    D --> E[Database UPSERT with TxID]\n    E --> F{Success?}\n    F -->|Yes| G[Send Offsets to Transaction]\n    F -->|No| H[Abort Transaction]\n    G --> I[Commit Transaction]\n    H --> J[Retry Processing]\n    I --> K[Next Poll]\n    J --> B","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"channel":"data-engineering","subChannel":"streaming","sourceUrl":"https://confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/","videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn","Netflix","Spotify","Stripe","Twitter","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T08:32:53.754Z","createdAt":"2025-12-24 12:51:26"}],"subChannels":["general","streaming"],"companies":["Airbnb","Amazon","Anthropic","Discord","Google","LinkedIn","Meta","MongoDB","Netflix","Spotify","Stripe","Twitter","Uber"],"stats":{"total":4,"beginner":2,"intermediate":1,"advanced":1,"newThisWeek":4}}