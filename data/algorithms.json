{"questions":[{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","answer":"Choose Linked Lists for frequent insertions/deletions at arbitrary positions (O(1) vs O(n) for arrays) and when memory allocation needs to be dynamic. Arrays excel at O(1) random access, better cache locality (15-30x faster due to spatial locality), and lower memory overhead (no extra pointers). Use Linked Lists in LRU caches, undo/redo systems, or when implementing queues/stacks dynamically.","explanation":"## Time Complexity Trade-offs\n- **Array**: O(1) access, O(n) insert/delete (shift elements)\n- **Linked List**: O(n) access, O(1) insert/delete at known position\n\n## Memory Considerations\n- **Array**: Contiguous memory, cache-friendly, no pointer overhead\n- **Linked List**: Non-contiguous, 8-16 bytes overhead per node (next/prev pointers)\n\n## Real-World Applications\n- **LRU Cache**: Doubly-linked list + hashmap for O(1) operations\n- **Text Editors**: Gap buffers (arrays) vs linked lists for text manipulation\n- **Browser History**: Linked lists for undo/redo functionality\n\n## When to Choose\n- **Array**: Fixed-size datasets, random access needed, memory-constrained environments\n- **Linked List**: Frequent size changes, insert/delete at ends or arbitrary positions\n\n## Performance Impact\nModern CPUs' cache lines (64 bytes) make arrays 20-50x faster for sequential access due to prefetching, making arrays preferable unless O(1) insertions are critical.","diagram":"flowchart TD\n  A[Data Structure Selection] --> B{Frequent Insertions/Deletions?}\n  B -->|Yes| C[Choose Linked List]\n  B -->|No| D{Need O(1) Random Access?}\n  D -->|Yes| E[Choose Array]\n  D -->|No| F{Memory Fragmentation Concern?}\n  F -->|Yes| G[Linked List - Dynamic Memory]\n  F -->|No| H[Array - Contiguous Memory]\n  C --> I[Node: Data + Pointer]\n  E --> J[Fixed Size Elements]\n  G --> K[Non-contiguous Storage]\n  H --> L[Cache-friendly Access]\n  I --> M[O(1) Insert/Delete at Head]\n  J --> N[O(1) Index Access]\n  K --> O[No Pre-allocation Needed]\n  L --> P[Better CPU Cache Performance]","difficulty":"beginner","tags":["struct","comparison","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a train of toy cars connected by hooks. That's a Linked List! If you want to add a new car in the middle, you just unhook two cars and hook the new one in between - super easy! But if you want to find the 5th car, you have to count from the front: 1, 2, 3, 4, 5. Now imagine a row of boxes on a shelf - that's an Array! Finding the 5th box is instant (just look at position 5!), but adding a new box in the middle means pushing all the other boxes over. So: need to add/remove things a lot? Use the train (Linked List). Need to quickly find things by position? Use the shelf (Array)!","relevanceScore":null,"lastUpdated":"2025-12-23T06:13:24.153Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","answer":"Trie provides O(k) prefix search where k is word length, ideal for autocomplete. Space-efficient for common prefixes (e.g., 'pre' shared by 'prefix', 'prefixes'). Hash maps offer O(1) average lookup but can't efficiently find all keys with given prefix. Trie's hierarchical structure enables prefix enumeration, useful for type-ahead suggestions. Trade-offs: higher memory overhead per node, slower for exact matches vs hash maps.","explanation":"## Interview Context\nThis question tests data structure knowledge and trade-off analysis, crucial for optimizing search functionality in applications like autocomplete, spell checkers, and IP routing.\n\n## Key Concepts\n- **Trie Structure**: Tree-like data structure where each node represents a character\n- **Prefix Search**: Efficiently find all words starting with given prefix\n- **Space Complexity**: O(n*m) where n is number of words, m is average word length\n- **Time Complexity**: O(k) for insert/search/startsWith where k is word length\n\n## Implementation Details\n```javascript\nclass TrieNode {\n  constructor() {\n    this.children = {};\n    this.isEndOfWord = false;\n  }\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode();\n  }\n  \n  insert(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) {\n        node.children[char] = new TrieNode();\n      }\n      node = node.children[char];\n    }\n    node.isEndOfWord = true;\n  }\n  \n  search(word) {\n    let node = this.root;\n    for (let char of word) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return node.isEndOfWord;\n  }\n  \n  startsWith(prefix) {\n    let node = this.root;\n    for (let char of prefix) {\n      if (!node.children[char]) return false;\n      node = node.children[char];\n    }\n    return true;\n  }\n}\n```\n\n## Trade-offs vs Hash Maps\n- **Trie Advantages**: Prefix search O(k), space efficiency for shared prefixes, ordered traversal\n- **Hash Map Advantages**: Lower memory overhead, simpler implementation, O(1) average exact lookup\n- **Use Cases**: Tries for autocomplete/suggestions, hash maps for exact key-value storage\n\n## Follow-up Questions\n1. How would you implement delete operation in a Trie?\n2. What optimizations would you apply for a large-scale autocomplete system?\n3. How would you handle Unicode characters and different languages in a Trie?","diagram":"graph TD\n    A[Root] --> A1[a] --> P1[pp] --> P2[p] --> P3[l] --> E1[apple]\n    A --> A2[a] --> P4[pp] --> P5[l] --> E2[apply]\n    A --> A3[a] --> P6[p] --> P7[l] --> E3[application]\n    A --> A4[a] --> P8[pp] --> P9[l] --> E4[approach]\n    \n    style A fill:#FF6B6B\n    style E1 fill:#4ECDC4\n    style E2 fill:#4ECDC4\n    style E3 fill:#4ECDC4\n    style E4 fill:#95E1D3","difficulty":"intermediate","tags":["struct","basics"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine building with LEGO blocks! A trie is like organizing your LEGOs by color first, then shape. When you want to find all the red blocks, you just go to the red box - instant! A hash map is like tossing all LEGOs in one big toy chest and having to dig through everything to find what you want. Tries are super fast because they share the first parts of words, just like how 'cat' and 'car' both start with 'c'. It's like having a shortcut that saves you time and space!","relevanceScore":null,"lastUpdated":"2025-12-22T09:50:41.974Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","answer":"Use HashMap for O(1) key lookup and DoublyLinkedList for O(1) insertion/removal. Synchronize access with ReentrantReadWriteLock for thread safety.","explanation":"## Concept Overview\nLRU (Least Recently Used) cache combines HashMap for fast key lookup and DoublyLinkedList for maintaining access order. When cache is full, least recently used items are evicted.\n\n## Implementation Details\n- **HashMap**: Maps keys to Node references for O(1) lookup\n- **DoublyLinkedList**: Maintains access order with head (most recent) and tail (least recent)\n- **Thread Safety**: Use ReentrantReadWriteLock - read lock for get(), write lock for put()\n- **Eviction**: Remove tail node when capacity exceeded\n\n## Code Example\n```java\npublic class LRUCache<K, V> {\n    private final Map<K, Node<K, V>> map;\n    private final DoublyLinkedList<K, V> list;\n    private final int capacity;\n    private final ReadWriteLock lock = new ReentrantReadWriteLock();\n    \n    public V get(K key) {\n        lock.readLock().lock();\n        try {\n            Node<K, V> node = map.get(key);\n            if (node != null) {\n                list.moveToHead(node);\n                return node.value;\n            }\n            return null;\n        } finally {\n            lock.readLock().unlock();\n        }\n    }\n}\n```\n\n## Common Pitfalls\n- **Race Conditions**: Forgetting proper synchronization can corrupt the linked list\n- **Memory Leaks**: Not removing references when evicting nodes\n- **Performance**: Using synchronized blocks instead of ReadWriteLock reduces concurrency\n- **Edge Cases**: Handling null keys/values and capacity of 0","diagram":"graph TD\n    A[Client Request] --> B{Operation?}\n    B -->|get| C[Read Lock]\n    B -->|put| D[Write Lock]\n    C --> E[HashMap Lookup]\n    E --> F{Key Exists?}\n    F -->|Yes| G[Move to Head]\n    F -->|No| H[Return Null]\n    G --> I[Return Value]\n    D --> J{Key Exists?}\n    J -->|Yes| K[Update Value]\n    J -->|No| L[Create New Node]\n    K --> M[Move to Head]\n    L --> M\n    M --> N{Capacity Full?}\n    N -->|Yes| O[Remove Tail]\n    N -->|No| P[Add to Head]\n    O --> P\n    P --> Q[Release Lock]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-21T12:44:27.553Z","createdAt":"2025-12-23 12:53:08"},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time?","answer":"Use array representation where parent at i has children at 2i+1 and 2i+2, bubble up/down to maintain heap property.","explanation":"## Why This Is Asked\nTests understanding of heap data structure, array manipulation, and time complexity analysis - fundamental for Zoom's real-time features.\n\n## Expected Answer\nCandidate should explain heap property (parent ≤ children), array indexing, and implement bubble-up and bubble-down operations.\n\n## Code Example\n```python\nclass MinHeap:\n    def __init__(self):\n        self.heap = []\n    \n    def insert(self, val):\n        self.heap.append(val)\n        self._bubble_up(len(self.heap) - 1)\n    \n    def extractMin(self):\n        if not self.heap: return None\n        min_val = self.heap[0]\n        self.heap[0] = self.heap[-1]\n        self.heap.pop()\n        self._bubble_down(0)\n        return min_val\n    \n    def peek(self):\n        return self.heap[0] if self.heap else None\n    \n    def _bubble_up(self, idx):\n        while idx > 0:\n            parent = (idx - 1) // 2\n            if self.heap[parent] <= self.heap[idx]:\n                break\n            self.heap[parent], self.heap[idx] = self.heap[idx], self.heap[parent]\n            idx = parent\n    \n    def _bubble_down(self, idx):\n        while True:\n            left = 2 * idx + 1\n            right = 2 * idx + 2\n            smallest = idx\n            \n            if left < len(self.heap) and self.heap[left] < self.heap[smallest]:\n                smallest = left\n            if right < len(self.heap) and self.heap[right] < self.heap[smallest]:\n                smallest = right\n            \n            if smallest == idx:\n                break\n            \n            self.heap[idx], self.heap[smallest] = self.heap[smallest], self.heap[idx]\n            idx = smallest\n```\n\n## Follow-up Questions\n- How would you implement a max-heap using the same structure?\n- What's the time complexity of building a heap from an unsorted array?\n- How would you handle duplicate values in the heap?","diagram":"flowchart TD\n    A[Insert Value] --> B[Add to End of Array]\n    B --> C[Bubble Up: Compare with Parent]\n    C -->|Parent > Child| D[Swap Positions]\n    D --> E[Continue Bubbling Up]\n    C -->|Parent ≤ Child| F[Heap Property Satisfied]\n    G[ExtractMin] --> H[Replace Root with Last Element]\n    H --> I[Bubble Down: Compare with Children]\n    I -->|Child < Parent| J[Swap with Smaller Child]\n    J --> K[Continue Bubbling Down]\n    I -->|Parent ≤ Children| L[Heap Property Restored]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=HCEr35qpawQ","longVideo":"https://www.youtube.com/watch?v=RBSGKlAvoiM"},"companies":["Cisco","Unity","Zoom"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:10:40.694Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","answer":"Use a sliding window with a hashmap for error counts and a min-heap of size K for top K tracking. As new events arrive, increment counts and update heap. For expired events, decrement counts and remove from heap if count drops below heap minimum. Time complexity: O(log K) per event, Space: O(K).","explanation":"## Algorithm\n- Maintain hashmap: error_message → count within window\n- Use min-heap (size K) storing (count, error_message) pairs\n- For each event: update hashmap, then update heap if needed\n- For expired events: decrement hashmap, adjust heap if count changes\n\n## Complexity Analysis\n- **Time**: O(log K) per event (heap operations)\n- **Space**: O(K) for heap + O(M) for hashmap where M = unique errors in window\n\n## Edge Cases\n- Handle ties in error frequency (lexicographic ordering)\n- Empty window or K > available errors\n- High-frequency events causing heap churn\n\n## Follow-up Questions\n1. How would you handle distributed logs across multiple servers?\n2. What if N is very large (days/weeks) - how to optimize memory?\n3. How to implement this with exact counts vs approximate counting?","diagram":"flowchart TD\n    A[Log Event Stream] --> B[Sliding Window Queue]\n    B --> C[Frequency Hash Map]\n    C --> D{Heap Size < K?}\n    D -->|Yes| E[Push to Min Heap]\n    D -->|No| F{Freq > Heap Min?}\n    F -->|Yes| G[Replace Heap Min]\n    F -->|No| H[Discard]\n    E --> I[Top K Error Messages]\n    G --> I\n    H --> I\n    B --> J{Event Expired?}\n    J -->|Yes| K[Remove from Queue]\n    J -->|No| C\n    K --> L[Decrement Frequency]\n    L --> C","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T16:44:28.268Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree?","answer":"Use a segment tree where each node stores sum of its range, enabling O(log n) point updates and range sum queries.","explanation":"## Core Concept\nA segment tree is a binary tree where each node represents a range of the array and stores aggregate data (sum in this case). The root represents the entire array, leaves represent individual elements.\n\n## Implementation Details\n- Build tree in O(n) time by recursively computing sums\n- Point update: traverse from leaf to root, updating O(log n) nodes\n- Range query: decompose query range into O(log n) tree nodes\n- Use array representation with 4*n size for efficiency\n\n## Key Operations\n- `build(node, start, end)`: Construct tree recursively\n- `update(node, start, end, idx, val)`: Update single element\n- `query(node, start, end, l, r)`: Get range sum\n\n## Edge Cases\n- Handle empty ranges and invalid indices\n- Consider lazy propagation for range updates\n- Manage integer overflow for large sums","diagram":"flowchart TD\n  A[Root: Sum[0-7]] --> B[Left: Sum[0-3]]\n  A --> C[Right: Sum[4-7]]\n  B --> D[Left: Sum[0-1]]\n  B --> E[Right: Sum[2-3]]\n  C --> F[Left: Sum[4-5]]\n  C --> G[Right: Sum[6-7]]\n  D --> H[Leaf: arr[0]]\n  D --> I[Leaf: arr[1]]\n  E --> J[Leaf: arr[2]]\n  E --> K[Leaf: arr[3]]","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:40:19.599Z","createdAt":"2025-12-23 12:53:10"},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","answer":"Use a hash map to store numbers and their indices while iterating. For each number, check if target - current exists in the map. This gives O(n) time and O(n) space, better than the O(n²) brute force ","explanation":"## Problem\nFind two numbers in an array that sum to a target value.\n\n## Approach\nUse a hash map to track seen numbers and their indices.\n\n## Algorithm\n1. Initialize empty hash map\n2. Iterate through array\n3. For each number, calculate complement (target - current)\n4. Check if complement exists in map\n5. If found, return indices\n6. Otherwise, store current number in map\n\n## Complexity\n- Time: O(n) - single pass through array\n- Space: O(n) - hash map storage\n\n## Edge Cases\n- Handle duplicate numbers\n- Empty array or single element\n- Multiple valid pairs","diagram":"flowchart TD\n  A[Start] --> B[Initialize hash map]\n  B --> C[Iterate array]\n  C --> D[Calculate complement]\n  D --> E{Complement exists?}\n  E -->|Yes| F[Return indices]\n  E -->|No| G[Store number in map]\n  G --> H{More elements?}\n  H -->|Yes| C\n  H -->|No| I[No solution found]","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Amazon","Two Sigma"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T01:14:16.074Z","createdAt":"2025-12-23 12:53:10"},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","answer":"Use a sliding window with a deque for timestamps and a hash map for frequency counts. Maintain a max-heap for top K queries. For each action, update frequency map and remove expired entries from deque","explanation":"## Solution Overview\nImplement a time-bounded frequency counter with efficient queries.\n\n## Data Structures\n- **Hash Map**: Action → frequency count\n- **Deque**: Store timestamps for sliding window\n- **Max-Heap**: (frequency, action) for top K queries\n\n## Algorithm\n1. For each action, increment frequency in hash map\n2. Add timestamp to deque, remove expired entries\n3. When querying, build heap from frequency map\n4. Extract top K elements from heap\n\n## Complexity\n- **Update**: O(1) amortized\n- **Query**: O(K log K)\n- **Space**: O(U) where U = unique actions","diagram":"flowchart TD\n  A[New Action] --> B[Update Frequency Map]\n  B --> C[Add Timestamp to Deque]\n  C --> D[Remove Expired Entries]\n  D --> E[Query Request]\n  E --> F[Build Max-Heap]\n  F --> G[Extract Top K]","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"channel":"algorithms","subChannel":"data-structures","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","OpenAI"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T01:12:16.348Z","createdAt":"2025-12-24T01:12:16.348Z"},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space complexity.","answer":"Use DP with 3 variables tracking last 3 positions. dp[i] = dp[i-1] + dp[i-2] + dp[i-3]. Base: dp[0]=1, dp[1]=1, dp[2]=2","explanation":"## Approach\n\nThis is a classic dynamic programming problem similar to climbing stairs, but with three possible steps instead of two.\n\n## Solution\n\n### Recurrence Relation\n\nFor each step `i`, the number of ways to reach it is the sum of ways to reach the previous three steps:\n\n```\ndp[i] = dp[i-1] + dp[i-2] + dp[i-3]\n```\n\n### Base Cases\n\n- `dp[0] = 1` (one way to stay at ground)\n- `dp[1] = 1` (one step)\n- `dp[2] = 2` (1+1 or 2)\n\n### Space Optimization\n\nInstead of maintaining an array of size n, we only need three variables to track the last three positions:\n\n```python\ndef climbStairs(n):\n    if n <= 2:\n        return n if n > 0 else 1\n    \n    a, b, c = 1, 1, 2  # dp[0], dp[1], dp[2]\n    \n    for i in range(3, n + 1):\n        current = a + b + c\n        a, b, c = b, c, current\n    \n    return c\n```\n\n## Complexity\n\n- **Time**: O(n) - single pass through n steps\n- **Space**: O(1) - only three variables used\n\n## Example\n\nFor n=4:\n- dp[3] = dp[2] + dp[1] + dp[0] = 2 + 1 + 1 = 4\n- dp[4] = dp[3] + dp[2] + dp[1] = 4 + 2 + 1 = 7\n\nThere are 7 distinct ways to climb 4 steps.","diagram":"graph TD\n    A[\"n=4 (target)\"] --> B[\"n=3 (4 ways)\"]\n    A --> C[\"n=2 (2 ways)\"]\n    A --> D[\"n=1 (1 way)\"]\n    B --> E[\"n=2 (2 ways)\"]\n    B --> F[\"n=1 (1 way)\"]\n    B --> G[\"n=0 (1 way)\"]\n    C --> H[\"n=1 (1 way)\"]\n    C --> I[\"n=0 (1 way)\"]\n    D --> J[\"n=0 (1 way)\"]\n    \n    style A fill:#ff6b6b\n    style B fill:#4ecdc4\n    style C fill:#4ecdc4\n    style D fill:#4ecdc4\n    style E fill:#95e1d3\n    style F fill:#95e1d3\n    style G fill:#95e1d3\n    style H fill:#95e1d3\n    style I fill:#95e1d3\n    style J fill:#95e1d3","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=OW2-fcoFNps","longVideo":"https://www.youtube.com/watch?v=_i4Yxeh5ceQ"},"companies":["Adobe","Amazon","Goldman Sachs","Google","Meta"],"eli5":"Imagine you're climbing a staircase to reach a cookie jar! You can hop 1 step, 2 steps, or 3 steps at a time. To find out how many ways you can get to the top, think about it like playing with building blocks. If you're on step 1, there's only 1 way to get there (1 small hop). From step 2, you can either do two small hops or one big jump - that's 2 ways! For any step, you can come from the step right below (1 hop), two steps below (2 hops), or three steps below (3 hops). So just add up all the ways you could have reached those three previous steps! It's like counting all the different paths to get to your cookie - each step remembers the ways you could have arrived from the three steps before it.","relevanceScore":null,"lastUpdated":"2025-12-21T13:00:03.635Z","createdAt":"2025-12-23 12:53:08"},{"id":"al-164","question":"Given an array of positive integers and a target sum, how many distinct ways can you reach the target using unlimited repetitions of array elements? Return the count modulo 10^9+7.","answer":"Use DP with unbounded knapsack: dp[i] = sum(dp[i - nums[j]]) % MOD. Initialize dp[0] = 1. Iterate from 1 to target, for each i sum all dp[i - num] where num ≤ i. Time O(target×n), space O(target). Handle large results with modulo arithmetic.","explanation":"## Problem\nThis is a classic unbounded knapsack/counting problem where we need to find the number of combinations that sum to a target using unlimited repetitions.\n\n## Approach\nDynamic programming with 1D array where dp[i] represents ways to reach sum i. For each target value, we sum contributions from all valid previous states.\n\n## Code Example\n```cpp\nint combinationSum4(vector<int>& nums, int target) {\n    const int MOD = 1e9 + 7;\n    vector<long long> dp(target + 1, 0);\n    dp[0] = 1;\n    \n    for (int i = 1; i <= target; i++) {\n        for (int num : nums) {\n            if (i >= num) {\n                dp[i] = (dp[i] + dp[i - num]) % MOD;\n            }\n        }\n    }\n    return dp[target];\n}\n```\n\n## Complexity\n- Time: O(n × target) where n is array size\n- Space: O(target) for DP array\n\n## Edge Cases\n- Empty array → 0 ways\n- Target 0 → 1 way (empty combination)\n- Large results → use modulo arithmetic\n\n## Follow-up Questions\n1. How would you modify this if order matters vs doesn't matter?\n2. What if we limit the number of repetitions for each element?\n3. How would you optimize space further for large targets?","diagram":"flowchart TD\n    A[Start] --> B[Initialize dp[0] = 1]\n    B --> C[For each sum i from 1 to target]\n    C --> D[For each num in array]\n    D --> E{i >= num?}\n    E -->|Yes| F[dp[i] += dp[i - num]]\n    E -->|No| G[Skip]\n    F --> H[Next num]\n    G --> H\n    H --> I{More nums?}\n    I -->|Yes| D\n    I -->|No| J{More sums?}\n    J -->|Yes| C\n    J -->|No| K[Return dp[target]]","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have different toy blocks with numbers on them, and you want to build a tower exactly 10 units tall using these blocks. You can use any block as many times as you want! Like if you have blocks numbered 2, 3, and 5, you could use five 2-blocks, or two 3-blocks plus one 4-block, or one 5-block plus one 3-block plus one 2-block. The secret is to start small - first figure out how to make 1, then 2, then 3... Each time you want to make a new height, you just look at what blocks you have and add them to towers you already know how to build. It's like building with LEGOs - you start with what you know, then keep adding new pieces to reach bigger goals!","relevanceScore":null,"lastUpdated":"2025-12-22T09:52:11.777Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","answer":"Use DP where dp[i][j] represents minimum cost for substring s[i..j]. If s[i]==s[j], dp[i][j]=dp[i+1][j-1]. Otherwise dp[i][j]=min(dp[i+1][j]+1(delete s[i]), dp[i][j-1]+2(insert s[j+1])). Time O(n²), space O(n²) or optimized to O(n). This compares to LCS-based solution with O(n²) time but directly optimizes for the given cost structure.","explanation":"## Problem Context\nThis tests dynamic programming skills for string manipulation, commonly found in coding interviews at FAANG companies.\n\n## Solution Approach\nThe recurrence considers three operations: delete left char, insert matching char for left, or insert+delete both ends. Base cases: dp[i][i]=0 (single char), dp[i][i+1]=0 if equal else min(insert_cost, delete_cost).\n\n## Code Implementation\n```python\ndef min_palindrome_cost(s, insert_cost=2, delete_cost=1):\n    n = len(s)\n    dp = [[0] * n for _ in range(n)]\n    \n    for length in range(2, n+1):\n        for i in range(n-length+1):\n            j = i + length - 1\n            if s[i] == s[j]:\n                dp[i][j] = dp[i+1][j-1]\n            else:\n                dp[i][j] = min(\n                    dp[i+1][j] + delete_cost,\n                    dp[i][j-1] + insert_cost,\n                    dp[i+1][j-1] + insert_cost + delete_cost\n                )\n    return dp[0][n-1]\n```\n\n## Complexity Analysis\n- Time: O(n²) where n is string length\n- Space: O(n²) or O(n) with rolling optimization\n\n## Follow-up Questions\n- How would you modify this for substitution operations?\n- Can you reconstruct the actual transformation sequence?\n- How does this relate to the edit distance problem?","diagram":"graph TD\n    A[Start dp[i][j]] --> B{s[i] == s[j]}\n    B -->|Yes| C[dp[i+1][j-1]]\n    B -->|No| D[1 + min(dp[i+1][j-1], dp[i+1][j], dp[i][j-1])]\n    C --> E[Return dp[i][j]]\n    D --> E","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a row of colorful blocks with letters on them. You want to make the row look the same forwards and backwards - like a mirror! Each time you add a new block (insertion), it costs 2 coins. Each time you take away a block (deletion), it costs 1 coin. You look at your blocks from both ends and think: 'What's the cheapest way to make them match?' Sometimes you add blocks, sometimes you remove them. You keep checking smaller and smaller pieces until your whole row becomes a perfect mirror! It's like solving a puzzle where you want to spend the fewest coins to make everything symmetrical and pretty.","relevanceScore":null,"lastUpdated":"2025-12-22T10:01:56.823Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","answer":"Use sliding window DP: dp[i] = sum(dp[i-1] to dp[i-6]) % MOD. Maintain circular buffer of size 6 for O(1) space. Base case dp[0] = 1. Time O(n), Space O(1). Handle i<6 boundary conditions by limiting window range.","explanation":"## Problem\nCount ways to reach sum n using dice rolls (1-6), return modulo 10^9+7.\n\n## Approach\n**Sliding Window DP**: Each state depends on previous 6 states. Instead of O(n) array, use circular buffer of size 6.\n\n## Algorithm\n```python\ndef diceWays(n):\n    MOD = 10**9 + 7\n    dp = [0] * 6\n    dp[0] = 1  # dp[0] = 1\n    \n    for i in range(1, n + 1):\n        curr = sum(dp) % MOD\n        dp[i % 6] = curr\n    \n    return dp[n % 6]\n```\n\n## Complexity\n- **Time**: O(n) - single pass\n- **Space**: O(1) - constant 6-element buffer\n\n## Edge Cases\n- n = 0: return 1 (empty sequence)\n- n < 0: return 0\n- Large n: handle modulo overflow\n\n## Follow-up Questions\n1. How would you extend this to k-sided dice?\n2. Can you optimize further using matrix exponentiation?\n3. What if we needed to count sequences with exactly m rolls?","diagram":"flowchart TD\n  A[Start] --> B[Initialize dp[0]=1]\n  B --> C[For each i from 1 to target]\n  C --> D[Sum dp[i-die] for die=1-6]\n  D --> E[Store in dp[i]]\n  E --> F{i <= target?}\n  F -->|Yes| C\n  F -->|No| G[Return dp[target]]\n  G --> H[End]","difficulty":"intermediate","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a staircase with 10 steps and you want to climb to the top! You can take 1, 2, 3, 4, 5, or 6 steps at a time - just like rolling a dice! To find how many ways to reach step 10, we start from the bottom. There's exactly 1 way to stay at step 0 (do nothing!). For each step, we count all the ways we could have arrived there from any of the 6 steps before it. Like, to reach step 5, you could have come from step 4 (taking 1 step), OR from step 3 (taking 2 steps), OR from step 2, 1, or even step -1 (which doesn't exist, so we skip!). We add up all these possibilities. By the time we reach step 10, we'll know every possible combination of dice rolls that gets us there!","relevanceScore":null,"lastUpdated":"2025-12-22T09:56:32.303Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","answer":"Use greedy approach tracking current jump range and furthest reachable position. Iterate once, increment jumps when reaching current range end, update furthest position. Handle unreachable case by checking if furthest position doesn't advance. O(n) time, O(1) space.","explanation":"## Problem\nGiven an array where each element represents maximum jump length from that position, find minimum jumps to reach the last index.\n\n## Approach\n**Greedy Algorithm**: Track current jump range and furthest reachable position. Single pass O(n) solution.\n\n## Implementation\n```python\ndef minJumps(nums):\n    if len(nums) <= 1: return 0\n    jumps, cur_end, furthest = 0, 0, 0\n    \n    for i in range(len(nums)-1):\n        furthest = max(furthest, i + nums[i])\n        if i == cur_end:\n            jumps += 1\n            cur_end = furthest\n            if cur_end >= len(nums)-1:\n                break\n    return jumps if cur_end >= len(nums)-1 else -1\n```\n\n## Complexity\n- Time: O(n) - single pass\n- Space: O(1) - constant extra space\n\n## Edge Cases\n- Empty array: return 0\n- Single element: return 0\n- Unreachable: return -1\n\n## Follow-up Questions\n1. How would you modify this to return the actual jump path?\n2. What if we need to find all possible minimum jump paths?\n3. How does this change if we can only jump exactly the specified number of steps?","diagram":"graph TD\n    A[Start at index 0] --> B[Initialize jumps=0, end=0, furthest=0]\n    B --> C[Iterate through array]\n    C --> D{Reached end of current range?}\n    D -->|Yes| E[Increment jumps, set end=furthest]\n    D -->|No| F[Update furthest = max(furthest, i + nums[i])]\n    E --> F\n    F --> G{Can reach last index?}\n    G -->|Yes| H[Return jumps]\n    G -->|No| I[Continue to next position]\n    I --> C\n    H --> J[End]\n    C --> K{i >= n-1?}\n    K -->|Yes| H\n    K -->|No| D","difficulty":"advanced","tags":["dp","optimization"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you're playing hopscotch on a sidewalk. Each square tells you how many squares you can jump ahead. You want to reach the end with the fewest jumps! Look at all squares you can reach from your current spot. Pick the one that lets you jump farthest next. Keep doing this until you reach the end. If you find a spot where you can't jump forward at all, you're stuck - it's impossible! Like choosing the best stepping stones to cross a river with the fewest steps.","relevanceScore":null,"lastUpdated":"2025-12-22T09:54:30.198Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","answer":"DP optimizes recursion by storing results (memoization/tabulation) to avoid redundant computations, trading space for time.","explanation":"## Why Asked\nTests understanding of algorithmic optimization and problem-solving approaches\n## Key Concepts\n- Overlapping subproblems\n- Optimal substructure\n- Memoization vs tabulation\n- Time/space complexity trade-offs\n## Code Example\n```\n// Recursion: O(2^n)\nfunction fib(n) {\n  if (n <= 1) return n;\n  return fib(n-1) + fib(n-2);\n}\n\n// DP: O(n)\nfunction fibDP(n) {\n  const dp = [0, 1];\n  for (let i = 2; i <= n; i++) {\n    dp[i] = dp[i-1] + dp[i-2];\n  }\n  return dp[n];\n}\n```\n## Follow-up Questions\n- When would you use memoization vs tabulation?\n- What's the space complexity of bottom-up DP?\n- Can every recursive solution be converted to DP?","diagram":"flowchart TD\n  A[Problem] --> B{Has overlapping subproblems?}\n  B -->|No| C[Use plain recursion]\n  B -->|Yes| D{Has optimal substructure?}\n  D -->|No| E[Use other approaches]\n  D -->|Yes| F[Use Dynamic Programming]\n  F --> G{Implementation choice}\n  G --> H[Top-down memoization]\n  G --> I[Bottom-up tabulation]","difficulty":"advanced","tags":["dp","optimization","theory"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=oBt53YbR9Kk","longVideo":"https://youtube.com/watch?v=oBt53YbR9Kk"},"companies":["Amazon","Google","Meta"],"eli5":"Imagine you're building with LEGOs and need to make the same tower shape over and over. Plain recursion is like taking apart your tower each time and rebuilding it from scratch - super tiring! Dynamic programming is like taking a picture of each tower shape you build. When you need that shape again, you just look at your picture instead of rebuilding. You use more space to store your pictures, but you save lots of time and energy. Choose plain recursion for small, one-time projects. Choose dynamic programming when you'll need the same answers many times - like when building a big LEGO castle with lots of repeated sections!","relevanceScore":null,"lastUpdated":"2025-12-22T08:33:36.811Z","createdAt":"2025-12-23 12:53:08"},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","answer":"Use DP with tabulation: O(mn) time, O(mn) space. Optimize to O(n) space using rolling array. For path reconstruction, maintain parent pointers or backtrack from DP table. Handle edge cases like empty grid, single cell, and large values with 64-bit integers.","explanation":"## Approach\nThis is a classic dynamic programming problem where we build the solution bottom-up. The key insight is that the minimum cost to reach any cell depends only on the minimum costs to reach the cell above and the cell to the left.\n\n## Algorithm\n1. Initialize DP table with same dimensions as grid\n2. dp[0][0] = grid[0][0] (starting point)\n3. Fill first row: dp[0][j] = dp[0][j-1] + grid[0][j]\n4. Fill first column: dp[i][0] = dp[i-1][0] + grid[i][0]\n5. For remaining cells: dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]\n6. Path reconstruction by backtracking from bottom-right\n\n## Complexity\n- Time: O(mn) - we visit each cell once\n- Space: O(mn) for DP table, can optimize to O(n) using rolling array\n\n## Code Example\n```python\ndef minPathSum(grid):\n    if not grid or not grid[0]:\n        return 0, []\n    \n    m, n = len(grid), len(grid[0])\n    dp = [[0] * n for _ in range(m)]\n    parent = [[None] * n for _ in range(m)]\n    \n    dp[0][0] = grid[0][0]\n    \n    # Fill first row and column\n    for j in range(1, n):\n        dp[0][j] = dp[0][j-1] + grid[0][j]\n        parent[0][j] = (0, j-1)\n    \n    for i in range(1, m):\n        dp[i][0] = dp[i-1][0] + grid[i][0]\n        parent[i][0] = (i-1, 0)\n    \n    # Fill rest of table\n    for i in range(1, m):\n        for j in range(1, n):\n            if dp[i-1][j] < dp[i][j-1]:\n                dp[i][j] = dp[i-1][j] + grid[i][j]\n                parent[i][j] = (i-1, j)\n            else:\n                dp[i][j] = dp[i][j-1] + grid[i][j]\n                parent[i][j] = (i, j-1)\n    \n    # Reconstruct path\n    path = []\n    i, j = m-1, n-1\n    while i >= 0 and j >= 0:\n        path.append((i, j))\n        if parent[i][j]:\n            i, j = parent[i][j]\n        else:\n            break\n    \n    return dp[m-1][n-1], list(reversed(path))\n```\n\n## Follow-up Questions\n- How would you modify this solution if you could also move diagonally?\n- What if we needed to find the k-th minimum path instead of the absolute minimum?\n- How would you handle negative values in the grid?\n- Can you optimize space further using in-place modification?\n- How would this change if we could move in all four directions (need to detect cycles)?","diagram":"flowchart TD\n  A[Start at 0,0] --> B[Initialize DP array]\n  B --> C[Iterate through grid]\n  C --> D[Apply recurrence relation]\n  D --> E[Update DP array]\n  E --> F{Reached end?}\n  F -->|No| C\n  F -->|Yes| G[Return min cost]","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=oFkDldu3C_4"},"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T16:34:23.885Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","answer":"Use DP with memoization: recursively try all word prefixes, cache results for substrings. Time O(n³) worst case, space O(n²) for memo table. Backtrack to reconstruct all valid sentences from DP table.","explanation":"## Approach\n- Build DP table where dp[i] stores all valid sentences for substring s[i:]\n- For each position i, try all words in dictionary that match s[i:]\n- Recursively compute dp[i + word.length] and prepend word\n- Memoize results to avoid recomputation\n\n## Complexity\n- Time: O(n³) in worst case (n positions × n word lengths × n combinations)\n- Space: O(n²) for DP table + recursion stack\n\n## Implementation\n```python\ndef wordBreak(s, wordDict):\n    word_set = set(wordDict)\n    memo = {}\n    \n    def dfs(start):\n        if start in memo:\n            return memo[start]\n        if start == len(s):\n            return [\"\"]\n            \n        sentences = []\n        for end in range(start + 1, len(s) + 1):\n            word = s[start:end]\n            if word in word_set:\n                for suffix in dfs(end):\n                    sentence = word + (\"\" if suffix == \"\" else \" \" + suffix)\n                    sentences.append(sentence)\n        \n        memo[start] = sentences\n        return sentences\n    \n    return dfs(0)\n```","diagram":"flowchart TD\n  A[Start at index 0] --> B[Try all possible words]\n  B --> C{Word matches prefix?}\n  C -->|Yes| D[Recurse on remaining substring]\n  C -->|No| E[Skip to next word]\n  D --> F[Combine word with sub-sentences]\n  F --> G[Cache result for current index]\n  G --> H[Return all valid sentences]","difficulty":"advanced","tags":["dp","memoization","tabulation"],"channel":"algorithms","subChannel":"dynamic-programming","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T15:24:11.517Z","createdAt":"2025-12-23T15:24:11.517Z"},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","answer":"Use a dynamic Dijkstra variant with incremental updates and hierarchical decomposition, maintaining O(log n) per update and query.","explanation":"## Concept Overview\nDynamic shortest path requires handling frequent edge weight updates while maintaining fast query responses. Traditional Dijkstra's O(E + V log V) is too slow for production scale.\n\n## Implementation Details\n- **Hierarchical Decomposition**: Partition graph into clusters using METIS or custom partitioning\n- **Multi-level Indexing**: Maintain precomputed distances between cluster boundaries\n- **Incremental Updates**: Use dynamic programming to update only affected paths\n- **Lazy Recomputation**: Defer full recomputation until query performance degrades\n\n## Code Structure\n```python\nclass DynamicShortestPath:\n    def __init__(self, graph):\n        self.clusters = self.partition_graph(graph)\n        self.cluster_distances = self.precompute_inter_cluster()\n        self.local_paths = {c: {} for c in self.clusters}\n    \n    def update_edge(self, u, v, new_weight):\n        cluster = self.get_cluster(u)\n        self.invalidate_local_paths(cluster, u, v)\n        self.update_inter_cluster_if_boundary(u, v, new_weight)\n    \n    def query(self, source, target):\n        return self.bidirectional_dijkstra(source, target)\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Hierarchical structures can consume 3-5x memory\n- **Partition Quality**: Poor clustering leads to frequent cross-cluster queries\n- **Update Cascades**: Edge updates can trigger expensive recomputation cascades\n- **Concurrency**: Thread-safe updates require careful locking strategies","diagram":"graph TD\n    A[Client Query] --> B{Source/Target in Same Cluster?}\n    B -->|Yes| C[Local Dijkstra]\n    B -->|No| D[Multi-level Path Search]\n    D --> E[Cluster Boundary Search]\n    E --> F[Inter-cluster Distance Lookup]\n    F --> G[Local Path Assembly]\n    G --> H[Return Result]\n    I[Edge Update] --> J{Boundary Edge?}\n    J -->|Yes| K[Update Inter-cluster Index]\n    J -->|No| L[Invalidate Local Cache]\n    K --> M[Mark Affected Clusters]\n    L --> M","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T04:56:47.926Z","createdAt":"2025-12-23 12:53:08"},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","answer":"BFS explores level by level, finds shortest path; DFS goes deep first, uses less memory. BFS for shortest paths, DFS for exhaustive search.","explanation":"## Why Asked\nTests understanding of graph traversal fundamentals and algorithm selection\n## Key Concepts\nQueue vs stack implementation, time complexity O(V+E), space complexity differences, use cases\n## Code Example\n```\n// BFS\nfunction bfs(graph, start) {\n  const queue = [start];\n  const visited = new Set();\n  while (queue.length) {\n    const node = queue.shift();\n    if (!visited.has(node)) {\n      visited.add(node);\n      queue.push(...graph[node]);\n    }\n  }\n}\n\n// DFS\nfunction dfs(graph, start) {\n  const stack = [start];\n  const visited = new Set();\n  while (stack.length) {\n    const node = stack.pop();\n    if (!visited.has(node)) {\n      visited.add(node);\n      stack.push(...graph[node]);\n    }\n  }\n}\n```\n## Follow-up Questions\nHow to implement recursively? What about cycle detection? Time vs space trade-offs?","diagram":"flowchart TD\n  A[Graph Traversal] --> B[BFS: Queue-based]\n  A --> C[DFS: Stack-based]\n  B --> D[Level order]\n  B --> E[Shortest path]\n  C --> F[Deep exploration]\n  C --> G[Memory efficient]","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T08:36:13.236Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","answer":"Use BFS with queue, track visited nodes and distances, return distance when target found or -1 after exploring all reachable nodes.","explanation":"## Why This Is Asked\nTests fundamental graph traversal, BFS implementation, and pathfinding - critical for autonomous vehicle navigation and route planning.\n\n## Expected Answer\nCandidate should implement BFS with queue, visited set, distance tracking, handle edge cases like start=target, disconnected graphs, and explain why BFS guarantees shortest path in unweighted graphs.\n\n## Code Example\n```python\nfrom collections import deque\n\ndef shortest_path(graph, start, end):\n    if start == end:\n        return 0\n    \n    queue = deque([(start, 0)])\n    visited = {start}\n    \n    while queue:\n        node, dist = queue.popleft()\n        for neighbor in graph.get(node, []):\n            if neighbor == end:\n                return dist + 1\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append((neighbor, dist + 1))\n    \n    return -1\n```\n\n## Follow-up Questions\n- How would you modify this for weighted edges?\n- What's the time/space complexity?\n- How would you handle cycles differently?","diagram":"flowchart TD\n  A[Start BFS] --> B[Initialize queue with start]\n  B --> C[Mark start as visited]\n  C --> D[Queue empty?]\n  D -->|Yes| E[Return -1]\n  D -->|No| F[Dequeue node]\n  F --> G[Node equals target?]\n  G -->|Yes| H[Return distance]\n  G -->|No| I[Explore neighbors]\n  I --> J[Add unvisited neighbors to queue]\n  J --> D","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=wu0ckYkltus"},"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:57:42.851Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","answer":"Use topological sort with BFS to process tasks level by level. The minimum time equals the length of the longest path (critical path) in the DAG. O(V+E) time, O(V) space. Track completion times using DP: dp[node] = max(dp[parent]) + 1.","explanation":"## Core Approach\nThe problem reduces to finding the critical path - the longest path in the DAG. This represents the minimum time needed since tasks on different branches can run in parallel.\n\n## Algorithm\n1. Perform topological sort using Kahn's algorithm\n2. Use DP array where dp[node] = earliest completion time\n3. For each node in topological order: dp[node] = max(dp[parents]) + 1\n4. Answer = max(dp[node]) for all nodes\n\n## Code Example\n```python\ndef min_completion_time(n, edges):\n    adj = [[] for _ in range(n)]\n    indegree = [0] * n\n    \n    for u, v in edges:\n        adj[u].append(v)\n        indegree[v] += 1\n    \n    # Topological sort with DP\n    dp = [0] * n\n    queue = deque([i for i in range(n) if indegree[i] == 0])\n    \n    while queue:\n        u = queue.popleft()\n        for v in adj[u]:\n            dp[v] = max(dp[v], dp[u] + 1)\n            indegree[v] -= 1\n            if indegree[v] == 0:\n                queue.append(v)\n    \n    return max(dp)\n```\n\n## Edge Cases\n- Single task: time = 1\n- Independent tasks: time = 1 (parallel execution)\n- Linear chain: time = number of tasks\n- Disconnected components: consider each component's critical path\n\n## Real-World Applications\n- Build systems (Make, Bazel)\n- Workflow orchestration\n- CPU instruction scheduling\n- Project management (PERT/CPM)","diagram":"flowchart TD\n    A[Build Graph & Indegree] --> B[Queue Zero Indegree Tasks]\n    B --> C{Workers Available?}\n    C -->|Yes| D[Assign Task to Worker]\n    C -->|No| E[Wait for Next Worker]\n    D --> F[Update Task Dependencies]\n    E --> G[Advance Time]\n    F --> H{All Tasks Completed?}\n    H -->|No| C\n    H -->|Yes| I[Return Total Time]\n    G --> C","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"channel":"algorithms","subChannel":"graphs","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T05:56:42.419Z","createdAt":"2025-12-23 12:53:09"},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","answer":"Use XOR to find the unique element first, then partition array around it using modified counting sort with bit manipulation.","explanation":"## Approach\n\nThis problem combines bit manipulation with in-place partitioning:\n\n### Step 1: Find the Unique Element\nUse XOR operation on all elements. Since `a ^ a = 0` and `a ^ 0 = a`, all paired elements cancel out, leaving only the unique element.\n\n```javascript\nlet unique = 0;\nfor (let num of arr) {\n  unique ^= num;\n}\n```\n\n### Step 2: Partition Around Unique Element\nUse three-way partitioning (similar to Dutch National Flag):\n- Elements less than unique go left\n- Unique element in middle\n- Elements greater than unique go right\n\n```javascript\nlet low = 0, mid = 0, high = arr.length - 1;\nwhile (mid <= high) {\n  if (arr[mid] < unique) {\n    [arr[low], arr[mid]] = [arr[mid], arr[low]];\n    low++; mid++;\n  } else if (arr[mid] > unique) {\n    [arr[mid], arr[high]] = [arr[high], arr[mid]];\n    high--;\n  } else {\n    mid++;\n  }\n}\n```\n\n### Step 3: Sort Pairs\nWithin each partition, pairs are already together. Use counting sort or simply swap pairs into position.\n\n**Time Complexity:** O(n)\n**Space Complexity:** O(1)","diagram":"graph TD\n    A[Start: Unsorted Array] --> B[XOR all elements]\n    B --> C[Find unique element]\n    C --> D[Three-way partition]\n    D --> E[Elements < unique]\n    D --> F[Unique element]\n    D --> G[Elements > unique]\n    E --> H[Sort pairs in left partition]\n    G --> I[Sort pairs in right partition]\n    H --> J[Combine: Left + Unique + Right]\n    I --> J\n    J --> K[Sorted Array]\n    \n    style C fill:#90EE90\n    style D fill:#FFB6C1\n    style K fill:#87CEEB","difficulty":"intermediate","tags":["sort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=XKu_SEDAykw"},"companies":["Amazon","Apple","Google","Meta","Microsoft"],"eli5":"Imagine you have a big box of toy cars where every car has a twin except one special car that's all alone! To find that special car, we can play a magic game: take pairs of the same car and they disappear like magic! Only the lonely car stays. Now we know which car is special, so we can quickly put all the twin cars on one side and the special car in the middle. It's like organizing your toys super fast without needing extra boxes or asking for help!","relevanceScore":null,"lastUpdated":"2025-12-21T08:48:01.623Z","createdAt":"2025-12-23 12:53:08"},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","answer":"QuickSort: O(n log n) avg, O(n²) worst, in-place, cache-friendly, unstable. MergeSort: O(n log n) always, O(n) space, stable, great for linked lists. Timsort: O(n log n) worst, O(n) best, hybrid (merge+insertion), stable, optimized for real-world data patterns. Choose QuickSort for memory-constrained cache-optimized scenarios, MergeSort for stability requirements, Timsort for general-purpose sorting.","explanation":"## Algorithm Characteristics\n\n**QuickSort** excels with cache locality due to in-place partitioning, making it ~2-3x faster than MergeSort on arrays. However, its O(n²) worst case requires mitigation via median-of-three or random pivot selection.\n\n**MergeSort** guarantees O(n log n) performance and stability, crucial for sorting records by multiple keys. Its O(n) space requirement is problematic for memory-constrained environments but ideal for external sorting.\n\n**Timsort** (Python/Java default) combines MergeSort's stability with insertion sort's O(n) best case on nearly-sorted data. It detects natural runs, achieving 20-30% better performance on real-world datasets.\n\n## Production Considerations\n\n- **Stability matters** when sorting by multiple criteria (e.g., sort by department, then by name)\n- **Cache impact**: QuickSort's in-place nature reduces cache misses vs MergeSort's scattered memory access\n- **Hybrid approaches**: Most libraries use introsort (QuickSort + HeapSort fallback) to eliminate worst-case scenarios\n- **Parallelization**: MergeSort parallelizes naturally; QuickSort requires careful load balancing\n\n## Real-World Applications\n\n- **Database indexes**: Often use B-tree variants (modified MergeSort) for disk-based sorting\n- **In-memory analytics**: QuickSort variants for speed-critical aggregations\n- **Streaming data**: External MergeSort for datasets exceeding RAM capacity","diagram":"\ngraph TD\n    A[Array] --> P{Pick Pivot}\n    P --> L[Left < Pivot]\n    P --> R[Right > Pivot]\n    L --> Sort1[Recurse]\n    R --> Sort2[Recurse]\n","difficulty":"intermediate","tags":["sort","recursion","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a messy pile of numbered cards to sort. QuickSort is like picking one card (the 'boss'), then making two piles: cards smaller than the boss go left, bigger ones go right. Keep doing this with each pile until everything is sorted! It's fast but sometimes picks a bad boss. MergeSort is different - you split the pile in half, sort each half, then carefully combine them like a zipper. It's always reliable but needs extra table space for the combining. QuickSort is like a speedy but sometimes messy kid, MergeSort is like a careful kid who always cleans up but needs more room!","relevanceScore":null,"lastUpdated":"2025-12-24T12:40:33.240Z","createdAt":"2025-12-23 12:53:08"},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","answer":"Quicksort: O(n log n) avg, O(n²) worst, O(log n) space. Mergesort: O(n log n) always, O(n) space. Quicksort is in-place, mergesort is stable.","explanation":"## Why Asked\nTests understanding of fundamental sorting algorithms and their trade-offs in real-world scenarios.\n## Key Concepts\n- Divide and conquer strategies\n- Partitioning vs merging\n- In-place vs auxiliary space\n- Stability and performance characteristics\n## Code Example\n```\n// Quicksort partition\nfunction partition(arr, low, high) {\n  const pivot = arr[high];\n  let i = low - 1;\n  for (let j = low; j < high; j++) {\n    if (arr[j] < pivot) {\n      i++;\n      [arr[i], arr[j]] = [arr[j], arr[i]];\n    }\n  }\n  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];\n  return i + 1;\n}\n\n// Mergesort merge\nfunction merge(left, right) {\n  const result = [];\n  let i = 0, j = 0;\n  while (i < left.length && j < right.length) {\n    if (left[i] <= right[j]) result.push(left[i++]);\n    else result.push(right[j++]);\n  }\n  return result.concat(left.slice(i)).concat(right.slice(j));\n}\n```\n## Follow-up Questions\n- When would you choose one over the other?\n- How does pivot selection affect quicksort performance?\n- Can you implement a stable quicksort?","diagram":"flowchart TD\n  A[Start] --> B{Choose Algorithm}\n  B -->|Quicksort| C[Select Pivot]\n  B -->|Mergesort| D[Divide Array]\n  C --> E[Partition Elements]\n  D --> F[Recursive Sort]\n  E --> G[Recursive Calls]\n  F --> H[Merge Subarrays]\n  G --> I[Combine Results]\n  H --> J[Sorted Array]\n  I --> J\n  J --> K[End]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T04:51:12.165Z","createdAt":"2025-12-23 12:53:08"},{"id":"q-362","question":"Given an array of integers, implement quicksort and explain why its average case is O(n log n) but worst case is O(n²). When would you choose mergesort instead?","answer":"Quicksort: O(n log n) average, O(n²) worst. Choose mergesort when worst-case guarantees matter or data is nearly sorted.","explanation":"## Why This Is Asked\nTests fundamental algorithm knowledge, complexity analysis, and practical decision-making - core skills for consulting technical problems.\n\n## Expected Answer\nCandidate should implement quicksort, explain partitioning, discuss pivot selection impact on complexity, and compare with mergesort's stability and guaranteed O(n log n) performance.\n\n## Code Example\n```python\ndef quicksort(arr, low=0, high=None):\n    if high is None:\n        high = len(arr) - 1\n    if low < high:\n        pivot_idx = partition(arr, low, high)\n        quicksort(arr, low, pivot_idx - 1)\n        quicksort(arr, pivot_idx + 1, high)\n    return arr\n\ndef partition(arr, low, high):\n    pivot = arr[high]\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n```\n\n## Follow-up Questions\n- How does pivot selection affect performance?\n- What's the space complexity of your implementation?\n- How would you optimize for already-sorted arrays?","diagram":"flowchart TD\n  A[Unsorted Array] --> B[Choose Pivot Element]\n  B --> C[Partition Around Pivot]\n  C --> D{Elements < Pivot?}\n  D -->|Yes| E[Left Subarray]\n  D -->|No| F[Right Subarray]\n  E --> G[Recursive Quicksort]\n  F --> G\n  G --> H[Sorted Array]","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bain","PayPal","Roblox"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:10:03.103Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","answer":"Quicksort uses partitioning with O(n log n) average time. Choose it for in-place sorting with better cache performance. Worst case O(n²) occurs with sorted arrays - avoid with random pivot or median-o","explanation":"## Algorithm\n- Choose pivot element\n- Partition array around pivot\n- Recursively sort subarrays\n\n## When to Use\n- In-place sorting needed\n- Better cache locality\n- Average case performance matters\n\n## Trade-offs\n- **Pros**: O(1) space, fast in practice\n- **Cons**: O(n²) worst case, not stable\n\n## Pivot Selection\n```python\ndef quicksort(arr, low, high):\n    if low < high:\n        pivot = partition(arr, low, high)\n        quicksort(arr, low, pivot-1)\n        quicksort(arr, pivot+1, high)\n```","diagram":"flowchart TD\n  A[Choose Pivot] --> B[Partition Array]\n  B --> C[Left Subarray < Pivot]\n  B --> D[Right Subarray > Pivot]\n  C --> E[Recursive Sort]\n  D --> F[Recursive Sort]\n  E --> G[Combine]\n  F --> G","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"channel":"algorithms","subChannel":"sorting","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Oracle","Snowflake"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:42:37.709Z","createdAt":"2025-12-23 12:53:10"},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","answer":"Use recursive DFS: return 1 + max(depth(left), depth(right)) for each node. For iterative BFS, use queue with level counting. Both O(n) time, O(h) recursive space vs O(w) BFS space. Handle empty tree (return 0) and single node cases.","explanation":"## Solution Overview\nMaximum depth (height) of binary tree requires traversing all nodes. Two standard approaches:\n\n## Recursive DFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    return 1 + max(maxDepth(root.left), maxDepth(root.right))\n```\n- Time: O(n) - visits each node once\n- Space: O(h) - call stack depth equals tree height\n- Risk: Stack overflow for deep skewed trees\n\n## Iterative BFS\n```python\ndef maxDepth(root):\n    if not root: return 0\n    queue = collections.deque([root])\n    depth = 0\n    while queue:\n        depth += 1\n        for _ in range(len(queue)):\n            node = queue.popleft()\n            if node.left: queue.append(node.left)\n            if node.right: queue.append(node.right)\n    return depth\n```\n- Time: O(n) - processes each node once\n- Space: O(n) - worst case when tree is complete\n\n## Edge Cases\n- Empty tree: return 0\n- Single node: return 1\n- Skewed tree: consider iterative approach to avoid stack overflow\n\n## Follow-up Questions\n- How would you modify this for N-ary trees?\n- Can you solve this using Morris traversal for O(1) space?\n- How would you find minimum depth instead?","diagram":"graph TD\n    A[Root] --> B[Left Child]\n    A --> C[Right Child]\n    B --> D[Left Leaf]\n    B --> E[Right Leaf]\n    C --> F[Left Leaf]\n    C --> G[Right Leaf]\n    D --> H[null]\n    D --> I[null]\n    E --> J[null]\n    E --> K[null]\n    F --> L[null]\n    F --> M[null]\n    G --> N[null]\n    G --> O[null]","difficulty":"beginner","tags":["tree","binary"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a toy tree made of building blocks! Each block can have two smaller blocks hanging from it - one on the left and one on the right. To find how tall the tree is, you start at the very top block and ask: \"How tall am I?\" Each block says: \"I'm 1 block tall PLUS the taller of my two helper blocks below me!\" If a block has no more blocks under it, it says \"I'm just 1 block tall!\" You keep asking this question down through all the blocks until you reach the bottom. Then all the blocks tell you their height, and you pick the biggest number you heard. That's how tall your toy tree is!","relevanceScore":null,"lastUpdated":"2025-12-22T09:51:53.063Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","answer":"Use inorder traversal which visits nodes in sorted order. For O(h) space, use recursive DFS with counter. For O(1) space, use Morris traversal with threading. Both run in O(k) time average, O(n) worst case. Morris avoids recursion stack but modifies tree temporarily.","explanation":"## Interview Context\nThis question tests tree traversal knowledge and space optimization awareness.\n\n## Recursive Approach\n```python\ndef kthSmallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        self.count += 1\n        if self.count == k: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Iterative Approach (Morris Traversal)\n```python\ndef kthSmallest(root, k):\n    count = 0\n    curr = root\n    while curr:\n        if not curr.left:\n            count += 1\n            if count == k: return curr.val\n            curr = curr.right\n        else:\n            pred = curr.left\n            while pred.right and pred.right != curr:\n                pred = pred.right\n            if not pred.right:\n                pred.right = curr\n                curr = curr.left\n            else:\n                pred.right = None\n                count += 1\n                if count == k: return curr.val\n                curr = curr.right\n```\n- Time: O(n)\n- Space: O(1)\n\n## Edge Cases\n- k > n: return null/raise exception\n- Empty tree: return null\n- Duplicate values: handle based on requirements\n\n## Follow-up Questions\n1. How would you modify this for kth largest element?\n2. What if the tree is not a BST?\n3. How to optimize for multiple k queries on the same tree?","diagram":"flowchart TD\n  A[Input: BST Root + k] --> B{Counter = 0}\n  B --> C[Inorder Traversal]\n  C --> D[Visit Left Subtree]\n  D --> E[Process Current Node]\n  E --> F{Counter == k?}\n  F -->|Yes| G[Return Current Node Value]\n  F -->|No| H[Counter++]\n  H --> I[Visit Right Subtree]\n  I --> D","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=wGXB9OWhPTg"},"companies":null,"eli5":"Imagine you have a line of kids standing by height, from shortest to tallest. Your teacher asks you to find the 3rd shortest kid. You'd just walk down the line counting: 1, 2, 3... and point to that kid! A tree is like a special playground where every branch has smaller kids on the left and bigger kids on the right. To find the kth smallest, you start at the leftmost kid (the shortest) and walk through the playground in order, counting as you go. When you reach the number you're looking for, that's your kid! It's like following a treasure map that always leads you to the kids in height order.","relevanceScore":null,"lastUpdated":"2025-12-22T13:30:35.744Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","answer":"Use inorder traversal with counter. O(h) space for recursion stack, O(n) time worst case. Handle k > total nodes and empty tree.","explanation":"## Why This Is Asked\nTests understanding of BST properties, traversal algorithms, and handling real-world constraints like duplicates - crucial for financial data systems at HRT.\n\n## Expected Answer\nCandidate should explain inorder traversal property (sorted order), implement iterative version to avoid stack overflow, discuss space-time trade-offs, and handle edge cases like k out of bounds.\n\n## Code Example\n```python\ndef kth_smallest(root, k):\n    stack = []\n    curr = root\n    count = 0\n    \n    while stack or curr:\n        while curr:\n            stack.append(curr)\n            curr = curr.left\n        curr = stack.pop()\n        count += 1\n        if count == k:\n            return curr.val\n        curr = curr.right\n    return None\n```\n\n## Follow-up Questions\n- How would you optimize for repeated kth_smallest queries?\n- What if the tree is dynamic (frequent insertions/deletions)?\n- How would you implement this iteratively without extra space?","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left to min]\n  B --> C[Visit node, increment count]\n  C --> D{count == k?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right]\n  F --> C\n  E --> G[End]","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=fAAZixBzIAI"},"companies":["Hrt","New Relic","Sap"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:53:38.691Z","createdAt":"2025-12-23 12:53:09"},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","answer":"Use inorder traversal with a counter. Since BST's left-root-right yields sorted order, traverse recursively, decrement k when visiting node, return when k=0. Space O(h) for recursion stack, time O(n) ","explanation":"## Approach\n- Inorder traversal naturally visits nodes in ascending order\n- Maintain counter to track when we reach kth element\n- Early termination when found\n\n## Implementation\n```python\ndef kth_smallest(root, k):\n    def inorder(node):\n        if not node: return None\n        left = inorder(node.left)\n        if left: return left\n        k[0] -= 1\n        if k[0] == 0: return node.val\n        return inorder(node.right)\n    return inorder(root)\n```\n\n## Complexity\n- Time: O(n) worst case, O(k) average\n- Space: O(h) recursion stack\n\n## Edge Cases\n- k > number of nodes\n- Empty tree\n- Duplicate values","diagram":"flowchart TD\n  A[Start at root] --> B[Traverse left subtree]\n  B --> C[Visit current node]\n  C --> D{Is kth element?}\n  D -->|Yes| E[Return value]\n  D -->|No| F[Traverse right subtree]\n  F --> G[Continue traversal]","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"channel":"algorithms","subChannel":"trees","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Cloudflare"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:44:38.025Z","createdAt":"2025-12-24T02:44:38.025Z"}],"subChannels":["data-structures","dynamic-programming","graphs","sorting","trees"],"companies":["Adobe","Amazon","Anthropic","Apple","Bain","Cisco","Cloudflare","Cruise","Goldman Sachs","Google","Hashicorp","Hrt","Instacart","LinkedIn","Meta","Microsoft","NVIDIA","Netflix","New Relic","OpenAI","Oracle","PayPal","Roblox","Sap","Snowflake","Twitter","Two Sigma","Uber","Unity","Vercel","Zoom"],"stats":{"total":29,"beginner":10,"intermediate":13,"advanced":6,"newThisWeek":29}}