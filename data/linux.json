{"questions":[{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","answer":"Core Linux commands include: file operations (ls, cp, mv, rm), text processing (grep, sed, awk, cut), process management (ps, top, kill, nice), network tools (netstat, curl, wget, ssh), permissions (chmod, chown, chgrp), package management (apt, yum, dnf), and system monitoring (df, du, free, iostat). These form the foundation for shell scripting and DevOps automation.","explanation":"## Interview Context\nThis question tests fundamental Linux proficiency required for DevOps roles. Interviewers want to see practical knowledge beyond basic commands.\n\n## Key Areas to Cover\n- **File System Operations**: Navigation, manipulation, permissions\n- **Process Management**: Monitoring, controlling, prioritizing\n- **Network Tools**: Diagnostics, data transfer, remote access\n- **Text Processing**: Pattern matching, data extraction, transformation\n- **System Monitoring**: Resource usage, performance analysis\n\n## Code Examples\n```bash\n# Find and kill processes by name\nps aux | grep 'nginx' | awk '{print $2}' | xargs kill -9\n\n# Monitor system resources in real-time\ntop -p $(pgrep nginx) -d 2\n\n# Network troubleshooting\nnetstat -tulpn | grep :80\ncurl -I https://api.example.com/health\n```\n\n## Follow-up Questions\n1. How would you write a shell script to automatically restart a service if it's not running?\n2. What's the difference between `grep`, `sed`, and `awk` for text processing?\n3. How would you troubleshoot high CPU usage on a Linux server?","diagram":"\ngraph TD\n    Linux --> Files[File Ops: ls, cp, mv]\n    Linux --> Sys[System: top, df, ps]\n    Linux --> Text[Text: grep, awk, sed]\n","difficulty":"beginner","tags":["linux","shell"],"channel":"linux","subChannel":"commands","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=zB_3FIGRWRU"},"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"],"eli5":"Imagine you're the boss of a giant playground with lots of toys and games. Linux commands are like your magic words to control everything! 'ls' is like looking at all your toys on the shelf. 'cp' is like making copies of your favorite drawings. 'mv' is like moving toys to different boxes. 'grep' is like finding specific words in your storybooks. 'ps' is like checking which friends are playing on the swings. 'curl' is like sending secret messages to other playgrounds. 'chmod' is like deciding who can play with which toys - you, your friends, or everyone. These magic words help you keep the playground running smoothly, fix broken toys, and make games happen automatically while you eat snacks!","relevanceScore":null,"lastUpdated":"2025-12-24T16:41:59.319Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","answer":"Implement a streaming solution using Go with buffered I/O and a min-heap. Process in 1GB chunks with `bufio.Scanner`, count IP frequencies using a hash map, then maintain a top-10 min-heap. Use `sync.Pool` for memory reuse and parallel processing with worker pools. For comparison, Python's `collections.Counter` with generators works well for smaller datasets.","explanation":"## Interview Context\nThis question tests distributed processing, memory management, and algorithmic optimization skills for senior engineering roles.\n\n## Technical Approach\n- **Memory Mapping**: Use `mmap` for zero-copy file access, avoiding data duplication\n- **Streaming Algorithm**: Process file in fixed-size chunks (64KB-1MB) with bounded memory\n- **Data Structure**: Min-heap (O(log n)) for top-10 maintenance vs full sort (O(n log n))\n- **Parallel Processing**: Split file into N chunks, process with multiprocessing Pool\n- **Fault Tolerance**: Checkpoint every 100MB, enable resume on failure\n\n## Code Example\n```python\nimport mmap\nimport heapq\nfrom collections import defaultdict\nfrom multiprocessing import Pool\n\ndef process_chunk(chunk):\n    ip_counts = defaultdict(int)\n    for line in chunk.split(b'\\n'):\n        if line:\n            ip = line.split()[0]\n            ip_counts[ip] += 1\n    return ip_counts\n\ndef parallel_ip_count(filename, chunks=8):\n    with open(filename, 'rb') as f:\n        mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n        chunk_size = len(mm) // chunks\n        \n        with Pool(chunks) as pool:\n            results = pool.map(process_chunk, \n                [mm[i*chunk_size:(i+1)*chunk_size] for i in range(chunks)])\n        \n        # Merge results and maintain top-10\n        total_counts = defaultdict(int)\n        for result in results:\n            for ip, count in result.items():\n                total_counts[ip] += count\n        \n        return heapq.nlargest(10, total_counts.items(), key=lambda x: x[1])\n```\n\n## Performance Optimization\n- **Memory**: <100MB regardless of file size\n- **Speed**: ~8x speedup with 8-core parallel processing\n- **Trade-offs**: CPU vs memory, complexity vs maintainability\n\n## Follow-up Questions\n1. How would you handle log file rotation during processing?\n2. What if IP addresses follow a power-law distribution?\n3. How would you extend this to streaming real-time logs?","diagram":"flowchart TD\n    A[50GB Log Files] --> B[find -print0]\n    B --> C[xargs -0 -P 8]\n    C --> D[cut -d' ' -f1]\n    D --> E[sort -S 2G -T /tmp]\n    E --> F[uniq -c]\n    F --> G[sort -k1,1nr]\n    G --> H[head -10]\n    H --> I[Top 10 IPs]","difficulty":"advanced","tags":["find","xargs","cut","sort"],"channel":"linux","subChannel":"commands","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a huge box of LEGOs with millions of pieces, but you can only play with a few at a time. You want to find the 10 colors you see most often! You'd grab a handful of LEGOs at a time, count each color, and keep a special box for your top 10 favorites. If a new color appears more times than your least favorite, you swap them! You do this over and over until you've looked at all the LEGOs. It's like sorting Halloween candy - you keep a small pile of your 10 favorite kinds, and when you find better candy, you trade out the less good ones. This way, you never need to dump all the candy on the floor at once!","relevanceScore":null,"lastUpdated":"2025-12-22T10:17:32.557Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","answer":"Check `dmesg | grep -i oom-killer` for OOM events. Use `cat /proc/meminfo` to examine memory fragmentation. Review `overcommit_memory` and `overcommit_ratio` in `/proc/sys/vm/`. Monitor with `sar -r` ","explanation":"## Memory Management Issues\n\nLinux OOM killer activates when available memory + swap < pages_min * 4, not when RAM is fully used.\n\n## Diagnostic Steps\n\n- Check OOM killer logs: `dmesg | grep -i oom-killer`\n- Examine memory fragmentation: `cat /proc/meminfo | grep -E '(MemFree|MemAvailable|Slab|PageTables)'`\n- Monitor memory pressure: `cat /proc/pressure/memory`\n- Review overcommit settings: `cat /proc/sys/vm/overcommit_memory`\n\n## Common Causes\n\n- Memory fragmentation preventing large allocations\n- Overcommitment allowing more memory than physically available\n- Kernel memory usage (slabs, page tables) not visible in `free`\n- Memory leaks in kernel modules\n\n## Solutions\n\n```bash\n# Disable overcommit\necho 0 > /proc/sys/vm/overcommit_memory\n\n# Add swap space\nfallocate -l 2G /swapfile\nchmod 600 /swapfile\nmkswap /swapfile\nswapon /swapfile\n\n# Tune memory management\necho 65536 > /proc/sys/vm/min_free_kbytes\n```","diagram":"flowchart TD\n  A[Process Memory Request] --> B{Physical RAM Available?}\n  B -->|No| C[Check Swap]\n  B -->|Yes| D[Allocate Memory]\n  C -->|Swap Available| E[Use Swap]\n  C -->|No Swap| F{Overcommit Enabled?}\n  F -->|Yes| G[Allow Allocation]\n  F -->|No| H[OOM Killer Activates]\n  G --> I[Memory Pressure Builds]\n  I --> H\n  H --> J[Kill Process]\n  J --> K[Log to dmesg]","difficulty":"advanced","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Tesla"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:46:51.191Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","answer":"Use `lsof -i :8080` to identify processes, then `kill -15 PID` for graceful termination. If unresponsive, use `kill -9 PID`. For multiple processes: `pkill -f ':8080'` sends SIGTERM to all matching pr","explanation":"## Process Identification\n- `lsof -i :8080` lists processes using the port\n- `netstat -tulpn | grep :8080` alternative method\n\n## Safe Termination\n- `kill -15 PID` (SIGTERM) allows graceful shutdown\n- `kill -9 PID` (SIGKILL) forces immediate termination\n\n## Batch Operations\n- `pkill -f ':8080'` terminates all matching processes\n- `fuser -k 8080/tcp` kills processes by port","diagram":"flowchart TD\n  A[Identify Port Usage] --> B[lsof -i :8080]\n  B --> C{Process Responding?}\n  C -->|Yes| D[kill -15 PID]\n  C -->|No| E[kill -9 PID]\n  D --> F[Verify Termination]\n  E --> F","difficulty":"beginner","tags":["linux"],"channel":"linux","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Google"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-25T01:15:00.586Z","createdAt":"2025-12-25T01:15:00.586Z"}],"subChannels":["commands","general"],"companies":["Amazon","Apple","Cloudflare","Google","Hashicorp","Microsoft","Netflix","Tesla"],"stats":{"total":4,"beginner":2,"intermediate":0,"advanced":2,"newThisWeek":4}}