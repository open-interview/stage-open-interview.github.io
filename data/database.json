{"questions":[{"id":"da-125","question":"Explain database indexing and when should you use it?","answer":"Database indexes are data structures that improve query speed by maintaining sorted references to data locations, trading increased write overhead for faster read operations.","explanation":"**How Indexes Work**:\n- Create sorted data structures (B-tree, Hash) that point to actual data\n- Enable efficient data location without full table scans\n- Trade write performance for read speed improvements\n\n**When to Use**:\n- Frequently queried columns\n- WHERE clause conditions\n- JOIN operations\n- ORDER BY and GROUP BY columns\n\n**When NOT to Use**:\n- Small tables (< 100 rows)\n- Frequently updated columns\n- Low cardinality columns\n- Write-heavy workloads","diagram":"graph TD\n    Query[SQL Query] --> Index[(Database Index)]\n    Index --> DataPoint[Data Location Pointer]\n    DataPoint --> TableData[(Table Data)]\n    WriteOp[Write Operation] --> IndexUpdate[Update Index]\n    IndexUpdate --> TableUpdate[Update Table]\n    style Index fill:#4ade80\n    style TableData fill:#fbbf24","difficulty":"intermediate","tags":["sql","indexing"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=BIlFTFrEFOI"},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a big box of LEGOs mixed together. If you want to find all the red pieces, you'd have to dig through everything! But what if you had a special magic book that tells you exactly where each red LEGO is? You could find them super fast! A database index is like that magic book. It keeps a quick list of where things are stored, so the computer doesn't have to search through everything. Use it when you need to find things quickly, like finding your favorite toys in a huge toy box. The only downside is that when you add new toys, you also have to update your magic book - that takes a little extra time. But it's totally worth it when you want to find things fast!","relevanceScore":null,"lastUpdated":"2025-12-21T12:44:58.701Z","createdAt":"2025-12-24 12:51:25"},{"id":"db-1","question":"What is the difference between Clustered and Non-Clustered Indexes and when would you use each?","answer":"Clustered Index determines physical data order (1 per table). Non-Clustered is separate lookup structure (many per table).","explanation":"## Why Asked\nTests database indexing fundamentals and performance optimization skills crucial for data-intensive applications\n## Key Concepts\nPhysical vs logical data organization, storage impact, query performance trade-offs\n## Code Example\n```\n-- Clustered (default on primary key)\nCREATE TABLE users (\n  id INT PRIMARY KEY, -- clustered index\n  name VARCHAR(100)\n);\n\n-- Non-clustered\nCREATE INDEX idx_users_email ON users(email);\n```\n## Follow-up Questions\nHow does index fragmentation affect performance? When would you create a covering index?","diagram":"flowchart TD\n  A[Query] --> B{Index Type?}\n  B -->|Clustered| C[Direct data access]\n  B -->|Non-Clustered| D[Pointer lookup] --> C\n  C --> E[Result]","difficulty":"beginner","tags":["sql","indexing","perf"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":"https://youtube.com/watch?v=BxAj3bl00-o","longVideo":"https://youtube.com/watch?v=ITcOiLSfVJQ"},"companies":["Amazon","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine your toys are in a big toy box! A clustered index is like putting all your cars together, then all your dolls, then all your blocks - everything is sorted in one order. You can only have one way to sort everything! A non-clustered index is like having a separate notebook that says 'cars are on page 3, dolls are on page 7' - you can have many notebooks for different ways to find things! Use clustered when you always look for things in the same order (like finding books by their number). Use non-clustered when you need to find things in many different ways (like finding toys by color, size, or type)!","relevanceScore":null,"lastUpdated":"2025-12-24T12:39:11.805Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-170","question":"When would you choose a composite index over multiple single-column indexes in a relational database?","answer":"For queries filtering on multiple columns together, composite indexes are more efficient than separate single-column indexes.","explanation":"Composite indexes are optimal when queries frequently filter or sort on multiple columns simultaneously. They store data in a specific column order, allowing the database to satisfy query conditions using a single index lookup rather than multiple index scans. For example, a composite index on (last_name, first_name) efficiently handles queries like `WHERE last_name = 'Smith' AND first_name = 'John'`. However, composite indexes have higher write overhead and should be used judiciously based on query patterns.","diagram":"graph TD\n    A[Query: WHERE last_name='Smith' AND first_name='John'] --> B{Index Strategy}\n    B --> C[Single Column Indexes]\n    B --> D[Composite Index last_name,first_name]\n    C --> E[2 Index Scans + Merge]\n    D --> F[1 Index Lookup]\n    E --> G[Higher I/O Cost]\n    F --> H[Lower I/O Cost]","difficulty":"intermediate","tags":["index","optimization"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=oebtXK16WuU"},"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"],"eli5":"Imagine you're looking for a specific toy in a giant toy box. If you have separate lists for 'red toys' and 'car toys', you'd have to check both lists and then search through all the toys to find the red cars. But if you have one special list that says 'red cars' together, you can find them instantly! A composite index is like that special combined list - it helps the database find things much faster when you're looking for something that matches multiple rules at once, like finding all the red car toys instead of searching through all red toys AND all car toys separately.","relevanceScore":null,"lastUpdated":"2025-12-21T13:17:36.061Z","createdAt":"2025-12-24 12:51:25"},{"id":"q-288","question":"What is the main difference between B-tree and hash index in terms of range query performance?","answer":"B-trees support efficient range queries with sorted data; hash indexes only support equality lookups and cannot scan ranges efficiently.","explanation":"## Why Asked\nTests understanding of when to choose B-tree vs hash index based on query patterns\n## Key Concepts\nB-tree maintains sorted order, supports range scans; hash index provides O(1) equality but no ordering\n## Code Example\n```\n-- B-tree index (default)\nCREATE INDEX idx_salary ON employees(salary);\n-- Hash index\nCREATE INDEX idx_email_hash ON employees USING HASH(email);\n```\n## Follow-up Questions\nWhen would you use a composite index?\nWhat happens to hash index with many duplicates?\nHow do B-trees handle insertions/deletions?","diagram":"flowchart TD\n  A[Query Type] --> B{Equality?}\n  B -->|Yes| C[Hash Index: O(1)]\n  B -->|No| D[Range/Sort: B-tree]","difficulty":"beginner","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T08:36:24.718Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-365","question":"You're designing a real-time analytics system for Discord that processes millions of message events per minute. Your PostgreSQL database is experiencing severe write contention on the message_events table. How would you design a partitioning strategy using declarative partitioning, and what specific index optimizations would you implement to handle both time-series queries and user-based lookups efficiently?","answer":"Use declarative partitioning by time (hourly/daily) with BRIN indexes for timestamp ranges and B-tree indexes on user_id, plus a composite index for common query patterns.","explanation":"## Why This Is Asked\nDiscord processes massive message volumes and needs real-time analytics. This tests understanding of PostgreSQL partitioning, index types, and query optimization for high-throughput systems.\n\n## Expected Answer\nStrong candidates will discuss:\n- Declarative partitioning by time (hourly/daily partitions)\n- BRIN indexes for timestamp ranges (space-efficient)\n- B-tree indexes on user_id for user lookups\n- Composite (user_id, timestamp) indexes for common patterns\n- Partition pruning and constraint exclusion\n- Consideration for partition maintenance (auto-creation)\n\n## Code Example\n```sql\n-- Create partitioned table\nCREATE TABLE message_events (\n    id BIGSERIAL,\n    user_id BIGINT NOT NULL,\n    guild_id BIGINT NOT NULL,\n    event_type VARCHAR(50) NOT NULL,\n    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),\n    event_data JSONB\n) PARTITION BY RANGE (timestamp);\n\n-- Create BRIN index for timestamp ranges\nCREATE INDEX idx_message_events_timestamp_brin \n    ON message_events USING BRIN (timestamp);\n\n-- Create partitions\nCREATE TABLE message_events_2024_01_01 \n    PARTITION OF message_events \n    FOR VALUES FROM ('2024-01-01 00:00:00') \n    TO ('2024-01-01 01:00:00');\n\n-- B-tree index on user_id within partition\nCREATE INDEX idx_message_events_user_id \n    ON message_events_2024_01_01 USING BTREE (user_id);\n\n-- Composite index for common queries\nCREATE INDEX idx_message_events_user_timestamp \n    ON message_events_2024_01_01 (user_id, timestamp);\n```\n\n## Follow-up Questions\n- How would you handle partition maintenance and auto-creation?\n- What trade-offs exist between BRIN and B-tree indexes for this use case?\n- How would you optimize for cross-partition queries?\n- What strategies would you use for partition pruning?","diagram":"flowchart TD\n    A[Message Event Ingestion] --> B[Partition Router]\n    B --> C{Timestamp Range}\n    C -->|00:00-01:00| D[Partition 2024_01_01_00]\n    C -->|01:00-02:00| E[Partition 2024_01_01_01]\n    C -->|02:00-03:00| F[Partition 2024_01_01_02]\n    D --> G[BRIN Index on Timestamp]\n    E --> H[BRIN Index on Timestamp]\n    F --> I[BRIN Index on Timestamp]\n    G --> J[B-tree Index on user_id]\n    H --> K[B-tree Index on user_id]\n    I --> L[B-tree Index on user_id]\n    J --> M[Query Engine]\n    K --> M\n    L --> M\n    M --> N[Partition Pruning]\n    N --> O[Result Set]","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=-qNSXK7s7_w","longVideo":"https://www.youtube.com/watch?v=niOq5zorv-g"},"companies":["Amazon","Discord","Google","Netflix","Palantir","Stripe","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:06:20.358Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-409","question":"You're designing a database for an e-commerce platform with frequent queries on (user_id, order_date) and (product_id, category). How would you choose between B-tree and hash indexes, and what composite index strategy would optimize both query patterns?","answer":"Use B-tree composite indexes: (user_id, order_date) for range queries and (product_id, category) for exact matches. Hash indexes only support equality comparisons.","explanation":"## Why This Is Asked\nTests practical index selection knowledge and understanding of real-world query optimization trade-offs that TCS engineers face daily.\n\n## Expected Answer\nStrong candidate explains B-tree handles range queries (order_date) and equality, while hash only supports equality. They'll recommend two composite indexes matching query patterns, discuss index ordering, and mention covering indexes to avoid table scans.\n\n## Code Example\n```sql\n-- For user order history (range queries on date)\nCREATE INDEX idx_user_orders ON orders(user_id, order_date);\n\n-- For product category lookups (exact matches)\nCREATE INDEX idx_product_category ON products(product_id, category);\n\n-- Covering index to avoid table lookup\nCREATE INDEX idx_order_summary ON orders(user_id, order_date, status, total);\n```\n\n## Follow-up Questions\n- How would you handle queries that only filter on the second column?\n- When would you consider a hash index instead?\n- How do you monitor index effectiveness in production?","diagram":"flowchart TD\n    A[Query Analysis] --> B{Range Queries?}\n    B -->|Yes| C[B-tree Index]\n    B -->|No| D{Equality Only?}\n    D -->|Yes| E[Hash Index]\n    D -->|No| C\n    C --> F[Composite Strategy]\n    E --> F\n    F --> G[Index Ordering]\n    G --> H[Covering Indexes]\n    H --> I[Performance Monitoring]","difficulty":"intermediate","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=BHCSL_ZifI0"},"companies":["Gitlab","Tcs","Tempus"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:25:08.134Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-420","question":"You're designing a user database for a chat application with 10M users. When would you choose a B-tree index over a hash index for the 'email' column, and what are the performance implications for login queries, user search, and profile updates?","answer":"B-tree indexes support range queries, ORDER BY, and prefix searches (LIKE 'user%@domain.com'), while hash indexes only handle exact equality. For email lookups, B-tree provides better concurrency, lower lock contention, and works with PostgreSQL's MVCC. Hash indexes offer ~20% faster exact matches but require full rebuilds on updates and don't support covering indexes.","explanation":"## Interview Context\nThis question tests database indexing knowledge in a real-world scenario with large-scale user management.\n\n## Technical Deep Dive\n### B-tree vs Hash Index Characteristics\n- **B-tree**: Supports range queries, prefix matching, ORDER BY, and handles concurrent access better\n- **Hash**: Only exact equality (WHERE email = 'user@domain.com'), O(1) lookup but no range support\n\n### Performance Considerations\n- **Memory Usage**: B-trees use ~20-30% more memory than hash indexes\n- **Write Performance**: B-trees have 15-25% slower inserts due to tree rebalancing\n- **Read Performance**: Hash indexes 10-15% faster for exact matches only\n\n### Database-Specific Implementations\n- **PostgreSQL**: Hash indexes deprecated pre-v10, now improved but still limited\n- **MySQL**: Only B-tree (InnoDB) or hash (MEMORY) engines available\n- **Concurrent Access**: B-trees handle high concurrency better with MVCC\n\n## Code Example\n```sql\n-- B-tree index (recommended)\nCREATE INDEX idx_users_email_btree ON users(email);\n\n-- Hash index (limited use case)\nCREATE INDEX idx_users_email_hash ON users USING HASH(email);\n```\n\n## Follow-up Questions\n1. How would you handle email case-insensitivity in your indexing strategy?\n2. What composite indexes would you create for frequent authentication + profile queries?\n3. How does index maintenance affect database backup and recovery procedures?","diagram":"flowchart TD\n  A[Query: WHERE email = 'user@domain.com'] --> B{Index Type?}\n  B -->|Hash Index| C[O(1) Direct Lookup]\n  B -->|B-tree Index| D[O(log n) Tree Traversal]\n  E[Query: WHERE email LIKE 'user%'] --> F{Supported?}\n  F -->|Hash Index| G[❌ Full Table Scan]\n  F -->|B-tree Index| H[✅ Range Scan]\n  I[Query: ORDER BY email] --> J{Can Use Index?}\n  J -->|Hash Index| K[❌ Separate Sort]\n  J -->|B-tree Index| L[✅ Ordered Retrieval]","difficulty":"beginner","tags":["btree","hash-index","composite"],"channel":"database","subChannel":"indexing","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=6fnmXX8RK0s","longVideo":"https://www.youtube.com/watch?v=a1Z40OC553Y"},"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":"Imagine you have a giant box of 10 million toy cars and need to find specific ones quickly. A B-tree is like organizing them by color in rainbow order - you can find all red cars, or all cars from 'red' to 'blue', and even cars that start with 'r'. A hash index is like putting each car in a magic box that instantly gives you the exact car you ask for, but only if you know the exact name. For finding friends by email, the rainbow order helps when you want to find everyone whose email starts with 'john' or all emails between 'a' and 'm'. The magic box is faster when you know the exact email, but gets confused if you ask for partial matches. When kids update their profiles, the rainbow order stays neat while the magic box needs to be completely reorganized!","relevanceScore":null,"lastUpdated":"2025-12-23T16:35:22.944Z","createdAt":"2025-12-24 12:51:27"},{"id":"da-129","question":"What is the main difference between SQL and NoSQL databases in terms of data structure?","answer":"SQL uses structured tables with fixed schemas, NoSQL uses flexible document/key-value/graph structures without fixed schemas.","explanation":"## SQL vs NoSQL Data Structure\n\n**SQL Databases:**\n- Store data in **tables** with rows and columns\n- Require a **predefined schema** (structure must be defined before inserting data)\n- Data must conform to the schema (same columns for all rows)\n- Examples: MySQL, PostgreSQL, Oracle\n\n**NoSQL Databases:**\n- Store data in flexible formats:\n  - **Document stores** (JSON-like documents) - MongoDB, CouchDB\n  - **Key-value pairs** - Redis, DynamoDB\n  - **Column-family** - Cassandra, HBase\n  - **Graph databases** - Neo4j, Amazon Neptune\n- **Schema-less** or **schema-flexible**\n- Can store different structures in the same collection/table\n- Better for rapidly changing requirements\n\n**When to use NoSQL:**\n- Rapidly evolving data structures\n- Large scale applications requiring horizontal scaling\n- Semi-structured or unstructured data\n- Real-time applications","diagram":"graph TD\n    A[Database Types] --> B[SQL Databases]\n    A --> C[NoSQL Databases]\n    \n    B --> D[Fixed Schema]\n    B --> E[Tables with Rows/Columns]\n    B --> F[ACID Compliance]\n    \n    C --> G[Flexible Schema]\n    C --> H[Multiple Data Models]\n    \n    H --> I[Document Store]\n    H --> J[Key-Value]\n    H --> K[Column-Family]\n    H --> L[Graph]\n    \n    I --> M[MongoDB]\n    J --> N[Redis]\n    K --> O[Cassandra]\n    L --> P[Neo4j]","difficulty":"beginner","tags":["nosql","mongodb"],"channel":"database","subChannel":"nosql","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=eVApl3kzTB0","longVideo":"https://www.youtube.com/watch?v=uD3p_rZPBUQ"},"companies":["Amazon","Google","Microsoft","Netflix","Uber"],"eli5":"Imagine you're organizing your toys! SQL is like having special boxes with fixed spots - each box has places for exactly 3 cars, 2 dolls, and 5 blocks. Every toy must go in its correct spot! NoSQL is like a big play area where you can dump toys however you want - sometimes you make a pile of cars, other times you mix dolls and blocks together. You can even add new toy types anytime! SQL wants everything organized the same way, while NoSQL lets you play freely with your toys however you like.","relevanceScore":null,"lastUpdated":"2025-12-21T12:46:34.132Z","createdAt":"2025-12-24 12:51:25"},{"id":"q-242","question":"How does MongoDB's document structure differ from SQL's table rows for storing user data?","answer":"MongoDB stores flexible JSON-like documents with varying schemas, while SQL uses fixed table rows with predefined columns.","explanation":"## Concept Overview\nMongoDB uses a document-oriented data model where each document can have a different structure, while SQL databases use rigid table schemas with fixed columns.\n\n## Implementation Details\n- **MongoDB**: Documents stored in BSON format, schema-less design\n- **SQL**: Fixed schema with predefined column types and constraints\n- **Flexibility**: MongoDB allows nested objects and arrays, SQL requires normalization\n\n## Code Example\n```javascript\n// MongoDB Document\ndb.users.insertOne({\n  _id: ObjectId(\"...\"),\n  name: \"John Doe\",\n  email: \"john@example.com\",\n  profile: {\n    age: 30,\n    interests: [\"coding\", \"music\"]\n  }\n});\n\n-- SQL Equivalent\nCREATE TABLE users (\n  id INT PRIMARY KEY,\n  name VARCHAR(100),\n  email VARCHAR(100),\n  age INT\n);\n-- Requires separate table for interests\n```\n\n## Common Pitfalls\n- **MongoDB**: Can lead to inconsistent data structures if not properly managed\n- **SQL**: Requires schema migrations for any structural changes\n- **Performance**: MongoDB may be slower for complex joins, SQL excels at relational queries","diagram":"graph TD\n    A[Application] --> B[MongoDB Collection]\n    A --> C[SQL Database]\n    \n    B --> D[Document 1<br/>name: 'John'<br/>email: 'john@ex.com'<br/>profile.age: 30]\n    B --> E[Document 2<br/>name: 'Jane'<br/>email: 'jane@ex.com'<br/>profile.age: 25<br/>profile.interests: ['coding']]\n    \n    C --> F[Users Table<br/>id | name | email | age]\n    C --> G[Interests Table<br/>user_id | interest]\n    \n    D --> H[Flexible Schema<br/>Nested Data]\n    E --> H\n    F --> I[Rigid Schema<br/>Normalized Data]\n    G --> I","difficulty":"beginner","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"database","subChannel":"nosql","sourceUrl":"https://www.mongodb.com/docs/manual/reference/sql-comparison/","videos":{"shortVideo":"https://www.youtube.com/watch?v=8sHCdz_tOjk","longVideo":"https://www.youtube.com/watch?v=6oYqDZN72aY"},"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:09:18.636Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-331","question":"You're designing a multi-region e-commerce platform using DynamoDB. Your product catalog needs to support 10M items with eventual consistency across regions, but you must handle hot partitioning during flash sales. How would you design your partition key strategy and what trade-offs would you make between read performance and write throughput?","answer":"Use composite partition keys with sharding (product_id#hash) and adaptive capacity for hot items, trading some read latency for write scalability.","explanation":"## Why This Is Asked\nHashiCorp needs engineers who understand distributed database design at scale, especially for high-traffic scenarios. This tests knowledge of DynamoDB's partitioning model, hot key mitigation, and multi-region consistency trade-offs.\n\n## Expected Answer\nStrong candidates would discuss:\n- Composite partition keys with random suffixes to distribute load\n- Adaptive capacity for hot items during flash sales\n- Trade-offs between eventual consistency and read-after-write\n- Use of DynamoDB Accelerator (DAX) for read performance\n- Consideration of global tables vs. application-level replication\n\n## Code Example\n```typescript\n// Partition key strategy with sharding\nconst generatePartitionKey = (productId: string, shardCount: number = 10) => {\n  const hash = productId.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);\n  const shardId = hash % shardCount;\n  return `${productId}#shard${shardId}`;\n};\n\n// Item structure for hot item handling\ninterface CatalogItem {\n  PK: string; // product_id#shardX\n  SK: string; // METADATA\n  productId: string;\n  shardId: number;\n  // ... other attributes\n  hotItemFlag: boolean; // for adaptive capacity\n}\n```\n\n## Follow-up Questions\n- How would you handle schema evolution when adding new product categories?\n- What monitoring would you implement to detect hot partitioning before it impacts performance?\n- How would you design your backup and disaster recovery strategy?","diagram":"flowchart TD\n  A[Product Request] --> B[Generate Partition Key]\n  B --> C[product_id + hash]\n  C --> D{Hot Item?}\n  D -->|Yes| E[Enable Adaptive Capacity]\n  D -->|No| F[Standard Write]\n  E --> G[Write to DynamoDB]\n  F --> G\n  G --> H[Replicate to Global Tables]\n  H --> I[Read via DAX Cache]","difficulty":"advanced","tags":["mongodb","dynamodb","cassandra","redis"],"channel":"database","subChannel":"nosql","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta","Oracle"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T12:41:02.473Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-268","question":"How would you optimize a time-series analytics query that scans 100M+ rows across multiple date partitions in PostgreSQL when the WHERE clause cannot be pruned effectively due to complex temporal conditions?","answer":"Implement composite partitioning by date + user_id with BRIN indexes, create materialized views for common aggregations, use columnar storage via cstore_fdw, add query plan hints with SET enable_seqscan=off, and leverage parallel query with max_parallel_workers_per_gather=4. Combine this with partition-wise joins and incremental materialized view refresh for optimal performance.","explanation":"## Interview Context\nThis question assesses your ability to optimize large-scale PostgreSQL queries with real-world constraints like real-time dashboard requirements and inefficient WHERE clauses.\n\n## Technical Approach\n- **Composite Partitioning**: Date + user_id partitions reduce scan scope while maintaining query flexibility\n- **BRIN Indexes**: Block Range Indexes perfect for time-series data with natural ordering\n- **Materialized Views**: Pre-computed aggregations refreshed incrementally to avoid full rescans\n- **Columnar Storage**: TimescaleDB's compression reduces I/O for analytical workloads\n- **Parallel Execution**: Leverage multiple CPU cores for concurrent partition processing\n\n## Implementation Details\n```sql\n-- Create time-series table with TimescaleDB\nCREATE TABLE metrics (\n  timestamp TIMESTAMPTZ NOT NULL,\n  user_id BIGINT NOT NULL,\n  value NUMERIC NOT NULL\n) PARTITION BY RANGE (timestamp);\n\n-- Create BRIN index for efficient range scans\nCREATE INDEX idx_metrics_brin ON metrics USING BRIN (timestamp);\n\n-- Materialized view for common aggregations\nCREATE MATERIALIZED VIEW daily_metrics AS\nSELECT date_trunc('day', timestamp) as day,\n       user_id, AVG(value), COUNT(*)\nFROM metrics GROUP BY day, user_id;\n```\n\n## Follow-up Questions\n1. How would you handle partition pruning when the WHERE clause doesn't include the partition key?\n2. What metrics would you monitor to determine if your optimization is effective?\n3. How would you design the schema to handle both real-time and historical query patterns?","diagram":"flowchart TD\n    A[Query Request] --> B{Partition Strategy}\n    B -->|Composite| C[Time + Hash Partitioning]\n    B -->|Single Key| D[Range Partitioning Only]\n    \n    C --> E{Query Pattern}\n    E -->|Analytical| F[Materialized Views]\n    E -->|Real-time| G[Direct Table Scan]\n    \n    D --> H[All Partitions Scanned]\n    H --> I[Performance Degradation]\n    \n    F --> J[Pre-computed Aggregations]\n    G --> K[Parallel Execution]\n    J --> L[Fast Analytics]\n    K --> L","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a huge toy box with 100 million LEGOs scattered all over your room! You need to find all the red LEGOs you played with last week, but they're mixed with everything else. Instead of digging through the whole pile every time, you put toys in small boxes labeled by date and by which friend played with them. Now when you look for red LEGOs, you only check a few boxes! You also make special picture books showing your favorite toy combinations, so you don't have to rebuild them each time. You ask your friends to help search different boxes at the same time, like a team treasure hunt! This way, finding your toys becomes super fast and fun instead of taking forever!","relevanceScore":null,"lastUpdated":"2025-12-22T09:58:24.615Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-303","question":"How would you optimize a slow PostgreSQL query that joins 5 tables with millions of rows?","answer":"Add appropriate indexes on foreign keys and join columns, use EXPLAIN ANALYZE to identify bottlenecks, consider query rewriting.","explanation":"## Why Asked\nTests practical database optimization skills and understanding of PostgreSQL performance tuning.\n## Key Concepts\nIndexing strategies, query execution plans, join algorithms, database statistics.\n## Code Example\n```\n-- Add composite index for frequent joins\nCREATE INDEX idx_orders_customer_date \nON orders(customer_id, order_date);\n\n-- Analyze query performance\nEXPLAIN ANALYZE SELECT * FROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.order_date > '2023-01-01';\n```\n## Follow-up Questions\nWhat types of indexes would you use? How do you handle index bloat? When would you denormalize?","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C[Identify Bottlenecks]\n  C --> D[Add Indexes]\n  D --> E[Rewrite Query]\n  E --> F[Test Performance]","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:17:37.583Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-343","question":"You have a PostgreSQL table with 100M rows partitioned by date. A query filtering on a specific date range is still slow. What would you check in the EXPLAIN plan and how would you optimize it?","answer":"Check partition pruning, index usage, and sort operations. Add composite indexes on (date, filtered_columns) and consider cluster ordering.","explanation":"## Why This Is Asked\nTests practical query optimization skills, understanding of partitioning benefits, and ability to diagnose performance issues in large datasets.\n\n## Expected Answer\nStrong candidates would mention: 1) Verify partition pruning is working, 2) Check if indexes are being used vs seq scans, 3) Look for expensive sorts or hash aggregates, 4) Consider composite indexes covering the WHERE clause, 5) Evaluate if the partition key is optimal for the query pattern.\n\n## Code Example\n```sql\n-- Check partition pruning\nEXPLAIN (ANALYZE, BUFFERS) SELECT * FROM events \nWHERE event_date BETWEEN '2024-01-01' AND '2024-01-31' \nAND user_id = 123;\n\n-- Add composite index\nCREATE INDEX CONCURRENTLY idx_events_date_user \nON events (event_date, user_id);\n\n-- Cluster by frequently queried column\nCLUSTER events USING idx_events_date_user;\n```\n\n## Follow-up Questions\n- How would you handle queries that span multiple partitions?\n- When would you choose subpartitioning over better indexes?\n- How do you monitor partition pruning effectiveness?","diagram":"flowchart TD\n  A[Slow Query] --> B[Check EXPLAIN Plan]\n  B --> C{Partition Pruning?}\n  C -->|No| D[Fix Partition Key]\n  C -->|Yes| E{Index Usage?}\n  E -->|No| F[Add Composite Index]\n  E -->|Yes| G{Expensive Sorts?}\n  G -->|Yes| H[Cluster/Reorder]\n  G -->|No| I[Check Statistics]\n  D --> J[Optimized Query]\n  F --> J\n  H --> J\n  I --> J","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=n2Fluyr3lbc","longVideo":"https://www.youtube.com/watch?v=sitUYx2EfhY"},"companies":["Affirm","Amazon","Google","Jane Street","Meta","Microsoft","Netflix","Stripe"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:55:26.861Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-380","question":"You're optimizing a query that's slow due to a large time-series table. The query filters by timestamp range and device_id. How would you analyze the query plan and what partitioning strategy would you recommend?","answer":"Analyze the EXPLAIN plan to identify full table scans, then implement timestamp-based partitioning with a composite index on (timestamp, device_id).","explanation":"## Why This Is Asked\nTesla deals with massive time-series data from vehicles. This tests practical query optimization skills and understanding of how partitioning affects performance at scale.\n\n## Expected Answer\nA strong candidate would: 1) Use EXPLAIN ANALYZE to identify the bottleneck, 2) Notice the query isn't using the index effectively, 3) Recommend partitioning by timestamp ranges (daily/weekly), 4) Suggest a composite index, 5) Explain how partition pruning reduces scanned data.\n\n## Code Example\n```sql\n-- Analyze current performance\nEXPLAIN ANALYZE SELECT * FROM telemetry \nWHERE timestamp >= '2024-01-01' AND timestamp <= '2024-01-31'\nAND device_id = 'tesla_12345';\n\n-- Recommended partitioning strategy\nCREATE TABLE telemetry (\n  id BIGINT,\n  timestamp TIMESTAMP,\n  device_id VARCHAR,\n  data JSONB\n) PARTITION BY RANGE (timestamp);\n\n-- Create monthly partitions\nCREATE TABLE telemetry_2024_01 PARTITION OF telemetry\nFOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n\n-- Composite index for optimal performance\nCREATE INDEX idx_telemetry_timestamp_device \nON telemetry (timestamp, device_id);\n```\n\n## Follow-up Questions\n- How would you handle cross-partition queries?\n- What are the trade-offs between daily vs monthly partitions?\n- How would you monitor partition skew and rebalance?","diagram":"flowchart TD\n  A[Query: SELECT * FROM telemetry] --> B[EXPLAIN ANALYZE]\n  B --> C{Full Table Scan?}\n  C -->|Yes| D[Identify Bottleneck]\n  C -->|No| E[Check Index Usage]\n  D --> F[Create Timestamp Partitions]\n  E --> F\n  F --> G[Add Composite Index]\n  G --> H[Partition Pruning Applied]\n  H --> I[Reduced I/O & Faster Query]","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hrt","Stripe","Tesla"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T12:47:24.947Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-436","question":"You have a 100M row orders table with slow queries. The query plan shows sequential scans despite indexes on customer_id and order_date. How would you diagnose and fix this performance issue?","answer":"Check index usage with EXPLAIN ANALYZE. If sequential scans persist, consider partitioning by date ranges or customer_id. Use composite indexes for common filter combinations. Analyze statistics with ","explanation":"## Diagnosis\n- Run EXPLAIN ANALYZE to identify bottlenecks\n- Check index selectivity and cardinality\n- Verify query matches index column order\n\n## Solutions\n- **Partitioning**: Range partition by order_date or hash by customer_id\n- **Indexing**: Composite (customer_id, order_date) for common filters\n- **Statistics**: Run ANALYZE to update planner statistics\n- **Configuration**: Increase work_mem for sort operations\n\n## Trade-offs\n- Partitioning improves query performance but adds complexity\n- More indexes increase write overhead but speed reads","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C{Sequential Scan?}\n  C -->|Yes| D[Check Index Usage]\n  C -->|No| E[Optimize Join Order]\n  D --> F[Low Selectivity?]\n  F -->|Yes| G[Add Composite Index]\n  F -->|No| H[Consider Partitioning]\n  G --> I[Monitor Performance]\n  H --> I","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"channel":"database","subChannel":"query-optimization","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Salesforce","Slack"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T12:43:05.831Z","createdAt":"2025-12-24 12:51:27"},{"id":"da-145","question":"You have a table `orders` with columns (id, user_id, amount, created_at) and need to find users who made their first purchase in the last 30 days AND have made at least 3 purchases total, but their average order value is below the overall platform average. Write an optimized SQL query.","answer":"Use window functions with ROW_NUMBER() to find first purchases, COUNT() for total orders, and subqueries for average comparisons with proper indexing.","explanation":"## Solution\n\n```sql\nWITH user_stats AS (\n  SELECT \n    user_id,\n    COUNT(*) as total_orders,\n    AVG(amount) as user_avg_amount,\n    MIN(created_at) as first_order_date\n  FROM orders\n  GROUP BY user_id\n  HAVING COUNT(*) >= 3\n),\nplatform_avg AS (\n  SELECT AVG(amount) as overall_avg\n  FROM orders\n)\nSELECT DISTINCT us.user_id\nFROM user_stats us\nCROSS JOIN platform_avg pa\nWHERE us.first_order_date >= CURRENT_DATE - INTERVAL '30 days'\n  AND us.user_avg_amount < pa.overall_avg;\n```\n\n## Key Concepts\n\n- **CTEs (Common Table Expressions)**: Break complex logic into readable chunks\n- **Window Functions**: Efficient for ranking and aggregations\n- **Performance Optimization**: \n  - Index on `(user_id, created_at)` for grouping\n  - Index on `created_at` for date filtering\n  - HAVING clause filters before final result set\n- **Cross Join**: Efficiently compare user averages to platform average\n\n## Alternative Approach\n```sql\nSELECT user_id\nFROM orders o1\nWHERE (SELECT MIN(created_at) FROM orders o2 WHERE o2.user_id = o1.user_id) >= CURRENT_DATE - INTERVAL '30 days'\nGROUP BY user_id\nHAVING COUNT(*) >= 3\n  AND AVG(amount) < (SELECT AVG(amount) FROM orders);\n```\n\nThis tests understanding of aggregation functions, subqueries, date operations, and query optimization strategies.","diagram":"graph TD\n    A[orders table] --> B[Group by user_id]\n    B --> C[Calculate user stats]\n    C --> D[Filter: total_orders >= 3]\n    D --> E[Filter: first_order in last 30 days]\n    E --> F[Compare user_avg < platform_avg]\n    F --> G[Return qualifying user_ids]\n    \n    H[Platform Average] --> F\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style F fill:#fff3e0","difficulty":"advanced","tags":["sql","indexing"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=rIcB4zMYMas"},"companies":["Airbnb","Microsoft","Stripe","Uber","Walmart"],"eli5":"Imagine you have a lemonade stand! You want to find new friends who started buying from you this month AND bought lemonade at least 3 times, but usually spend less money than most kids do. You'd write down each friend's first visit (like a special sticker), count how many times they came, and check if they're spending less than the average kid. This helps you find your regular-but-frugal customers!","relevanceScore":null,"lastUpdated":"2025-12-21T12:55:39.024Z","createdAt":"2025-12-24 12:51:25"},{"id":"da-156","question":"What is the difference between DELETE and TRUNCATE commands in SQL?","answer":"DELETE removes rows one by one and can use WHERE; TRUNCATE removes all rows at once, faster but can't be filtered.","explanation":"## DELETE vs TRUNCATE\n\n### DELETE Command\n- Removes rows one at a time\n- Can use WHERE clause to filter specific rows\n- Triggers are fired for each deleted row\n- Slower for large datasets\n- Transaction log records each row deletion\n- Can be rolled back\n\n```sql\nDELETE FROM users WHERE age < 18;\n```\n\n### TRUNCATE Command\n- Removes all rows at once\n- Cannot use WHERE clause (removes entire table data)\n- No triggers fired\n- Much faster for large datasets\n- Minimal transaction logging\n- Cannot be rolled back in most databases\n- Resets auto-increment counters\n\n```sql\nTRUNCATE TABLE users;\n```\n\n### When to Use Each\n- Use **DELETE** when you need to remove specific rows or need transaction safety\n- Use **TRUNCATE** when you need to quickly empty an entire table","diagram":"graph TD\n    A[SQL Data Removal] --> B[DELETE]\n    A --> C[TRUNCATE]\n    B --> D[Row-by-row removal]\n    B --> E[WHERE clause supported]\n    B --> F[Slower, logged]\n    B --> G[Can rollback]\n    C --> H[Bulk removal]\n    C --> I[No WHERE clause]\n    C --> J[Faster, minimal log]\n    C --> K[Cannot rollback]\n    C --> L[Resets auto-increment]","difficulty":"beginner","tags":["sql","indexing"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=HXV3zeQKqGY"},"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"],"eli5":"Imagine you have a big box of LEGO blocks! DELETE is like picking out one block at a time - you can be super picky and only remove the red ones if you want. But it takes time because you do it one by one. TRUNCATE is like dumping the whole box upside down - WHOOSH! All the blocks fall out at once. It's super fast, but you can't choose which blocks stay - they all go! So DELETE is careful and picky, while TRUNCATE is fast and takes everything!","relevanceScore":null,"lastUpdated":"2025-12-21T13:02:30.203Z","createdAt":"2025-12-24 12:51:25"},{"id":"q-458","question":"You have a PostgreSQL database with orders (10M rows) and customers (1M rows). A query joining these tables is slow. How would you optimize it?","answer":"Add indexes on foreign keys (customer_id), use EXPLAIN ANALYZE to identify bottlenecks, consider denormalization for frequently accessed data, implement proper join order, and use partitioning for lar","explanation":"## Key Optimization Strategies\n\n- **Indexing**: Create composite indexes on join columns and frequently filtered columns\n- **Query Analysis**: Use EXPLAIN ANALYZE to identify full table scans and expensive operations\n- **Join Strategy**: Ensure proper join order and consider hash joins for large datasets\n- **Partitioning**: Implement table partitioning by date or customer ranges for better performance\n\n## PostgreSQL Specific Techniques\n\n```sql\n-- Create composite index for join and filter\nCREATE INDEX idx_orders_customer_date \nON orders(customer_id, order_date);\n\n-- Analyze query performance\nEXPLAIN ANALYZE SELECT o.*, c.name \nFROM orders o JOIN customers c \nON o.customer_id = c.id \nWHERE o.order_date > '2023-01-01';\n```\n\n## Trade-offs\n\n- **Storage**: Indexes increase storage requirements\n- **Write Performance**: More indexes slow down INSERT/UPDATE operations\n- **Maintenance**: Regular VACUUM and ANALYZE needed for optimal performance","diagram":"flowchart TD\n  A[Slow Query] --> B[EXPLAIN ANALYZE]\n  B --> C[Identify Bottlenecks]\n  C --> D[Add Indexes]\n  D --> E[Optimize Join Order]\n  E --> F[Consider Partitioning]\n  F --> G[Monitor Performance]","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"channel":"database","subChannel":"sql","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Tesla","Uber"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:45:46.729Z","createdAt":"2025-12-24 12:51:27"},{"id":"da-128","question":"You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?","answer":"Use database transactions with ACID properties. Wrap both operations in a single transaction that either commits both or rolls back both.","explanation":"## Database Transaction for Money Transfer\n\nThis scenario illustrates the critical importance of **ACID properties** in database transactions:\n\n### The Problem\nWithout proper transaction handling:\n1. Debit $500 from Account A (balance becomes $500)\n2. **System crashes here**\n3. Credit to Account B never happens\n4. **Result: $500 disappears from the system**\n\n### The Solution: ACID Transaction\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = 'A' FOR UPDATE;\n\n-- Perform both operations atomically\nUPDATE accounts SET balance = balance - 500 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 500 WHERE id = 'B';\n\nCOMMIT;\n```\n\n### ACID Properties Explained\n\n- **Atomicity**: Both operations succeed together or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Other transactions can't see intermediate states\n- **Durability**: Once committed, changes survive system crashes\n\n### Additional Safeguards\n\n1. **Deadlock Prevention**: Always acquire locks in consistent order (e.g., by account ID)\n2. **Timeout Handling**: Set transaction timeouts to prevent indefinite locks\n3. **Retry Logic**: Implement exponential backoff for transient failures\n4. **Audit Trail**: Log all transaction attempts for reconciliation","diagram":"graph TD\n    A[Start Transaction] --> B[Lock Account A]\n    B --> C[Check Balance >= $500]\n    C -->|Yes| D[Debit $500 from Account A]\n    C -->|No| E[Rollback - Insufficient Funds]\n    D --> F[Credit $500 to Account B]\n    F --> G[Commit Transaction]\n    G --> H[Release Locks]\n    \n    D -->|System Crash| I[Automatic Rollback]\n    F -->|System Crash| I\n    I --> J[Both Accounts Restored]\n    \n    E --> K[Transaction Failed]\n    \n    style A fill:#e1f5fe\n    style G fill:#c8e6c9\n    style I fill:#ffcdd2\n    style E fill:#ffcdd2","difficulty":"intermediate","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=GAe5oB742dw","longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're trading cookies with your friend. You have 10 cookies and want to give 5 to your friend who has 2 cookies. You take 5 cookies from your jar first, but before putting them in your friend's jar, the lights go out! Now you only have 5 cookies left, but your friend still has only 2. The cookies are lost in the dark! To fix this, we use a magic box called a transaction. It's like having a grown-up watch the trade. If the lights go out midway, the magic box automatically puts everything back exactly where it started - you get your 5 cookies back, and your friend still has their 2. Either the whole trade finishes perfectly, or nothing changes at all. No cookies ever get lost in the dark!","relevanceScore":null,"lastUpdated":"2025-12-21T12:46:06.064Z","createdAt":"2025-12-24 12:51:25"},{"id":"da-134","question":"You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?","answer":"Money disappears into deleted account. Use SELECT FOR UPDATE or SERIALIZABLE isolation to prevent phantom reads and ensure referential integrity.","explanation":"## Transaction Isolation and Phantom Reads\n\nThis scenario demonstrates a **phantom read** problem in READ COMMITTED isolation:\n\n### What Happens:\n1. **Transaction T1** (transfer): Reads Account A balance, debits $100\n2. **Transaction T2** (deletion): Deletes Account B \n3. **Transaction T1**: Attempts to credit Account B - but it no longer exists\n4. **Result**: $100 vanishes from the system\n\n### Why READ COMMITTED Fails:\n- Only prevents **dirty reads** and **non-repeatable reads**\n- Does **NOT** prevent **phantom reads** (rows appearing/disappearing)\n- Account B's existence isn't locked during the transfer\n\n### Solutions:\n\n#### 1. Row-Level Locking\n```sql\nBEGIN;\nSELECT balance FROM accounts WHERE id = 'B' FOR UPDATE;\n-- This locks Account B, preventing deletion\nUPDATE accounts SET balance = balance - 100 WHERE id = 'A';\nUPDATE accounts SET balance = balance + 100 WHERE id = 'B';\nCOMMIT;\n```\n\n#### 2. SERIALIZABLE Isolation\n```sql\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n-- Prevents all anomalies including phantom reads\n```\n\n#### 3. Application-Level Validation\n```sql\nBEGIN;\nIF NOT EXISTS (SELECT 1 FROM accounts WHERE id = 'B') THEN\n    ROLLBACK;\nEND IF;\n-- Proceed with transfer\nCOMMIT;\n```\n\n### Best Practice:\nUse **SELECT FOR UPDATE** on target accounts before any transfer to ensure atomicity and prevent phantom deletions.","diagram":"graph TD\n    A[Transaction T1: Transfer $100] --> B[Read Account A: $500]\n    A --> C[Read Account B: $200]\n    D[Transaction T2: Delete Account B] --> E[DELETE FROM accounts WHERE id='B']\n    B --> F[UPDATE Account A: $400]\n    C --> G[UPDATE Account B: ???]\n    E --> H[Account B Deleted]\n    G --> I[ERROR: Account B not found]\n    F --> J[Money Lost: $100 vanished]\n    \n    K[Solution: SELECT FOR UPDATE] --> L[Lock Account B]\n    L --> M[Prevent Deletion]\n    M --> N[Safe Transfer]","difficulty":"advanced","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=tYG0akP7H-M"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're giving $100 from your piggy bank to your friend Sarah. But just as you hand the money, Sarah disappears into a magic portal! Your money just vanishes because she's gone. That's what happens when Account B gets deleted during the transfer - the $100 goes poof! To prevent this, you could first grab Sarah's hand and promise you won't let go until she takes the money (that's like SELECT FOR UPDATE). Or you could make everyone stand in a line where no one can disappear until all trades are finished (that's SERIALIZABLE). This way, money never gets lost!","relevanceScore":null,"lastUpdated":"2025-12-21T12:48:46.960Z","createdAt":"2025-12-24 12:51:25"},{"id":"da-170","question":"You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?","answer":"Use ACID transactions with BEGIN, UPDATE accounts, COMMIT/ROLLBACK. Ensure atomicity by debiting source and crediting destination in single transaction.","explanation":"## Transaction Design for Money Transfers\n\n### Key Requirements\n- **Atomicity**: Both debit and credit must succeed or fail together\n- **Consistency**: Total money in system remains constant\n- **Isolation**: Concurrent transfers don't interfere\n- **Durability**: Completed transfers survive system crashes\n\n### Implementation Strategy\n\n```sql\nBEGIN TRANSACTION;\n\n-- Check sufficient funds\nSELECT balance FROM accounts WHERE id = :source_id FOR UPDATE;\n\n-- Debit source account\nUPDATE accounts \nSET balance = balance - :amount \nWHERE id = :source_id AND balance >= :amount;\n\n-- Credit destination account  \nUPDATE accounts \nSET balance = balance + :amount \nWHERE id = :dest_id;\n\n-- Verify both operations succeeded\nIF (rowcount_source = 1 AND rowcount_dest = 1) THEN\n    COMMIT;\nELSE\n    ROLLBACK;\nEND IF;\n```\n\n### Crash Recovery\n- **Before COMMIT**: Transaction is rolled back on restart\n- **After COMMIT**: Changes are durable due to write-ahead logging\n- **During COMMIT**: Database ensures atomic completion\n\n### Concurrency Control\n- Use row-level locks with `FOR UPDATE`\n- Implement deadlock detection and retry logic\n- Consider isolation levels (READ COMMITTED vs SERIALIZABLE)\n\n### Monitoring\n- Track transaction success/failure rates\n- Monitor lock contention and deadlock frequency\n- Audit trail for all financial transactions","diagram":"graph TD\n    A[Client Initiates Transfer] --> B[BEGIN TRANSACTION]\n    B --> C[LOCK Source Account]\n    C --> D[CHECK Balance >= Amount]\n    D --> E{Sufficient Funds?}\n    E -->|No| F[ROLLBACK - Return Error]\n    E -->|Yes| G[DEBIT Source Account]\n    G --> H[LOCK Destination Account]\n    H --> I[CREDIT Destination Account]\n    I --> J{Both Updates Successful?}\n    J -->|No| K[ROLLBACK - Return Error]\n    J -->|Yes| L[COMMIT Transaction]\n    L --> M[Release Locks]\n    M --> N[Return Success]\n    \n    O[System Crash] --> P{Crash Timing}\n    P -->|Before COMMIT| Q[Automatic ROLLBACK on Restart]\n    P -->|After COMMIT| R[Changes Preserved via WAL]\n    P -->|During COMMIT| S[Database Ensures Atomic Completion]","difficulty":"intermediate","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=AcqtAEzuoj0"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"],"eli5":"Imagine you're trading toys with your friend. You give them your red car, and they give you their blue ball. But what if your mom calls you away in the middle? You don't want to lose your car AND not get the ball!\n\nSo you put both toys in a special magic box. The box only opens if BOTH toys are inside. If something goes wrong, the box closes and keeps everything safe - no toys lost!\n\nBank transfers work the same way. The money goes from your account to your friend's account inside a magic box. If the computer crashes, the box closes and puts the money back where it started. Nobody loses money and nobody gets extra money. It's all or nothing - just like toy trading should be!","relevanceScore":null,"lastUpdated":"2025-12-21T13:16:36.003Z","createdAt":"2025-12-24 12:51:25"},{"id":"da-172","question":"In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios you must handle?","answer":"Use coordinator to prepare/commit phases, handle node failures, timeouts, and network partitions with recovery protocols.","explanation":"## Two-Phase Commit Implementation\n\n### Phase 1: Prepare\n1. **Coordinator sends prepare** to all participants\n2. **Participants validate** they can commit (lock resources, write to log)\n3. **Participants respond** with 'vote-commit' or 'vote-abort'\n\n### Phase 2: Commit\n- If all vote-commit: coordinator sends commit, participants acknowledge\n- If any vote-abort: coordinator sends abort, participants rollback\n\n### Failure Scenarios\n1. **Coordinator failure during prepare**: Participants timeout and abort\n2. **Coordinator failure after decision**: Use transaction logs to recover\n3. **Participant failure**: Coordinator retries, other participants wait\n4. **Network partition**: Timeout mechanisms prevent indefinite blocking\n\n### Optimizations\n- **Three-phase commit** adds pre-commit phase to reduce blocking\n- **Paxos/Raft** for coordinator election\n- **Timeout handling** with exponential backoff","diagram":"graph TD\n    A[Coordinator] -->|Prepare Request| B[Participant 1]\n    A -->|Prepare Request| C[Participant 2]\n    A -->|Prepare Request| D[Participant 3]\n    B -->|Vote-Commit| A\n    C -->|Vote-Commit| A\n    D -->|Vote-Commit| A\n    A -->|Global Commit| B\n    A -->|Global Commit| C\n    A -->|Global Commit| D\n    B -->|Ack| A\n    C -->|Ack| A\n    D -->|Ack| A\n    E[Failure Scenario] --> F[Coordinator Crash]\n    E --> G[Participant Timeout]\n    E --> H[Network Partition]","difficulty":"advanced","tags":["acid","transactions"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=eltn4x788UM"},"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"],"eli5":"Imagine you and your friends are building a giant LEGO castle together. Before anyone adds their piece, everyone raises their hand to say 'I'm ready!' That's the first phase - checking if everyone can participate. Once all hands are up, you all say 'GO!' and everyone adds their piece at the same time. If someone's hand doesn't go up, or if they get distracted and don't add their piece when you say 'GO,' you have to start over. You also need a plan for when friends get called away for dinner mid-game or when someone can't hear you from across the playground. The two-phase commit is just like making sure everyone is ready before doing something important together!","relevanceScore":null,"lastUpdated":"2025-12-22T04:57:13.447Z","createdAt":"2025-12-24 12:51:26"},{"id":"db-2","question":"How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?","answer":"Atomicity ensures all-or-nothing execution, Consistency maintains valid states, Isolation prevents interference, Durability guarantees persistence.","explanation":"**Banking Transfer Scenario**: Account A transfers $100 to Account B\n\n• **Atomicity**: Either both debit from A AND credit to B succeed, or both fail and rollback completely\n• **Consistency**: Database maintains valid state - total money remains constant, account balances never go negative\n• **Isolation**: Concurrent transactions see consistent snapshots - if Account C checks A's balance during transfer, they see either before or after state, never partial\n• **Durability**: Once transaction commits, changes persist even through system crashes via Write-Ahead Logging","diagram":"graph TD\n    Start[Transfer $100: A → B] --> Atomic{Atomicity Check}\n    Atomic -->|Success| Consistent[Consistency Validation]\n    Atomic -->|Failure| Rollback[Complete Rollback]\n    Consistent --> Isolated[Isolation Control]\n    Isolated --> Durable[Durability Commit]\n    Durable --> Complete[Transaction Complete]\n    Rollback --> Failed[Transaction Failed]","difficulty":"intermediate","tags":["acid","transactions","theory"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Amazon","Goldman Sachs","Google","PayPal","Stripe"],"eli5":"Imagine you're moving your favorite toy from box A to box B. Atomicity means either the whole move happens or nothing at all - you can't leave the toy floating in between! Consistency is like making sure both boxes still follow the rules (no box gets too heavy or empty). Isolation is like having a private room where no one else can mess with your toys while you're moving them. Durability is like taking a picture to remember exactly where each toy ended up, even if the lights go out. Your $100 transfer works the same way - it either moves completely or not at all, keeping both accounts safe and correct!","relevanceScore":null,"lastUpdated":"2025-12-24T12:39:47.897Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-190","question":"What is the difference between READ COMMITTED and REPEATABLE READ isolation levels in database transactions, and how does MVCC implementation affect their behavior?","answer":"READ COMMITTED sees only committed data at query time, allowing non-repeatable reads. REPEATABLE READ uses MVCC snapshots to guarantee consistent reads within a transaction, preventing non-repeatable reads but still allowing phantom reads in most implementations like MySQL InnoDB.","explanation":"## Key Differences\n\n**READ COMMITTED**: Each query sees a fresh snapshot of committed data\n**REPEATABLE READ**: Single snapshot for entire transaction duration\n\n## MVCC Implementation\n\n- **PostgreSQL**: Both levels use MVCC, but REPEATABLE READ creates transaction-wide snapshot\n- **MySQL InnoDB**: REPEATABLE READ prevents most phantom reads through next-key locking\n- **SQL Server**: REPEATABLE READ uses range locks to prevent phantom reads\n\n## Performance Trade-offs\n\n- **READ COMMITTED**: Lower memory usage, better for long-running transactions\n- **REPEATABLE READ**: Higher memory for snapshot maintenance, potential lock contention\n\n## Real-world Scenarios\n\n**Banking**: REPEATABLE READ prevents balance changes during statement generation\n**Analytics**: READ COMMITTED preferred for real-time reporting with minimal locking\n\n## Edge Cases\n\n- **Hot rows**: REPEATABLE READ may cause lock escalation\n- **Long transactions**: Snapshot maintenance overhead increases with time\n- **Replication lag**: READ COMMITTED may show inconsistent data across replicas","diagram":"flowchart TD\n    A[Transaction Starts] --> B{Isolation Level}\n    B -->|READ COMMITTED| C[Query 1: Reads Committed Data]\n    B -->|REPEATABLE READ| D[Query 1: Takes Snapshot]\n    C --> E[Other Transaction Commits]\n    D --> E\n    E --> F{Query 2}\n    F -->|READ COMMITTED| G[Sees New Committed Data]\n    F -->|REPEATABLE READ| H[Sees Same Snapshot Data]\n    G --> I[Non-repeatable Read Possible]\n    H --> J[Consistent Read Guaranteed]","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T06:27:59.373Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-317","question":"Explain how MVCC (Multi-Version Concurrency Control) works and how it prevents lost updates in a database system?","answer":"MVCC creates multiple versions of data rows, allowing reads to proceed without blocking writes and preventing lost updates through version comparison.","explanation":"## Why Asked\nInterview context at Microsoft and similar companies tests understanding of database concurrency mechanisms and how they handle simultaneous operations.\n## Key Concepts\n- Snapshot isolation\n- Version chains\n- Transaction visibility\n- Write-write conflicts\n## Code Example\n```\n-- Transaction 1\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n-- Transaction 2 (concurrent)\nBEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n-- MVCC prevents lost updates\n```\n## Follow-up Questions\n- How does MVCC differ from two-phase locking?\n- What are the storage overhead implications?\n- How does garbage collection work in MVCC?","diagram":"flowchart TD\n  A[Start Transaction] --> B[Create Snapshot]\n  B --> C[Read from Version Chain]\n  C --> D[Write New Version]\n  D --> E[Commit Check]\n  E --> F[End]","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs"},"companies":["Microsoft","Plaid","Warner Bros"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T13:31:42.971Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-353","question":"You're building a collaborative design tool where multiple users can edit the same document simultaneously. How would you use database transactions and isolation levels to prevent conflicts while maintaining good performance?","answer":"Use MVCC with READ COMMITTED isolation, implement optimistic locking with version columns, and handle conflicts with retry logic.","explanation":"## Why This Is Asked\nTests understanding of concurrent access patterns, transaction isolation, and performance trade-offs - critical for Canva's collaborative editing features.\n\n## Expected Answer\nStrong candidates discuss: MVCC benefits, READ COMMITTED vs SERIALIZABLE trade-offs, optimistic vs pessimistic locking, conflict resolution strategies, and how to balance consistency with performance.\n\n## Code Example\n```typescript\n// Optimistic locking with version check\nasync function updateDocument(docId: string, changes: any, expectedVersion: number) {\n  const result = await db.transaction(async (tx) => {\n    const doc = await tx.query.documents.findFirst({\n      where: { id: docId, version: expectedVersion }\n    });\n    \n    if (!doc) throw new Error('Document modified by another user');\n    \n    return await tx.update.documents\n      .set({ ...changes, version: expectedVersion + 1 })\n      .where({ id: docId });\n  });\n  \n  return result;\n}\n```\n\n## Follow-up Questions\n- How would you handle long-running transactions?\n- What isolation level would you choose for analytics queries?\n- How would you detect and resolve deadlocks?","diagram":"flowchart TD\n  A[User starts edit] --> B[Read document with version]\n  B --> C[Make changes locally]\n  C --> D[Attempt update with version check]\n  D --> E{Version matches?}\n  E -->|Yes| F[Update successful]\n  E -->|No| G[Conflict detected]\n  G --> H[Refresh and retry]\n  F --> I[End]\n  H --> B","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":"https://www.youtube.com/watch?v=pomxJOFVcQs","longVideo":"https://www.youtube.com/watch?v=qcInj-XW1Vc"},"companies":["Adobe","Amazon","Canva","Epic Games","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T13:00:33.646Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-397","question":"In a high-transaction payment system using PostgreSQL, how would you design a transaction isolation strategy to prevent lost updates while maintaining high concurrency for account transfers?","answer":"Use SERIALIZABLE isolation with explicit row-level locking and retry logic for account transfers to prevent lost updates while maintaining concurrency.","explanation":"## Why This Is Asked\nPayPal needs to ensure financial transaction integrity under high load. This tests understanding of ACID properties, isolation levels, and practical database design for financial systems.\n\n## Expected Answer\nStrong candidates will discuss:\n- SERIALIZABLE vs REPEATABLE READ trade-offs\n- Explicit row-level locking with SELECT FOR UPDATE\n- Deadlock detection and retry mechanisms\n- Connection pooling and transaction timeout handling\n- Monitoring for serialization failures\n\n## Code Example\n```typescript\nasync function transferFunds(fromId: number, toId: number, amount: number) {\n  const maxRetries = 3;\n  \n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    const tx = await db.transaction({ isolationLevel: 'serializable' });\n    \n    try {\n      // Lock both accounts to prevent concurrent modifications\n      const [fromAccount, toAccount] = await Promise.all([\n        tx.query('SELECT balance FROM accounts WHERE id = $1 FOR UPDATE', [fromId]),\n        tx.query('SELECT balance FROM accounts WHERE id = $1 FOR UPDATE', [toId])\n      ]);\n      \n      if (fromAccount.balance < amount) {\n        await tx.rollback();\n        throw new Error('Insufficient funds');\n      }\n      \n      await Promise.all([\n        tx.query('UPDATE accounts SET balance = balance - $1 WHERE id = $2', [amount, fromId]),\n        tx.query('UPDATE accounts SET balance = balance + $1 WHERE id = $2', [amount, toId])\n      ]);\n      \n      await tx.commit();\n      return;\n    } catch (error) {\n      await tx.rollback();\n      if (error.code === '40001' && attempt < maxRetries) {\n        // Serialization failure, retry with exponential backoff\n        await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 100));\n        continue;\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you handle distributed transactions across multiple databases?\n- What monitoring metrics would you track for transaction performance?\n- How would you optimize this for thousands of concurrent transfers?","diagram":"flowchart TD\n  A[Client Request Transfer] --> B[Begin Serializable Transaction]\n  B --> C[SELECT FROM Account FOR UPDATE]\n  C --> D[SELECT TO Account FOR UPDATE]\n  D --> E{Sufficient Balance?}\n  E -->|No| F[Rollback & Return Error]\n  E -->|Yes| G[UPDATE FROM Account Balance]\n  G --> H[UPDATE TO Account Balance]\n  H --> I{Serialization Failure?}\n  I -->|Yes| J[Rollback & Retry]\n  I -->|No| K[Commit Transaction]\n  J --> B\n  F --> L[End]\n  K --> L","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Netflix","PayPal","Square","Stripe"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T12:50:09.054Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-428","question":"You're building a booking system for Airbnb where multiple users can reserve the same property simultaneously. How would you design the transaction handling to prevent double bookings while maintaining high availability?","answer":"Use SERIALIZABLE isolation with optimistic concurrency control. Implement row-level locks on property availability tables, use MVCC snapshot reads for checking availability, and apply application-leve","explanation":"## Problem Context\nAirbnb's booking system faces race conditions where multiple guests can book the same property simultaneously, leading to overbookings and customer dissatisfaction.\n\n## Technical Solution\n- **Database Design**: Separate availability calendar table with row-level locks\n- **Isolation Level**: SERIALIZABLE for critical booking operations\n- **Concurrency Pattern**: Optimistic locking with version columns\n- **Performance Strategy**: Read replicas for availability checks, write-through caching\n\n## Implementation Details\n- Use PostgreSQL's SELECT FOR UPDATE to lock specific date ranges\n- Implement retry logic with exponential backoff for serialization failures\n- Cache availability data with 5-minute TTL, invalidate on bookings\n- Use distributed transactions across booking and payment services\n\n## Trade-offs Considered\n- SERIALIZABLE ensures consistency but reduces throughput\n- Row-level locks prevent deadlocks better than table locks\n- Caching improves read performance but adds complexity","diagram":"flowchart TD\n  A[Guest Initiates Booking] --> B[Check Availability via Read Replica]\n  B --> C{Property Available?}\n  C -->|Yes| D[Begin Serializable Transaction]\n  C -->|No| E[Return Unavailable]\n  D --> F[SELECT FOR UPDATE on Property Dates]\n  F --> G[Re-check Availability]\n  G --> H{Still Available?}\n  H -->|Yes| I[Create Booking Record]\n  H -->|No| J[Rollback & Retry]\n  I --> K[Update Availability Calendar]\n  K --> L[Invalidate Cache]\n  L --> M[Commit Transaction]\n  M --> N[Send Confirmation]","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"channel":"database","subChannel":"transactions","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-23T16:38:38.904Z","createdAt":"2025-12-24 12:51:27"}],"subChannels":["indexing","nosql","query-optimization","sql","transactions"],"companies":["Adobe","Affirm","Airbnb","Amazon","Apple","Bloomberg","Canva","Discord","Epic Games","Gitlab","Goldman Sachs","Google","Hashicorp","Hrt","Jane Street","Meta","Microsoft","Netflix","Oracle","Palantir","PayPal","Plaid","Salesforce","Slack","Square","Stripe","Tcs","Tempus","Tesla","Uber","Walmart","Warner Bros"],"stats":{"total":28,"beginner":8,"intermediate":11,"advanced":9,"newThisWeek":28}}