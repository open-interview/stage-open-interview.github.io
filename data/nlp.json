{"questions":[{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","answer":"I'd use a transformer-based model like BERT fine-tuned on domain data. Key preprocessing: tokenization with subword handling, negation scope detection using dependency parsing, and custom slang dictio","explanation":"## Implementation Approach\n- **Model Selection**: BERT or RoBERTa fine-tuned on sentiment data\n- **Negation Handling**: Dependency parsing to identify negation scope\n- **Domain Adaptation**: Continued pretraining on company-specific reviews\n\n## Preprocessing Pipeline\n- Tokenization with subword vocabulary\n- Slang normalization using custom dictionary\n- Negation detection and scope marking\n- Text cleaning preserving sentiment-bearing words\n\n## Performance Considerations\n- Batch processing for efficiency\n- Model quantization for deployment\n- A/B testing with baseline models","diagram":"flowchart TD\n  A[Raw Reviews] --> B[Text Cleaning]\n  B --> C[Slang Normalization]\n  C --> D[Negation Detection]\n  D --> E[Tokenization]\n  E --> F[BERT Model]\n  F --> G[Sentiment Score]","difficulty":"intermediate","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM","Square"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:47:32.335Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-500","question":"How would you implement basic text preprocessing for sentiment analysis, including tokenization, stop word removal, and stemming?","answer":"Use NLTK for preprocessing: tokenize with word_tokenize, remove stop words using stopwords corpus, apply PorterStemmer for stemming. Handle punctuation, convert to lowercase, and filter empty tokens. ","explanation":"## Text Preprocessing Pipeline\n\n- **Tokenization**: Split text into individual words using word_tokenize()\n- **Normalization**: Convert to lowercase and remove punctuation\n- **Stop word removal**: Filter common words using NLTK's stopwords corpus\n- **Stemming**: Apply PorterStemmer to reduce words to root forms\n- **Filtering**: Remove empty tokens and special characters\n\n```python\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef preprocess_text(text):\n    tokens = nltk.word_tokenize(text.lower())\n    stop_words = set(stopwords.words('english'))\n    stemmer = PorterStemmer()\n    \n    filtered = [stemmer.stem(token) for token in tokens \n                if token.isalpha() and token not in stop_words]\n    return filtered\n```\n\nThis pipeline is essential for NLP tasks as it reduces noise and standardizes text representation.","diagram":"flowchart TD\n  A[Raw Text] --> B[Tokenization]\n  B --> C[Lowercase & Punctuation Removal]\n  C --> D[Stop Word Filtering]\n  D --> E[Stemming]\n  E --> F[Clean Tokens]","difficulty":"beginner","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Google"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-25T01:15:31.137Z","createdAt":"2025-12-25T01:15:31.137Z"},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","answer":"Tokenization splits text into individual tokens (words, punctuation), while stemming reduces words to root forms by removing suffixes using algorithms like Porter or Snowball. Stemming is faster but can produce non-words (e.g., 'studies' → 'studi'). Lemmatization uses dictionary mapping to produce valid words ('studies' → 'study') but is slower. Choose stemming for search/retrieval where speed matters, lemmatization for analysis requiring accurate word forms.","explanation":"## Key Differences\n**Tokenization**: Text segmentation into tokens (words, subwords, punctuation). Essential first step for any NLP pipeline.\n**Stemming**: Rule-based suffix removal using algorithms like Porter (lightweight) or Snowball (language-specific). Fast but crude.\n**Lemmatization**: Dictionary-based morphological analysis returning valid root words. Slower but more accurate.\n\n## Trade-offs\n- **Speed**: Stemming ~10x faster than lemmatization\n- **Accuracy**: Lemmatization produces linguistically valid roots\n- **Memory**: Stemming requires minimal resources\n\n## When to Use Each\n**Stemming**: Search engines, spam detection, document clustering where speed > precision\n**Lemmatization**: Question answering, sentiment analysis, chatbots where word meaning matters\n\n## Implementation Example\n```python\n# Stemming\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nprint(stemmer.stem('studies'))  # 'studi'\n\n# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('studies'))  # 'study'\n```\n\n## Real-world Impact\nSearch engines often use stemming for broader matching, while sophisticated NLP applications prefer lemmatization for maintaining semantic accuracy.","diagram":"graph TD\n    A[Raw Text] --> B[Tokenization]\n    B --> C[Tokens: 'running', 'dogs']\n    C --> D[Stemming]\n    D --> E[Stemmed: 'run', 'dog']\n    B --> F[Feature Extraction]\n    D --> G[Search Indexing]\n    F --> H[ML Model Input]\n    G --> I[Text Retrieval]","difficulty":"beginner","tags":["tokenization","stemming","ner"],"channel":"nlp","subChannel":"text-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T06:28:45.056Z","createdAt":"2025-12-24 12:51:27"}],"subChannels":["general","text-processing"],"companies":["Airbnb","Apple","DoorDash","Google","IBM","Square"],"stats":{"total":3,"beginner":2,"intermediate":1,"advanced":0,"newThisWeek":3}}