{"questions":[{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","answer":"I'd use a transformer-based model like BERT fine-tuned on domain data. Key preprocessing: tokenization with subword handling, negation scope detection using dependency parsing, and custom slang dictio","explanation":"## Implementation Approach\n- **Model Selection**: BERT or RoBERTa fine-tuned on sentiment data\n- **Negation Handling**: Dependency parsing to identify negation scope\n- **Domain Adaptation**: Continued pretraining on company-specific reviews\n\n## Preprocessing Pipeline\n- Tokenization with subword vocabulary\n- Slang normalization using custom dictionary\n- Negation detection and scope marking\n- Text cleaning preserving sentiment-bearing words\n\n## Performance Considerations\n- Batch processing for efficiency\n- Model quantization for deployment\n- A/B testing with baseline models","diagram":"flowchart TD\n  A[Raw Reviews] --> B[Text Cleaning]\n  B --> C[Slang Normalization]\n  C --> D[Negation Detection]\n  D --> E[Tokenization]\n  E --> F[BERT Model]\n  F --> G[Sentiment Score]","difficulty":"intermediate","tags":["nlp"],"channel":"nlp","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","IBM","Square"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:47:32.335Z","createdAt":"2025-12-24T02:47:32.335Z"},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","answer":"Tokenization splits text into individual tokens (words, punctuation), while stemming reduces words to root forms by removing suffixes using algorithms like Porter or Snowball. Stemming is faster but can produce non-words (e.g., 'studies' → 'studi'). Lemmatization uses dictionary mapping to produce valid words ('studies' → 'study') but is slower. Choose stemming for search/retrieval where speed matters, lemmatization for analysis requiring accurate word forms.","explanation":"## Key Differences\n**Tokenization**: Text segmentation into tokens (words, subwords, punctuation). Essential first step for any NLP pipeline.\n**Stemming**: Rule-based suffix removal using algorithms like Porter (lightweight) or Snowball (language-specific). Fast but crude.\n**Lemmatization**: Dictionary-based morphological analysis returning valid root words. Slower but more accurate.\n\n## Trade-offs\n- **Speed**: Stemming ~10x faster than lemmatization\n- **Accuracy**: Lemmatization produces linguistically valid roots\n- **Memory**: Stemming requires minimal resources\n\n## When to Use Each\n**Stemming**: Search engines, spam detection, document clustering where speed > precision\n**Lemmatization**: Question answering, sentiment analysis, chatbots where word meaning matters\n\n## Implementation Example\n```python\n# Stemming\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nprint(stemmer.stem('studies'))  # 'studi'\n\n# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nprint(lemmatizer.lemmatize('studies'))  # 'study'\n```\n\n## Real-world Impact\nSearch engines often use stemming for broader matching, while sophisticated NLP applications prefer lemmatization for maintaining semantic accuracy.","diagram":"graph TD\n    A[Raw Text] --> B[Tokenization]\n    B --> C[Tokens: 'running', 'dogs']\n    C --> D[Stemming]\n    D --> E[Stemmed: 'run', 'dog']\n    B --> F[Feature Extraction]\n    D --> G[Search Indexing]\n    F --> H[ML Model Input]\n    G --> I[Text Retrieval]","difficulty":"beginner","tags":["tokenization","stemming","ner"],"channel":"nlp","subChannel":"text-processing","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":74,"lastUpdated":"2025-12-24T06:28:45.056Z","createdAt":"2025-12-23 12:53:08"}],"subChannels":["general","text-processing"],"companies":["DoorDash","IBM","Square"],"stats":{"total":2,"beginner":1,"intermediate":1,"advanced":0,"newThisWeek":2}}