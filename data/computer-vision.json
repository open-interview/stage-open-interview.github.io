{"questions":[{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","answer":"Use a distributed pipeline with GPU clusters for inference, model quantization (TensorRT/ONNX), edge caching for popular content, and ensemble models. Implement batch processing, model versioning, and","explanation":"## Architecture\n- **Inference Layer**: GPU clusters with model quantization\n- **Caching**: Redis for frequent detections, CDN edge caching\n- **Load Balancing**: Kubernetes with auto-scaling based on queue depth\n- **Monitoring**: Real-time accuracy metrics and latency tracking\n\n## Optimization Strategies\n- **Model Compression**: TensorRT optimization, 8-bit quantization\n- **Batch Processing**: Dynamic batching based on load\n- **Fallback Mechanism**: CPU inference when GPU unavailable\n- **Model Ensemble**: Combine YOLOv8 with ResNet for accuracy\n\n## Production Considerations\n- **Model Versioning**: Canary deployments with gradual rollout\n- **Drift Detection**: Automated accuracy monitoring and alerts\n- **Cost Management**: Spot instances for non-critical workloads","diagram":"flowchart TD\n  A[Image Upload] --> B[Preprocessing]\n  B --> C[GPU Inference]\n  C --> D[Postprocessing]\n  D --> E[Cache Check]\n  E --> F[Result Storage]\n  G[Load Balancer] --> C\n  H[Monitor] --> I[Alert System]\n  J[Model Registry] --> C","difficulty":"advanced","tags":["computer-vision"],"channel":"computer-vision","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-24T02:45:30.998Z","createdAt":"2025-12-24 12:51:27"},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","answer":"Use ResNet blocks with channel attention, apply EfficientNet's compound scaling formula φ = α^β · γ^φ, and optimize with mixed precision.","explanation":"## Concept\nHybrid CNN combines ResNet's residual connections (identity shortcuts) with EfficientNet's compound scaling that balances network depth, width, and resolution using a fixed compound coefficient φ.\n\n## Implementation\n```python\n# Hybrid block with residual + efficient scaling\nclass HybridBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        # EfficientNet MBConv with residual\n        self.mbconv = MBConv(in_ch, out_ch, stride)\n        self.shortcut = nn.Identity() if stride == 1 else nn.Conv2d(in_ch, out_ch, 1)\n        \n    def forward(self, x):\n        return self.mbconv(x) + self.shortcut(x)\n\n# Compound scaling: depth=φ^1.2, width=φ^0.8, resolution=φ^0.2\n```\n\n## Trade-offs\n**Pros**: Better accuracy-efficiency balance, stable gradients from residuals, optimized FLOPs\n**Cons**: Complex implementation, higher memory than pure EfficientNet, tuning complexity\n\n## Pitfalls\n- Mismatched channel dimensions in residual connections\n- Over-scaling leads to overfitting on small datasets\n- Ignoring hardware-specific optimizations (TensorRT, ONNX)","diagram":"flowchart TD\n    A[Input Image] --> B[Stem Conv 3x3]\n    B --> C[Hybrid Block 1: MBConv + Residual]\n    C --> D[Hybrid Block 2: MBConv + Residual]\n    D --> E[SE Block: Channel Attention]\n    E --> F[Hybrid Block 3: Downsample]\n    F --> G[Global Average Pool]\n    G --> H[Fully Connected]\n    H --> I[Softmax]\n    \n    C --> C1[Identity Shortcut]\n    C1 --> C2[Add]\n    D --> C2\n    E --> C2","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"channel":"computer-vision","subChannel":"image-classification","sourceUrl":"https://arxiv.org/abs/1905.11946","videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T08:35:10.741Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","answer":"YOLO divides image into S×S grid, each cell predicts B bounding boxes with confidence scores, class probabilities, and uses anchor boxes for better localization.","explanation":"## Interview Context\nThis question assesses understanding of modern object detection architectures and trade-offs between speed and accuracy. YOLO's unified approach is fundamental for real-time applications.\n\n## Technical Details\n\n### Core Architecture\n- **Backbone**: Darknet feature extractor (Darknet-53 for YOLOv3)\n- **Grid System**: S×S grid where each cell predicts B bounding boxes\n- **Output Tensor**: [S, S, B*(5+C)] where 5 = (x, y, w, h, confidence), C = class count\n\n### Key Components\n- **Anchor Boxes**: Pre-defined aspect ratios to improve bounding box prediction\n- **IoU Calculation**: Intersection over Union for confidence scoring\n- **Non-Maximum Suppression**: Removes redundant detections above IoU threshold\n- **Multi-Scale Predictions**: Feature maps at different scales for various object sizes\n\n### Loss Function\n```\nLoss = λ_coord * MSE(bbox) + λ_noobj * MSE(confidence) + MSE(class)\n```\n\n### Training Process\n- Single-stage training end-to-end\n- Uses mean squared error for bounding box regression\n- Cross-entropy for classification\n\n## Performance Comparison\n- **YOLOv1**: 45 FPS on Titan X (63.4% mAP)\n- **Faster R-CNN**: 7 FPS (73.2% mAP)\n- **YOLOv3**: 30 FPS (57.9% mAP)\n\n## Follow-up Questions\n1. How does YOLOv5 improve upon YOLOv3's architecture and training methodology?\n2. What are the trade-offs between YOLO and two-stage detectors like Faster R-CNN?\n3. How would you optimize YOLO for edge devices with limited computational resources?","diagram":"graph TD\n    A[Input Image 448×448] --> B[Backbone Network DarkNet-53]\n    B --> C[Feature Maps]\n    C --> D[Detection Head]\n    D --> E[Grid S×S]\n    E --> F[Each Cell Predicts]\n    F --> G[B Bounding Boxes]\n    F --> H[Confidence Scores]\n    F --> I[Class Probabilities]\n    G --> J[Non-Max Suppression]\n    H --> J\n    I --> J\n    J --> K[Final Detections]","difficulty":"beginner","tags":["yolo","rcnn","detr"],"channel":"computer-vision","subChannel":"object-detection","sourceUrl":null,"videos":null,"companies":null,"eli5":"Imagine you have a big chocolate chip cookie and you want to find all the chocolate chips really fast! You cut the cookie into tiny squares, like a checkerboard. Each little square has a special job - it looks for chocolate chips that might be hiding inside it. Each square also draws a box around any chips it finds and says how sure it is that it's really a chip. Some squares might think they see a chip when it's just a crumb, so they have to be very confident! The computer learns to do this super quickly by practicing with lots of cookies, so it can find all the chocolate chips in one quick look instead of searching slowly all over the cookie.","relevanceScore":null,"lastUpdated":"2025-12-22T08:33:19.669Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","answer":"U-Net uses contracting encoder for context, expanding decoder with skip connections to preserve spatial details for precise pixel-wise segmentation.","explanation":"## Why Asked\nTests understanding of advanced CNN architectures for medical imaging and computer vision segmentation tasks\n## Key Concepts\n- Encoder-decoder architecture\n- Skip connections for feature preservation\n- Contracting and expanding paths\n- Pixel-wise segmentation\n## Code Example\n```\ndef unet_block(x, skip):\n    x = Conv2D(64, 3, padding='same')(x)\n    x = concatenate([x, skip])\n    return x\n```\n## Follow-up Questions\n- How does U-Net handle class imbalance?\n- What are alternatives to skip connections?\n- How does it compare to FCN?","diagram":"flowchart TD\n  A[Input Image] --> B[Encoder Path]\n  B --> B1[Bottleneck]\n  B1 --> C[Decoder Path]\n  C --> D[Segmentation Output]\n  B -.->|Skip Connections| C","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":"https://www.youtube.com/watch?v=HS3Q_90hnDg"},"companies":["Amazon","Google","Meta"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:14:42.393Z","createdAt":"2025-12-24 12:51:26"},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","answer":"Use SAM's lightweight encoder with quantized ViT-B, implement prompt caching, and apply tensorRT optimization for sub-100ms inference.","explanation":"## Concept Overview\nReal-time medical segmentation requires balancing accuracy with strict latency constraints. SAM (Segment Anything Model) provides zero-shot segmentation but needs optimization for edge deployment.\n\n## Implementation Details\n- **Model Optimization**: Use SAM-ViT-B (lightweight) with INT8 quantization\n- **Prompt Engineering**: Implement prompt caching for similar anatomical regions\n- **Hardware Acceleration**: Deploy with TensorRT on NVIDIA Jetson or CoreML on Apple Silicon\n- **Batch Processing**: Process multiple slices in parallel when available\n\n## Code Example\n```python\n# Optimized SAM inference pipeline\nimport torch\nfrom segment_anything import sam_model_registry\n\nclass OptimizedSAM:\n    def __init__(self):\n        self.sam = sam_model_registry['vit_b'](checkpoint='sam_vit_b.pth')\n        self.sam.eval()\n        self.sam.cuda()\n        # Enable TensorRT optimization\n        self.sam = torch.compile(self.sam, mode='max-autotune')\n    \n    def segment_with_cache(self, image, prompt):\n        # Check prompt cache first\n        cache_key = hash(prompt.tobytes())\n        if cache_key in self.prompt_cache:\n            return self.prompt_cache[cache_key]\n        \n        masks = self.sam.predict(image, prompt)\n        self.prompt_cache[cache_key] = masks\n        return masks\n```\n\n## Common Pitfalls\n- **Memory Overhead**: Prompt caching can consume significant memory on edge devices\n- **Quantization Loss**: INT8 quantization may reduce fine-grained segmentation accuracy\n- **Prompt Sensitivity**: Medical images require precise prompt placement for accurate results\n- **Hardware Variability**: Different edge devices have varying compute capabilities","diagram":"graph TD[Input Medical Image] --> A[Preprocessing: Resize/Normalize]\nA --> B[Prompt Detection: Anatomical Region]\nB --> C{Prompt Cache Hit?}\nC -->|Yes| D[Return Cached Mask]\nC -->|No| E[SAM Encoder: ViT-B Lightweight]\nE --> F[Prompt-Guided Decoder]\nF --> G[Post-processing: Refine Boundaries]\nG --> H[Cache Result]\nH --> I[Output Segmentation Mask]\nD --> I\n\nsubgraph Edge Device Optimization\n    J[TensorRT Engine] --> K[INT8 Quantization]\n    K --> L[Memory Pool Management]\nend\n\nE -.-> J\nF -.-> J","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"channel":"computer-vision","subChannel":"segmentation","sourceUrl":null,"videos":null,"companies":["Apple","Google","Meta","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"lastUpdated":"2025-12-22T05:07:44.650Z","createdAt":"2025-12-24 12:51:26"}],"subChannels":["general","image-classification","object-detection","segmentation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA","Salesforce","Twitter"],"stats":{"total":5,"beginner":2,"intermediate":1,"advanced":2,"newThisWeek":5}}